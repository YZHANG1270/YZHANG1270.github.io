<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[å¦‚ä½•åœ¨Hexoä¸­æ·»åŠ æœ¬åœ°å›¾ç‰‡]]></title>
    <url>%2F2019%2F11%2F06%2Fadd-local-img%2F</url>
    <content type="text"><![CDATA[åœ¨å†™æ–‡ç« æ—¶ï¼Œç»å¸¸éœ€è¦æ’å…¥ä¸€äº›æœ¬åœ°çš„æˆªå›¾åˆ°æ–‡ç« ä¸­ï¼Œåœ¨æœ¬åœ°ç¼–è¾‘æ–‡ç« çš„æ—¶å€™è‡ªç„¶å¯ä»¥å°†æœ¬åœ°å›¾ç‰‡çš„é“¾æŽ¥æ’å…¥åˆ°æ–‡ç« ä¸­ï¼Œä½†è¿™å°±é¢ä¸´ä¸€ä¸ªé—®é¢˜ï¼šæœ¬åœ°çš„é“¾æŽ¥åœ¨æ–‡ç« å‘å¸ƒä¹‹åŽå¿…ç„¶å¤±æ•ˆï¼Œè¯¥æ€Žä¹ˆåŠžï¼Ÿ æŒ‰ç…§ç»éªŒï¼Œè§£å†³çš„åŠžæ³•è‡ªç„¶æ˜¯å°†å›¾ç‰‡ä¸Šä¼ åˆ°æŸä¸ªå›¾ç‰‡ç©ºé—´ï¼Œç„¶åŽå°†å›¾ç‰‡ç©ºé—´ä¸­å›¾ç‰‡çš„é“¾æŽ¥æ’å…¥æ–‡ç« ä¸­ã€‚ è¿™å½“ç„¶å¯ä»¥è§£å†³é—®é¢˜ï¼Œä½†æ˜¯æœªå…å¤ªéº»çƒ¦ã€‚ä»‹ç»ä¸¤ç§æ–¹æ³•ï¼š ç¬¬ä¸€ç§æ–¹æ³• åœ¨é…ç½®æ–‡ä»¶_config.ymlé‡Œä¿®æ”¹ï¼špost_asset_folder: true åœ¨Hexoå®‰è£…ç›®å½•ä¸‹æ‰§è¡Œ:npm install hexo-asset-image --saveï¼Œè¿™æ˜¯ä¸‹è½½å®‰è£…ä¸€ä¸ªå¯ä»¥ä¸Šä¼ æœ¬åœ°å›¾ç‰‡çš„æ’ä»¶ ç­‰å¾…ä¸€æ®µæ—¶é—´ä¹‹åŽï¼Œå†è¿è¡Œhexo n &quot;æ–‡ç« æ ‡é¢˜&quot;æ¥ç”Ÿæˆåšæ–‡æ—¶ï¼Œ/source/_postæ–‡ä»¶å¤¹ä¸­é™¤äº†æ–‡ç« æ ‡é¢˜.mdå¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªåŒåæ–‡ä»¶å¤¹ã€‚ åœ¨æ–°çš„åšæ–‡ä¸­æƒ³å¼•å…¥å›¾ç‰‡æ—¶ï¼Œå¯ä»¥å…ˆæŠŠå›¾ç‰‡å¤åˆ¶åˆ°åšæ–‡çš„åŒåæ–‡ä»¶å¤¹ï¼Œç„¶åŽåœ¨.mdä¸­æŒ‰ç…§å¸¸è§„çš„æ–¹å¼é¥®ç”¨å›¾ç‰‡å³å¯ï¼Œå¦‚![ä½ æƒ³è¾“å…¥çš„æ›¿ä»£æ–‡å­—](åšæ–‡æ ‡é¢˜/å›¾ç‰‡å.jpg)ã€‚æ³¨æ„ï¼Œæ­¤å¤„çš„å›¾ç‰‡è·¯å¾„å¿…é¡»ä½¿ç”¨ç›¸å¯¹è·¯å¾„ æ‰§è¡Œhexo g,æ£€æŸ¥ç”Ÿæˆçš„é¡µé¢ä¸­å›¾ç‰‡çš„srcåœ°å€ã€‚æ­¤æ—¶ç”Ÿæˆé¡µé¢ä¸­å›¾ç‰‡srcåœ°å€åº”è¯¥ä¸Žé¡µé¢çš„ç›¸å¯¹è·¯å¾„ä¸€è‡´ï¼ˆå…·ä½“è·¯å¾„å–å†³äºŽé¡µé¢è·¯å¾„æ ¼å¼è®¾ç½®ï¼‰ ç¬¬äºŒç§æ–¹æ³•ä»¥ä¸Šæ–¹æ³•å¯ä»¥è§£å†³æœ¬åœ°å›¾ç‰‡ä¸Šä¼ å’Œå¼•ç”¨çš„é—®é¢˜ï¼Œä½†æ˜¯åœ¨æ¯ä¸ªæ–‡ç« ä¸‹å»ºç«‹èµ„æºæ–‡ä»¶å¤¹å¥½å¤„æ˜¯åˆ†ç±»æ¸…æ¥šï¼Œç¼ºç‚¹æ˜¯å›¾ç‰‡å¤ç”¨ä¸æ–¹ä¾¿ï¼Œä¹Ÿä¸ç¬¦åˆç½‘ç«™è®¾è®¡çš„ä¸€èˆ¬è§„èŒƒã€‚ æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç¬¬äºŒç§æ–¹æ¡ˆï¼š åœ¨æœ¬åœ°sourceä¸­å»ºç«‹imgæ–‡ä»¶å¤¹ï¼Œå°†å¼•ç”¨åˆ°çš„å›¾ç‰‡å…¨éƒ¨æ”¾åœ¨æ­¤æ–‡ä»¶å¤¹ä¸­ã€‚è¿™æ ·æ“ä½œä¹Ÿä¾¿äºŽå›¾ç‰‡çš„å¤ç”¨ã€‚ æ³¨æ„ï¼Œé‡‡ç”¨è¿™ç§æ–¹æ³•æ—¶æ— éœ€ä¿®æ”¹_config.yml,ä¹Ÿæ— éœ€å®‰è£…hexo-asset-image]]></content>
  </entry>
  <entry>
    <title><![CDATA[HexoæŽ¥å…¥è°·æ­Œå¹¿å‘Š Google AdSense]]></title>
    <url>%2F2019%2F11%2F06%2Fgoogle-adsense%2F</url>
    <content type="text"><![CDATA[å‰ä¸¤å¤©åˆå¼€å§‹æŠ˜è…¾ç½‘ç«™ é¦–å…ˆä»‹ç»ä¸‹AdSenseï¼Œè°·æ­Œè”ç›Ÿ]]></content>
      <tags>
        <tag>ads</tag>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CCF-GAIR-2019 å‚ä¼šä½“éªŒ]]></title>
    <url>%2F2019%2F07%2F18%2Fccf-gair-2019%2F</url>
    <content type="text"><![CDATA[è¿™æ˜¯ç¬¬äºŒæ¬¡åŽ»CCF-GAIRï¼Œç¬¬äºŒå¹´ï¼Œç¬¬äºŒæ¬¡ï¼Œå¯ä»¥çœ‹æ¸…å¾ˆå¤šä¸œè¥¿ï¼Œæ¯”å¦‚æ•™æŽˆçš„pptä¼šé‡å¤ï¼Œæ¯”å¦‚å¹²è´§åœ¨ä»€ä¹ˆæ—¶å€™äº§å‡ºä¸é‡è¦ï¼Œåœ¨ä»€ä¹ˆæ—¶å€™å¬ä»€ä¹ˆæ—¶å€™ä¼ æ’­æ‰æ˜¯å‘æŒ¥ä»·å€¼çš„æ—¶å€™ï¼›ç¬¬ä¸€æ¬¡å¬çš„æ—¶å€™ä¼šè§‰å¾—æŽ¥è§¦äº†æ–°ä¸–ç•Œï¼›ç¬¬äºŒæ¬¡å¬å°±æ˜¯ï¼Œå“¦ï¼Œæˆ‘æ‡‚ã€‚å¤±åŽ»äº†åˆå­¦è€…çš„ä¹è¶£ã€‚ æ€»ä¹‹è¿™æ˜¯ä¸€åœºå¤§åž‹çš„å¤§ä½¬socialï¼Œæ¸£æ¸£æ‰“é…±æ²¹çš„ä¼šè®®ã€‚å¥½åœ¨äº²çœ¼è§äº†ä¸¤ä¸ªidolï¼Œå‘¨æ˜Žå’Œæ¨å¼ºã€‚ åŽ»å¹´å‚ä¼šä½“éªŒé“¾æŽ¥ï¼šCCF-GAIR-2018 å‚ä¼šä½“éªŒ ä»Šå¹´ä¸æ‰“ç®—åƒåŽ»å¹´é‚£æ ·ä»€ä¹ˆéƒ½å†™ä¸€ç‚¹äº†ï¼Œæ‰“ç®—åªå†™ç‚¹ä¸ŽNLPç›¸å…³çš„ä¸œè¥¿ã€‚ä½†æ˜¯è¯´å®žè¯ï¼Œç›¸å…³ä¼šåœºçš„ä¹Ÿä¸å¤šã€‚è™½ç„¶æ²¡æœ‰ä¸“é—¨çš„NLPä¼šåœºï¼Œä½†æ˜¯æ—å¬å…¶ä»–ä¼šåœºåŽè¿˜æ˜¯æœ‰ä¸€äº›NLPçš„å½±å­ï¼Œæ¯”å¦‚ï¼šå¹³å®‰çš„æ™ºèƒ½HRï¼Œç§‘å¤§è®¯é£žçš„æ™ºèƒ½æ‰¹ä½œä¸šï¼Œå¾®ä¼—é“¶è¡Œçš„æ™ºèƒ½é£ŽæŽ§ï¼Œå…¶å®ƒçš„è¿˜æœ‰æ™ºèƒ½èˆ†æƒ…ç›‘æŽ§ï¼Œæ™ºèƒ½å®¢æœï¼Œæœç´¢å¼•æ“Žç­‰ç­‰ã€‚æˆ‘æ‰“ç®—åªå†™å‘¨æ˜Žè€å¸ˆçš„åˆ†äº«ä¼šè§‚åŽæ„Ÿã€‚å‘¨æ˜Žè€å¸ˆæ˜¯æˆ‘çš„çˆ±è±†~ åšNLPçš„åº”è¯¥æ— äººä¸çŸ¥æ— äººä¸æ™“äº†å§ã€‚ä¹Ÿä¸æ‰“ç®—æŠ„ç€é›·é”‹ç½‘çš„ç¬”è®°äº†ï¼Œå…ˆä¸Šä¸€ç¯‡é›·é”‹ç½‘æ€»ç»“çš„ç¬”å½•ï¼šå‘¨æ˜Žï¼šè‡ªç„¶è¯­è¨€å¤„ç†çš„æœªæ¥ä¹‹è·¯ | CCF-GAIR 2019 é¢˜ç›®ï¼šè‡ªç„¶è¯­è¨€å¤„ç†çš„æœªæ¥ä¹‹è·¯ | æ¼”è®²äººï¼šå‘¨æ˜Žåˆ†äº«çš„å‰æœŸçš„NLPç§‘æ™®å’Œå‘å±•å²ç•¥è¿‡ï¼Œå‘¨æ˜Žè€å¸ˆå¯¹äºŽNLPçš„çŽ°çŠ¶å’Œæœªæ¥å‘å±•çš„çœ‹æ³•ï¼šè¿‡åŽ»40å¹´ï¼Œè‡ªç„¶è¯­è¨€åŸºæœ¬ä¸Šç»åŽ†äº†ä»Žè§„åˆ™åˆ°ç»Ÿè®¡ï¼Œåˆ°çŽ°åœ¨çš„ç¥žç»ç½‘ç»œã€‚ç›¸æ¯”è¿‡åŽ»ï¼Œç›®å‰å¯ä»¥è¯´æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æœ€é»„é‡‘çš„æ—¶æœŸï¼Œåœ¨å¾ˆå¤šé¢†åŸŸéƒ½å–å¾—äº†çªç ´æ€§çš„è¿›å±•ã€‚ä½†æˆ‘ä»¬å®¡æ…Žåœ°çœ‹åˆ°ç¥žç»ç½‘ç»œè‡ªç„¶è¯­è¨€å¤„ç†è¿‡åº¦ä¾èµ–è®¡ç®—èµ„æºå’Œæ•°æ®ï¼Œåœ¨å»ºæ¨¡ã€æŽ¨ç†å’Œè§£é‡Šæ–¹é¢è¿˜å­˜åœ¨è®¸å¤šçš„ä¸è¶³ã€‚ç›®å‰å­˜åœ¨çš„é—®é¢˜ï¼š ç¬¬ä¸€ä¸ªæ˜¯æ— ä¼‘æ­¢çš„è®¡ç®—èµ„æºçš„å†›å¤‡ç«žèµ›ã€‚è¿™ä¸ªæ¯”è¾ƒå¥½ç†è§£ï¼Œåœ¨ç›®å‰çš„NLPçš„æ¯”èµ›é‡Œï¼Œèµ„æºç›¸äº‰è¿˜æ˜¯ä¸€å¤§æ½®æµï¼Œå¤§å®¶å–œæ¬¢æŠŠæ³¨åŽ‹åœ¨æœºå™¨æ€§èƒ½æå‡ä¸Šã€‚å‘¨åšå£«åŽŸè¯ï¼šçŽ°åœ¨å¤§å®¶éƒ½ç”¨å¤§è§„æ¨¡çš„æœºå™¨è®­ç»ƒï¼ŒåŒæ ·çš„ç®—æ³•ï¼Œåªè¦è®­ç»ƒé€Ÿåº¦å¿«ï¼Œå°±å¯ä»¥å¿«é€Ÿè¿­ä»£ï¼Œç„¶åŽä½ çš„æ°´å¹³å°±æ¯”åˆ«äººé«˜ã€‚ä¸Žä¹‹åŒæ—¶ï¼Œå½“ç„¶ä¹Ÿç‰¹åˆ«è€—èµ„æºï¼Œè®¸å¤šæ¨¡åž‹ä¸€è®­ç»ƒå¯èƒ½è¦å¥½å‡ å¤©æˆ–è€…å¥½å‡ ä¸‡ç¾Žé‡‘ã€‚æœ‰æ—¶å€™å®ƒç®¡äº‹ï¼Œä½†æœ‰æ—¶å€™ä¹Ÿä¸ç®¡äº‹ã€‚ ç¬¬äºŒä¸ªæ˜¯è¿‡åº¦ä¾èµ–æ•°æ®ã€‚é¦–å…ˆä½ è¦æ ‡æ•°æ®ï¼Œæ ‡æ³¨çš„ä»£ä»·æ˜¯éžå¸¸å¤§çš„ã€‚å…¶æ¬¡ï¼Œæ•°æ®æœ‰éšå«æ­§è§†çš„é—®é¢˜ï¼Œé€šè¿‡æ•°æ®åˆ†æžï¼Œå¯èƒ½ä¼šå¾—åˆ°æ­§è§†æ€§çš„ç»“æžœã€‚å¦å¤–æ•°æ®æœ‰åå·®ï¼Œæ•°æ®åœ¨æ ‡æ³¨çš„æ—¶å€™è¯·äººæ ‡æ³¨ï¼Œäººéƒ½æ˜¯å·æ‡’çš„ï¼Œæƒ³æœ€ç®€å•çš„æ–¹æ³•åŽ»æ ‡æ³¨ï¼Œç»“æžœæ ‡æ³¨çš„æ•°æ®åƒç¯‡ä¸€å¾‹ï¼ŒåŸºäºŽè¿™æ ·çš„æ•°æ®å­¦çš„æ¨¡åž‹ä¹Ÿåªèƒ½è§£å†³æ ‡æ³¨çš„æ•°æ®ï¼Œæ‹¿åˆ°çœŸå®žä»»åŠ¡ä¸Šç”±äºŽè·Ÿä½ æ ‡æ³¨åˆ†å¸ƒä¸ä¸€æ ·ï¼Œæ‰€ä»¥æ ¹æœ¬ä¸å¥½ä½¿ã€‚æ¯”å¦‚è¯´æˆ‘ä»¬åšQ&amp;Aé—®ç­”ç³»ç»Ÿï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰çš„é—®ç­”é‡Œé¢éƒ½å‡è®¾æ˜¯ç¬¬ä¸€åï¼Œä½†åˆ°äº†æœç´¢å¼•æ“Žä¸Šæœ‰å¾ˆå¤šç®€å•çš„é—®é¢˜éƒ½è§£å†³ä¸å¥½ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰æ•°æ®éšç§ä¿æŠ¤ç­‰ç­‰é—®é¢˜ã€‚ å†çœ‹ç›®å‰ä½¿ç”¨ç¥žç»ç½‘ç»œå¤„ç†çš„ä¸‰ç§å…¸åž‹ä»»åŠ¡ï¼Œå¦‚æžœè§£å†³çš„å¥½ï¼Œè‡ªç„¶è¯­è¨€çš„ä»»åŠ¡å°±åŸºæœ¬OKäº†ï¼š Rich resource æ¯”å¦‚æœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼Œä¸Šä¸‹æ–‡ï¼Œè¿˜æœªèƒ½çœŸæ­£åšåˆ°æ­§ä¹‰æ¶ˆè§£ï¼Œäººç±»çŸ¥è¯†çš„å€Ÿé‰´ Low resource æ²¡ä»€ä¹ˆè¯­æ–™çš„ä»»åŠ¡ï¼Œå­¦èµ·æ¥å¾ˆéš¾ï¼Œå› æ­¤è¦å€ŸåŠ›ï¼š transfer learning è¿ç§»å­¦ä¹ ï¼ŒNLPçš„æ–°èŒƒå¼ï¼šé¢„è®­ç»ƒ+ç»†è°ƒï¼šæˆ‘ä»¬å¯ä»¥é’ˆå¯¹å¤§è§„æ¨¡çš„è¯­æ–™ï¼Œæå‰è®­ç»ƒå¥½ä¸€ä¸ªæ¨¡åž‹ï¼Œè¿™ä¸ªæ¨¡åž‹æ—¢ä»£è¡¨äº†è¯­è¨€çš„ç»“æž„ä¿¡æ¯ï¼Œä¹Ÿæœ‰å¯èƒ½ä»£è¡¨äº†æ‰€åœ¨é¢†åŸŸç”šè‡³å¸¸è¯†çš„ä¿¡æ¯ï¼Œåªä¸è¿‡æˆ‘ä»¬çœ‹ä¸æ‡‚ã€‚åŠ ä¸Šæˆ‘ä»¬æœªæ¥çš„é¢„å®šçš„ä»»åŠ¡ï¼Œè¿™ä¸ªä»»åŠ¡åªæœ‰å¾ˆå°çš„è®­ç»ƒæ ·æœ¬ï¼ŒæŠŠé€šè¿‡å¤§è®­ç»ƒæ ·æœ¬å¾—åˆ°çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œåšåˆ°å°è®­ç»ƒæ ·æœ¬ä¸Šï¼Œæ•ˆæžœå°±å¾—åˆ°äº†éžå¸¸å¥½çš„æå‡ã€‚ cross-lingual learning è·¨è¯­è¨€å­¦ä¹  unsupervised æ— ç›‘ç£ prior knowledge; human role å…ˆéªŒè§„åˆ™ï¼›å­—å…¸ï¼›äººçš„å¼ºåŒ–å­¦ä¹  åˆ©ç”¨ç§å­è¿›è¡Œè¿­ä»£å­¦ä¹ ï¼Œæ¯”å¦‚æˆ‘æœ‰ä¸€ä¸ªå°è¾žå…¸ï¼Œæœ‰å‡ æ¡è§„åˆ™ï¼Œæœ‰å‡ æ¡åŒè¯­ï¼Œæˆ‘èƒ½ä¸èƒ½ç”¨å®ƒå½“åšä¸€ä¸ªå¼•å­ï¼Œåšä¸€ä¸ªå†·å¯åŠ¨ï¼Œå¯åŠ¨ä¹‹åŽå†è¿­ä»£æ”¹è¿›ã€‚ Multi-turn task å¤šè½®ä»»åŠ¡ï¼Œä¾‹å¦‚æ™ºèƒ½å®¢æœï¼Œæ¶‰åŠè¯­ä¹‰åˆ†æžï¼ŒæŒ‡ä»£æ¶ˆè§£ï¼Œçœç•¥éƒ¨åˆ†è¡¥å……ç­‰ä»»åŠ¡ç›®å‰çš„æƒ…å†µ/åŠ£åŠ¿ï¼š ç¼ºä¹å¸¸è¯†&amp;æŽ¨ç†æŽ¨ç†æ˜¯è¦åšå¾ˆå¤šäº‹æƒ…ã€‚ç¬¬ä¸€æ˜¯è¦äº†è§£ä¸Šä¸‹æ–‡ï¼Œè¯´è¿‡ä»€ä¹ˆè¯ï¼Œç­”è¿‡ä»€ä¹ˆé—®é¢˜ï¼Œå¹²è¿‡ä»€ä¹ˆäº‹éƒ½è¦å­˜å‚¨èµ·æ¥ï¼Œè®°å¿†èµ·æ¥ã€‚ç¬¬äºŒæ˜¯å„ç§å„æ ·çš„çŸ¥è¯†è¦ç”¨èµ·æ¥ã€‚ç¬¬ä¸‰æ‰æ˜¯æŽ¨ç†çš„éƒ¨åˆ†ï¼Œè¿™é‡Œé¢æ¶‰åŠåˆ°è¯­ä¹‰åˆ†æžã€ä¸Šä¸‹æ–‡çš„æŒ‡ä»£æ¶ˆè§£ã€çœç•¥æ¶ˆè§£ã€‚æœ€åŽï¼Œè¿˜æœ‰å°±æ˜¯å¯è§£é‡Šçš„é—®é¢˜ï¼Œå¦‚æžœä½ çš„æŽ¨ç†ä¸å¯è§£é‡Šçš„è¯ï¼Œé‚£å°±æ²¡æœ‰äººä¼šç›¸ä¿¡ï¼Œå¯¼è‡´ä½ çš„ç³»ç»Ÿæ— æ³•è¿›è¡Œè¿›ä¸€æ­¥çš„æŽ¨è¿›ã€‚ å‰åŽä¸ä¸€è‡´ï¼šæ—¶é—´ã€ç©ºé—´ã€é€»è¾‘ä¸ä¸€è‡´ æœªæ¥çš„æ–¹å‘ï¼Œå¯è§£é‡Šçš„ï¼Œæœ‰çŸ¥è¯†çš„ï¼Œæœ‰é“å¾·çš„ï¼Œå¯è‡ªæˆ‘å­¦ä¹ çš„NLPï¼Œä»ŽçŽ°å­˜çš„å®žé™…ä»»åŠ¡å‡ºå‘ã€‚ å‘¨æ˜Žè€å¸ˆæ€»ç»“äº†NLPçš„æœªæ¥ä¹‹è·¯ä¸»è¦æœ‰6ä¸ªè§’åº¦éžå¸¸é‡è¦ï¼šè®¡ç®—æœºçš„èƒ½åŠ›ï¼Œæ•°æ®ï¼Œæ¨¡åž‹ï¼Œäººæ‰ï¼Œåˆä½œï¼Œåº”ç”¨ã€‚ å¾ˆå¤šå‘¨æ˜Žæ•™æŽˆæåˆ°çš„é—®é¢˜å…¶å®žæˆ‘ä¹‹å‰éƒ½æœ‰é‡åˆ°è¿‡ï¼Œä½†æ˜¯å½“æ—¶æƒ³æ³•å°±æ˜¯ï¼Œè¯¥æ€Žä¹ˆè§£å†³ï¼Œç»“æžœå°±æ˜¯èŠ±å¥½å¤§åŠ›æ°”éƒ½åªæå‡å¾ˆå°ç”šè‡³æ²¡æœ‰æå‡ï¼ŒåŽæ¥æ‰å‘çŽ°è¿™æ˜¯è¡Œä¸šå†…çš„é—®é¢˜ã€‚å› æ­¤åº”è¯¥å¤šå€Ÿé‰´ä»–äººåœ¨é‡åˆ°é—®é¢˜æ—¶çš„æƒ…å†µï¼Œå½“æ—¶æ— çŸ¥ä»¥ä¸ºæ˜¯åªæœ‰è‡ªå·±æ‰é‡åˆ°äº†è¿™äº›é—®é¢˜ï¼Œé’»åœ¨é‡Œé¢å°±ä¸å‡ºæ¥ï¼Œè¿™æ ·æ”¶æ•ˆç”šå¾®ã€‚ å†™åœ¨ä¼šåŽ è™½ç„¶NLPæ˜¯ç ”ç©¶å’Œå‘å±•äº†å¥½å¤šå¹´ï¼Œä½†æˆ‘è¿˜æ˜¯è§‰å¾—è¿™æ˜¯å¾ˆå¹´è½»çš„æŠ€æœ¯ï¼Œè°ˆä¸ä¸Šæˆç†Ÿï¼Œå› æ­¤è¿˜æœ‰å¾ˆå¤šå¯ä»¥ç ”ç©¶å’ŒæŽ¢ç´¢çš„åœ°æ–¹ï¼Œéš¾åº¦ä¹Ÿæ˜¯å¯æƒ³è€ŒçŸ¥çš„ï¼Œä½†æ˜¯èŽ«ååœ°å¯¹æœ‰æå‡ç©ºé—´çš„ä¸œè¥¿æ„Ÿå…´è¶£ï¼Œè¿™æ ·å¯¹åˆå­¦è€…ä¹Ÿæ¯”è¾ƒå‹å¥½ï¼Œè¿½èµ¶éš¾åº¦å°ã€‚ è¿™ç§ä¼šè®®æœ€å¥½æ˜¯å¸¦ç€é—®é¢˜åŽ»ï¼Œä¸ç„¶æ”¶èŽ·ä¸äº†ä»€ä¹ˆã€‚æ— è®ºå“ªç§å­¦ä¹ å½¢å¼ï¼Œæœ‰äººåˆ†äº«çš„åœºåˆé‡Œï¼Œé¦–å…ˆäº†è§£åˆ†äº«æ¦‚è¦å’Œåˆ†äº«æ–¹å‘ï¼Œå°†å…¶ä¸Žè‡ªå·±çš„çŸ¥è¯†ä½“ç³»åšåŒ¹é…ï¼Œåˆä¸ç›¸å…³åˆä¸æ„Ÿå…´è¶£çš„ï¼Œæ— è®ºäººå®¶åœ¨è®²ä»€ä¹ˆï¼Œå¬è€…éƒ½æ˜¯æµªè´¹æ—¶é—´ã€‚è€Œåˆšå¥½ä¸Žè‡ªå·±çš„ç ”ç©¶æ–¹å‘åŒ¹é…çš„åˆ†äº«ä¼šé‡Œï¼Œæœ€å¥½æ˜¯å¸¦ç€è‡ªå·±å·²æœ‰çš„æ€è€ƒè¿›åŽ»ï¼Œä¸ç„¶é”™è¿‡å°ä¸Šçš„äººåœ¨è®²ä»€ä¹ˆä¹Ÿæ˜¯åˆ†åˆ†é’Ÿçš„äº‹ã€‚ å°±æˆ‘æœ¬èº«æ¥è¯´ï¼Œå¾ˆæƒ³æœ‰äººå‘Šè¯‰æˆ‘åœ¨NLPé¢†åŸŸï¼Œåšä»€ä¹ˆæŠ€æœ¯æˆ–è€…åšä»€ä¹ˆé¡¹ç›®æ˜¯æ”¿æ²»æ­£ç¡®å­¦æœ¯æ­£ç¡®çš„é€‰æ‹©ï¼Œç„¶è€Œå¤§å®¶éƒ½å¿™ç€å‘å±•è‡ªå·±çš„é¡¹ç›®æ‰“è‡ªå·±çš„å¹¿å‘Šï¼Œæ²¡æœ‰äººæƒ³ä¸ºä½ çš„é€‰æ‹©è´Ÿè´£ï¼Œè‡³äºŽè‡ªå·±çœŸæ­£å…·ä½“è¦åšä»€ä¹ˆï¼Œè¿˜æ˜¯è¦é è‡ªå·±æ¥åˆ¤æ–­ã€‚ä¼šè®®ä¸Šåˆ†äº«æ¥åˆ†äº«åŽ»çš„ä¸œè¥¿ï¼Œç”šè‡³éƒ½æ˜¯åŠ¨åŠ¨æ‰‹æŒ‡è°·æ­Œç™¾åº¦çŸ¥ä¹Žèƒ½å‘Šè¯‰ä½ çš„ä¸œè¥¿ï¼Œé‚£ä¹ˆäº²ä¸´çŽ°åœºçš„æ„ä¹‰åˆ°åº•åœ¨å“ªé‡Œï¼Ÿæœ‰ä¸€ä¸ªæ°›å›´å¼ºè¿«è‡ªå·±æŽ¥æ”¶ä¿¡æ¯ï¼ŸNo idea yet. å†è¯´ç©¿ä¸€ç‚¹å°±æ˜¯ï¼Œæœ‰ä»€ä¹ˆä¸œè¥¿æ˜¯è¿™ä¸ªä¼šè®®èƒ½ç»™ä½ çš„è€Œæœç´¢å¼•æ“Žä¸èƒ½ç»™ä½ çš„ï¼Œè€Œä½ åœ¨ä¼šåœºæ‹¿åˆ°è¿™ä¸ªä¸œè¥¿äº†å—ï¼Ÿ æœ‰æ—¶å€™ï¼Œåšæ–°ä¸œè¥¿æ²¡æœ‰åšæœ‰ç”¨çš„ä¸œè¥¿æ¥çš„æœ‰æ„ä¹‰ã€‚æ‹›ä¸åœ¨æ–°ï¼Œç®¡ç”¨å°±è¡Œã€‚ Think Out of The Boxä¸‹é¢ç»“åˆè‡ªå·±çš„ç»åŽ†å’Œå·¥ä½œå†…å®¹èŠèŠNLPçš„ä¸€äº›è½åœ°æ–¹å‘å’Œè‡ªå·±æƒ³åšçš„ä¸œè¥¿ã€‚æ€è€ƒçš„æ–¹å‘å¤§è‡´ä»Žä¸‰ä¸ªæ–¹å‘ï¼š ç‰©è”ç½‘ä¸‹çš„NLPåº”ç”¨ è·¨é¢†åŸŸçš„NLPåº”ç”¨ NLP Trend 1. ç‰©è”ç½‘ä¸‹çš„NLPåº”ç”¨å› ä¸ºä»Žäº‹äºŽç‰©è”ç½‘çš„æ™ºèƒ½å®¶å±…è¡Œä¸šï¼Œå¯¹è¿™æ–¹é¢çš„åº”ç”¨è¿˜æ˜¯æ¯”è¾ƒæ„Ÿå…´è¶£çš„ã€‚ä¸»è¦èŠèŠä¸¤ä¸ªå·²ç»åœ¨å·¥ä¸šç•Œå‘è‚²çš„æ¯”è¾ƒå¥½çš„é¡¹ç›®ï¼šè¯­éŸ³åŠ©æ‰‹å’Œæ™ºèƒ½å®¢æœ ã€‚ç”šè‡³ä¸¤è€…æœ‰å¾ˆå¤šå¯ä»¥äº¤å‰çš„åœ°æ–¹ã€‚ è¯­éŸ³äº¤äº’ / è¯­éŸ³åŠ©æ‰‹é¦–å…ˆï¼Œä¸ºä»€ä¹ˆè¦åšè¯­éŸ³åŠ©æ‰‹å‘¢ï¼Œå°¤å…¶åœ¨å„å¤§åŽ‚å·²ç»å‡ºå“ä¼˜ç§€æˆç†Ÿçš„æ™ºèƒ½éŸ³ç®±/è¯­éŸ³åŠ©æ‰‹ï¼ˆå°çˆ±åŒå­¦ï¼Œå¾®è½¯å°å†°ï¼ŒAlexaï¼ŒSiriç­‰ç­‰ï¼‰ä¹‹åŽï¼Œä¸ºä»€ä¹ˆè¿˜è¦è‡ªå·±åšå‘¢ï¼Ÿè¿™é‡Œé¦–å…ˆæƒ³æ¾„æ¸…ï¼Œè¿™é‡Œæåˆ°çš„åšè¯­éŸ³åŠ©æ‰‹æ˜¯åœ¨å¾ˆåž‚ç›´é¢†åŸŸçš„ä¸€ä¸ªå°åŠŸèƒ½æ¨¡å—ã€‚å°±æ˜¯åœ¨æ™ºèƒ½å®¶å±…å°å®¶ç”µç”Ÿæ€çš„è¯­éŸ³äº¤äº’åŠŸèƒ½ï¼Œä¾‹å¦‚æŽ§åˆ¶è®¾å¤‡ï¼Œé¢„çº¦æ“ä½œï¼Œæˆ–æ˜¯è®¾å¤‡è”åŠ¨è‡ªåŠ¨åŒ–è®¾ç½®ç­‰ç­‰ã€‚è€Œä¸æ˜¯ä»Žå¤´å¼€å§‹é‡æ–°åšä¸€ä¸ªå°çˆ±åŒå­¦ï¼Œé‚£æ ·æ˜¯ä»¥åµå‡»çŸ³çš„è‡ªæ€è¡Œä¸ºã€‚ å› ä¸ºæœ¬èº«è¿™äº›ç½‘å…³ã€å¼€å…³ã€ä¼ æ„Ÿå™¨ç­‰è®¾å¤‡æ˜¯å…¬å¸ç”Ÿäº§çš„ä¸»åŠ›äº§å“ï¼Œå› æ­¤åœ¨ä¸Žç”¨æˆ·æœ€ç›´æŽ¥çš„è¯­éŸ³æ²Ÿé€šäº¤äº’æ—¶ï¼Œåº”è¯¥æŠŠçœŸå®žçš„è¯­éŸ³äº¤äº’æ–‡æœ¬æ•°æ®æ‹¿åˆ°ï¼Œæ ¹æ®å®žé™…åœºæ™¯å°†äº¤äº’åšçš„æ›´åŠ æ™ºèƒ½ä¾¿æ·ã€‚ éœ€è¦è¯´æ˜Žçš„å‡ ç‚¹ï¼š åž‚ç›´é¢†åŸŸï¼Œåœ¨æ™ºèƒ½å®¶å±…ï¼Œè¯­éŸ³äº¤äº’åº”è¯¥æ˜¯äººæœºäº¤äº’çš„ä¸»æµï¼Œä½œä¸ºä¸€å®¶æ™ºèƒ½å®¶å±…ç”Ÿæ€å…¬å¸ï¼Œè¯­éŸ³äº¤äº’æ˜¯å¿…ä¸å¯å°‘çš„æŠ€æœ¯ç§¯ç´¯ï¼Œè€Œè¿™äº›æ–‡æœ¬æ•°æ®æˆ–è€…æ˜¯è®¾å¤‡ä¿¡æ¯ï¼Œéƒ½æ˜¯å…¶å®ƒå…¬å¸å¾ˆå°‘æœ‰çš„ã€‚ åšè¾…åŠ©ï¼Œåªåšè‡ªå·±è¯¥åšçš„ï¼Œåªåšè‡ªå·±æ“…é•¿çš„ï¼ŒåµŒå…¥å¼è¯­éŸ³äº¤äº’ä¸èŠ±åŠ›æ°”åœ¨èŠå¤©ä¸Šï¼Œè€Œæ˜¯èšç„¦åœ¨äº§å“ä½¿ç”¨ï¼ŒæŽ§åˆ¶ï¼Œè”åŠ¨ç­‰åŠŸèƒ½ä¸Šã€‚ä½¿å¾—åŽŸå…ˆåªèƒ½åœ¨appä¸Šè§¦å±äº¤äº’å¢žåŠ æ–°çš„äº¤äº’æ–¹å¼ã€‚ å‰æœŸå¯èƒ½åŸºäºŽå¤§é‡çš„è§„åˆ™çº¦æŸï¼Œä¸ºäº†é“ºå¼€äº¤äº’æ¸ é“ï¼Œå¥½ç”¨æœ‰ç”¨å°±è¡Œã€‚ å¦‚æ­¤ä»¥æ¥ï¼Œå½“ä½¿ç”¨çŽ‡ä¸Šå‡ï¼Œå¸‚åœºæ‰©å¼ ï¼Œå¯æ”¶åˆ°å¤§é‡çš„äº§å“å‡çº§éœ€æ±‚ç­‰åé¦ˆæ—¶ï¼Œå¯ä»¥å¢žåŠ è¯­éŸ³åŠ©æ‰‹çš„è¾…åŠ©åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š ä½¿ç”¨ä»‹ç» é¢„çº¦å®¶ç”µ æŽ§åˆ¶å®¶ç”µ æ™ºèƒ½é—®ç­” èŠå¤© è‡ªåŠ¨åŒ–é…ç½® æ™ºèƒ½é—®ç­” / æœç´¢å¼•æ“Žè¿˜æ˜¯ä¸€æ ·çš„ï¼Œé¦–å…ˆæ˜Žç¡®ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªæ™ºèƒ½å®¢æœï¼Œåœ¨å…¶å®ƒå…¬å¸å¯èƒ½å·²ç»æœ‰æ›´æˆç†Ÿçš„è½åœ°æŠ€æœ¯çš„æƒ…å†µä¸‹ï¼Œä¸ºä»€ä¹ˆï¼ŸWhatâ€™s the difference here? é¦–å…ˆå› ä¸ºç‰©è”ç½‘çš„ç§‘æ™®è¿˜éœ€è¦ä¸€æ®µæ—¶é—´ï¼Œæ¯”å¦‚æ™ºèƒ½å®¶å±…çš„æ™®åŠï¼Œè¿˜éœ€è¦3-5å¹´ï¼Œè€Œæ™®åŠçš„è¿‡ç¨‹ä¸­å°‘ä¸äº†è¯¢é—®äº§å“çš„ä¿¡æ¯ï¼Œä½¿ç”¨çš„æ–¹æ³•ï¼Œèƒ½å®žçŽ°çš„åœºæ™¯ç­‰ç­‰ã€‚è¿™äº›éƒ½æ˜¯ä¸Žå…¬å¸å¼ºç›¸å…³çš„ä¸šåŠ¡ï¼Œä¹Ÿåˆ©ç”¨å…¬å¸äº§å“çš„æŽ¨å¹¿ã€‚æœ‰äº›äº‹æƒ…ä¸æ˜¯åˆ«äººåœ¨åšäº†è€Œæ”¶æ‰‹ï¼Œå†³å®šè¯¥ä¸è¯¥åšè¿™ä»¶äº‹çš„åŸºç¡€ï¼Œåº”è¯¥æ˜¯ä½ åœ¨ä»€ä¹ˆä½ç½®ä¸Šï¼Œä½ æœ‰æ²¡æœ‰è¿™ä¸ªä½¿å‘½åŽ»å°†è¿™ä»¶äº‹åšå¥½ã€‚ ç¬”è®°å¤‡æ³¨ï¼š å¯¹æ£€ç´¢queryçš„ç†è§£ æ–‡æœ¬æ”¹å†™ çº é”™ ç”¨æˆ·æœç´¢æ„å›¾ çŸ¥è¯†å›¾è°±/çŸ¥è¯†æŒ–æŽ˜ï¼Œç”¨äºŽæå‡æ™ºèƒ½é—®ç­”ä½“éªŒ ç›´æŽ¥åšè¯­éŸ³å®¢æœæœºå™¨äºº ï¼Œæ¶‰åŠå¤šè½®å¯¹è¯ã€‚ è®¢å•å¤„ç†ï¼ŒçœŸå®žåœºæ™¯ äº§å“æŽ¨è é…é€æœåŠ¡æŸ¥è¯¢ï¼Œäº§å“ä½¿ç”¨ç§‘æ™® 2. è·¨é¢†åŸŸçš„NLPåº”ç”¨è·¨é¢†åŸŸçš„åº”ç”¨å¯èƒ½æ˜¯ä¸Žå…¶å®ƒæ–¹å‘ç»“åˆï¼Œæ¯”å¦‚ä¸ŽCVç»“åˆçš„çœ‹å›¾è¯´è¯ã€‚ä½†æ˜¯è¿™ç›¸å½“äºŽï¼Œå¤„ç†äºŒæ‰‹ä¿¡æ¯ï¼ŒCVå›¾åƒèŽ·å–ç¬¬ä¸€æ‰‹ä¿¡æ¯ï¼Œå†æ ¹æ®å›¾åƒèŽ·å–çš„ä¿¡æ¯é‡Œå¤„ç†ç¬¬äºŒæ‰‹ä¿¡æ¯ã€‚æš‚æ—¶é™¤äº†å¥½çŽ©æ²¡æœ‰æ‰¾åˆ°ç‰¹åˆ«å¼ºçš„åº”ç”¨å®žçŽ°ç†ç”±ï¼Œå› ä¸ºè¯­éŸ³äº¤äº’å·²ç»æ˜¯å¾ˆå¼ºçš„äº¤äº’è¡Œä¸ºäº†ã€‚ å¦ä¸€ä¸ªè¯´æ³•å¯ä»¥æ˜¯è·¨éƒ¨é—¨çš„åº”ç”¨ï¼Œå‘å…¬å¸çš„å…¶å®ƒéƒ¨é—¨æä¾›å¹³å°æœåŠ¡ï¼š å•†å“æœç´¢ï¼Œå•†å“æŽ¨è éœ€æ±‚ç®¡ç†/other å¯è§†åŒ– 3. NLP Trend å¹³å°æ­å»ºæ¨¡å— æ‰“æ¯”èµ› Fine-tuneï¼Œtransfer learning æš‚æ—¶æƒ³åˆ°è¿™ä¹ˆå¤šï¼Œé‚£å°±å…ˆå†™åˆ°è¿™é‡Œå§ã€‚]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>ccf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word2vec]]></title>
    <url>%2F2019%2F06%2F19%2Fword2vec%2F</url>
    <content type="text"><![CDATA[ç­‰æœ‰ç©ºæ•´ç†ï¼Œå…ˆæ”¾çµæ„Ÿé“¾æŽ¥ã€‚ä»¥åŠï¼Œword2vecå°±æ˜¯åˆ©ç”¨å•è¯çš„çŽ¯å¢ƒåŽ»è¡¨è¾¾ä¸€ä¸ªå•è¯ã€‚å¦‚æžœæŸäº›å•è¯å¸¸å‡ºçŽ°çš„çŽ¯å¢ƒæ˜¯å¾ˆåƒçš„ï¼Œé‚£ä¹ˆå®ƒä»¬çš„å‘é‡å°±å¾ˆæŽ¥è¿‘ã€‚ä¹Ÿä¸æ˜¯æ‰€è°“çš„æ— ç›‘ç£ï¼Œæ˜¯æœ‰targetçš„ï¼Œåªä¸è¿‡è¿™ä¸ªtargetå¯ä»¥ç”¨ä¸€æ¡è§„åˆ™å½¢æˆï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚åœ¨æˆ‘çœ¼é‡Œï¼Œè¿™è¿˜æ˜¯æœ‰ç›‘ç£çš„è®­ç»ƒã€‚æ¯•ç«Ÿæ‰€æœ‰ç¥žç»ç½‘ç»œéƒ½æ˜¯æœ‰ç›‘ç£çš„å­¦ä¹ ã€‚ links å›¾è§£Word2vec Word2vecåˆæŽ¢ çŸ¥ä¹Žï¼šword2vecæ˜¯å¦‚ä½•å¾—åˆ°è¯å‘é‡çš„ï¼Ÿ gensim - Word2vec embeddings An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®ºæ–‡ç¬”è®° - Attention Is All You Needï¼ˆBERTåŽŸç†ï¼‰]]></title>
    <url>%2F2019%2F06%2F19%2Fattention-is-all-you-need%2F</url>
    <content type="text"><![CDATA[é¦–å…ˆï¼Œè®ºæ–‡é“¾æŽ¥ï¼šAttention Is All You Need æŒ‰ä¸‹é¢çš„é“¾æŽ¥é¡ºåºçœ‹å°±å¯ä»¥äº†ï¼Œåº”è¯¥éƒ½èƒ½çœ‹æ‡‚ã€‚ç­‰æœ‰ç©ºå†æ•´ç†æˆæ–‡ç« ã€‚ å¿…çœ‹çš„å‚è€ƒé“¾æŽ¥ï¼š æœ€æœ€ç²¾å½©çš„è®ºæ–‡è§£è¯»/å›¾è§£ï¼šThe Illustrated Transformer çŸ¥ä¹Žçš„ä¸€ç¯‡Transformerç¬”è®° å“ˆä½›nlpç»„ç”¨pytorchå®žçŽ°çš„Transformerä»£ç  è‹ç¥žçš„ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰ Attention Mechanismè¯¦ç»†ä»‹ç»ï¼šåŽŸç†ã€åˆ†ç±»åŠåº”ç”¨ ä¸‹é¢ä»‹ç»ä¸€ä¸‹åœ¨NLPä¸­å¸¸ç”¨attentionçš„è®¡ç®—æ–¹æ³•ï¼ˆé‡Œé¢å€Ÿé‰´äº†å¼ ä¿Šæž—åšå£«â€æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶(2017ç‰ˆ)â€é‡Œçš„ä¸€äº›å›¾ï¼‰ã€‚Attentionå‡½æ•°çš„æœ¬è´¨å¯ä»¥è¢«æè¿°ä¸ºä¸€ä¸ªæŸ¥è¯¢ï¼ˆqueryï¼‰åˆ°ä¸€ç³»åˆ—ï¼ˆé”®key-å€¼valueï¼‰å¯¹çš„æ˜ å°„ï¼Œå¦‚ä¸‹å›¾ã€‚ åœ¨è®¡ç®—attentionæ—¶ä¸»è¦åˆ†ä¸ºä¸‰æ­¥ï¼Œç¬¬ä¸€æ­¥æ˜¯å°†queryå’Œæ¯ä¸ªkeyè¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—å¾—åˆ°æƒé‡ï¼Œå¸¸ç”¨çš„ç›¸ä¼¼åº¦å‡½æ•°æœ‰ç‚¹ç§¯ï¼Œæ‹¼æŽ¥ï¼Œæ„ŸçŸ¥æœºç­‰ï¼›ç„¶åŽç¬¬äºŒæ­¥ä¸€èˆ¬æ˜¯ä½¿ç”¨ä¸€ä¸ªsoftmaxå‡½æ•°å¯¹è¿™äº›æƒé‡è¿›è¡Œå½’ä¸€åŒ–ï¼›æœ€åŽå°†æƒé‡å’Œç›¸åº”çš„é”®å€¼valueè¿›è¡ŒåŠ æƒæ±‚å’Œå¾—åˆ°æœ€åŽçš„attentionã€‚ç›®å‰åœ¨NLPç ”ç©¶ä¸­ï¼Œkeyå’Œvalueå¸¸å¸¸éƒ½æ˜¯åŒä¸€ä¸ªï¼Œå³key=valueã€‚]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>attention</tag>
        <tag>bert</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Json In Python]]></title>
    <url>%2F2019%2F04%2F26%2Fjson-in-python%2F</url>
    <content type="text"><![CDATA[çŽ©æ•°æ®çš„æ—¶å€™ï¼Œå…ä¸äº†ä¸ŽjsonæŽ¥è§¦ã€‚è¿™ä¸¤å¤©åœ¨çŽ©æ¼«å¨çš„APIï¼Œé‡‡é›†æ•°æ®å’Œå­˜å‚¨æ•°æ®æ—¶ï¼Œå› ä¸ºæˆ‘æ²¡æœ‰MongoDBæ•°æ®åº“ï¼Œå…ä¸äº†è¦å°†Json(dict)æ•°æ®æ ¼å¼ä¿å­˜ï¼Œå†å°†å…¶è½¬æ¢æˆpythonçš„å„ç§æ•°æ®æ ¼å¼ï¼šdataframeæˆ–dictç­‰ã€‚åœ¨æ­¤è®°å½•å‡ ä¸ªåœºæ™¯ï¼Œå’Œè§£å†³åŠžæ³•ï¼š Jsonå•æ¡ä¸¾ä¾‹ï¼šå¤šdictåµŒå¥—ï¼Œæ¨¡æ‹Ÿå®žé™…æŽ¥è§¦åˆ°çš„æ•°æ®ç±»åž‹1dct = &#123;'aaa': 'bbb', 'ccc': &#123;'ff': 'gg', 'dd': 'ee', 'hh': &#123;'i': 'j'&#125;&#125;&#125; CASE 1ï¼šé€æ¡èŽ·å¾—dictæ•°æ®æ—¶ï¼Œï¼ˆé€æ¡ï¼‰å†™è¿›data1.json12345678910# å¯¹ Marvel APIè¯·æ±‚æ•°æ®æ—¶ï¼Œæ˜¯é€æ¡èŽ·å–dictçš„ï¼Œæ­¤å¤„â€˜dctâ€™è¡¨ç¤ºé€æ¡èŽ·å–çš„å†…å®¹dct = &#123;'aaa': 'bbb', 'ccc': &#123;'ff': 'gg', 'dd': 'ee', 'hh': &#123;'i': 'j'&#125;&#125;&#125;with open(r'data1.json','w') as file: # æ–°å»º data1.jsonæ–‡ä»¶ for i in [1,2,3,4]: # [1,2,3,4]è¿™é‡Œå†™4æ¡ï¼Œåªåšç¤ºèŒƒ js = json.dumps(dct) # å°†pythonæ ¼å¼çš„dictç¼–ç æˆjsonéœ€è¦çš„æ ¼å¼ file.write(js) # é€æ¡å†™å…¥ file.write('\n') # é€æ¡æ¢è¡Œå†™å…¥file.close() CASE 2ï¼šï¼ˆé€æ¡ï¼‰è¯»å–data1.jsoné‡Œçš„å†…å®¹12345678910# data1.json é‡Œçš„é€æ¡å†…å®¹éƒ½æ˜¯ä»¥ str å½¢å¼å­˜å‚¨çš„# ç›¸å½“äºŽè¦å°† str(='dict') è½¬æ¢æˆ dictimport astff = open(r'data1.json','r')for q in ff: q = ast.literal_eval(q) print(q) # è¿™é‡Œçš„qå·²ç»æ˜¯pythonçš„dictæ ¼å¼ï¼Œå¯ä»¥æŒ‰keyå–valueff.close() è¿™é‡Œæˆ‘å°è¯•äº†å„ç§ json.load, pd.read_jsonéƒ½æ²¡æœ‰æˆåŠŸï¼Œæœ€åŽç”¨astä¸€æ­¥è§£å†³ã€‚ CASE 3ï¼šé€æ¡èŽ·å¾—dictæ•°æ®æ—¶ï¼Œï¼ˆé€æ¡ï¼‰å¯¹dictè¿›è¡Œå˜æ¢å†å†™è¿›data2.json12345678910# æœ‰æ—¶éœ€è¦å¯¹èŽ·å–çš„dictæ•°æ®å†ä½œå¤„ç†å†ä¿å­˜dct = &#123;'aaa': 'bbb', 'ccc': &#123;'ff': 'gg', 'dd': 'ee', 'hh': &#123;'i': 'j'&#125;&#125;&#125;with open(r'data2.json','w') as file: # æ–°å»º data2.jsonæ–‡ä»¶ for i in [1,2,3,4]: # [1,2,3,4]è¿™é‡Œå†™4æ¡ï¼Œåªåšç¤ºèŒƒ js = json.dumps(&#123;str(i): dct&#125;) file.write(js) # é€æ¡å†™å…¥ file.write('\n') # é€æ¡æ¢è¡Œå†™å…¥file.close() CASE 4ï¼šå¯¹data2.jsonçš„jsonæ–‡ä»¶è¿›è¡Œå†…å®¹è¯»å–12345678# æ­¤æ—¶ç”¨astå´æŠ¥é”™äº†ï¼Œå› æ­¤æ¢ yamlimport yamlff = open(r'data2.json', 'r')for q2 in ff: q2 = yaml.load(q2) print(q2) # è¿™é‡Œçš„q2å·²ç»æ˜¯pythonçš„dictæ ¼å¼ï¼Œå¯ä»¥æŒ‰keyå–valueff.close()]]></content>
      <tags>
        <tag>python</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¼«å¨è‹±é›„å›¾è°± - gephi]]></title>
    <url>%2F2019%2F04%2F25%2Fgephi%2F</url>
    <content type="text"><![CDATA[åœ¨Githubå»ºäº†ä¸€ä¸ªrepoï¼Œä¸»è¦ç”¨æ¥çŽ© Marvelæ¼«å¨ çš„æ•°æ® [github] ã€‚åŽŸæ„æ˜¯æƒ³ç”¨è¿™äº›åšä¸ªçŸ¥è¯†å›¾è°±çš„å°demoï¼Œé¡ºä¾¿åšåšå›¾è°±å¯è§†åŒ–ã€‚æ˜¨å¤©çœ‹åˆ°ä¸€ä¸ªåŸºäºŽgephiçš„æ¼«å¨è§’è‰²å›¾è°±çš„æ•™ç¨‹[tutorial] ï¼Œä¸»è¦æ•™ä½ æ€Žä¹ˆç”»å‡ºä¸‹é¢è¿™å¼ å›¾ï¼Œæƒ³å­¦çš„å¯ä»¥ç»§ç»­çœ‹ä¸‹åŽ»ï¼š ç”»å›¾å¤§æ¦‚éœ€è¦å“ªäº›ä¸œè¥¿ï¼š gephi æ‰€éœ€çš„æ•°æ® ä½¿ç”¨ gephi ç”Ÿæˆè‹±é›„å…³ç³»å›¾è°± èŽ·å–æ•°æ® æ–¹æ³•1 - çŽ°æˆçš„dataï¼šinode.csv ï¼Œiedge.csv æ–¹æ³•2 - è‡ªå·±èŽ·å–ï¼šmarvel_gephi.pyåŽŸpoæ˜¯æŠŠèŽ·å–çš„æ•°æ®å­˜å‚¨åœ¨MongoDBï¼Œæœ‰æ•°æ®åº“çš„å¯ä»¥å‚è€ƒæ•™ç¨‹åŽŸç  [tutorial] æ–¹æ³•3 - è‡ªå·±èŽ·å–å…¶ä»–æ•°æ® ç›¸å…³å‚æ•°è§£é‡Šï¼š èŠ‚ç‚¹ï¼ˆnodeï¼‰ä¹Ÿå°±æ˜¯é¡¶ç‚¹ï¼ˆverticalï¼‰ã€‚ä¸¤ä¸ªèŠ‚ç‚¹ç›¸è¿žçš„éƒ¨åˆ†ç§°ä¸ºè¾¹ï¼ˆedgeï¼‰ã€‚ inodeæ–‡ä»¶ä¸­ï¼šid ä¸ºè‹±é›„è§’è‰²id ï¼Œå³å›¾ä¸­çš„èŠ‚ç‚¹ï¼Œname ä¸ºè‹±é›„è§’è‰²åå­—ï¼Œid ä¸Ž name ä¸€ä¸€å¯¹åº”ï¼›weight ä¸ºæ¯ä¸€ä¸ªè‹±é›„ç›¸å…³çš„æ‰€æœ‰æ•…äº‹æ•°é‡ï¼Œå³èŠ‚ç‚¹çš„å¤§å°ï¼ˆæƒé‡ï¼‰ã€‚ iedge æ–‡ä»¶ä¸­ï¼šsource ä¸Ž target ä¸ºæº id ä¸Žç›®æ ‡ idï¼Œè¡¨ç¤ºä¸¤ä¸ªè§’è‰²ä¹‹é—´çš„è”ç³»ï¼Œå³å›¾ä¸­çš„è¾¹ï¼Œæœ¬æµ‹è¯•ä¸­å› é€‰æ‹©æ— å‘è¾¹ï¼Œæ‰€ä»¥ source ä¸Ž target ä¸è¡¨ç¤ºæ–¹å‘è¾¹è¡¨ç¤ºè¾¹ï¼ˆæ— å‘ï¼‰ï¼›weight ä¸ºåŒä¸€æ•…äº‹ä¸­å‡ºçŽ°è¿™ä¸¤ä¸ªè‹±é›„çš„æ•…äº‹æ•°é‡ï¼Œå³å›¾ä¸­è¾¹çš„å¤§å°ï¼ˆæƒé‡ï¼‰ã€‚ Gephi ä½¿ç”¨gephiå°±æ˜¯ä¸€ä¸ªç”»å›¾å·¥å…·ï¼Œç”»è¿™ç§å…³ç³»å›¾è°±ç‰¹åˆ«å¥½çœ‹ã€‚ä¸‹è½½ä¼šé‡åˆ°äº›çŽ¯å¢ƒé—®é¢˜ï¼Œä¸ªäººæ„Ÿè§‰é è¿æ°”å§ï¼Œå®žåœ¨ä¸è¡Œå°±æ¢ä¸ªçŽ¯å¢ƒè·‘ã€‚ æœ¬æ¬¡ä½¿ç”¨è¾“å…¥ gephi æ–‡æ¡£ï¼šinode.csv, iedge.csv æ­¥éª¤ï¼š ä»Žã€æ–‡ä»¶ã€‘â€”&gt;ã€æ‰“å¼€æ–‡ä»¶ã€‘ä¸­å¯¼å…¥èŠ‚ç‚¹æ–‡ä»¶ä¸Žè¾¹æ–‡ä»¶ï¼Œæ³¨æ„å¯¼å…¥è¾¹è¡¨æ ¼æ—¶ï¼Œé€‰æ‹©æ·»åŠ åˆ°ã€å·²å­˜åœ¨çš„å·¥ä½œç©ºé—´ã€‘ï¼ˆAppend to existing workspaceï¼‰ ã€‚ é€‰æ‹©ã€å¸ƒå±€ã€‘â€”&gt;ã€é€‰æ‹©ä¸€ä¸ªå¸ƒå±€ã€‘â€”&gt;ã€Fruchterman Reingoldã€‘ ï¼Œè¿›è¡Œç›¸å…³ç®—æ³•è¿ç®—ã€‚ é€‰æ‹©ã€ç»Ÿè®¡ã€‘â€”&gt;ã€è¿è¡Œæ¨¡å—åŒ–ã€‘ï¼Œè®¡ç®— modularity_class æ•°æ® å¤–è§‚4.1 é€‰æ‹©ã€å¤–è§‚ã€‘â€”&gt;ã€èŠ‚ç‚¹ã€‘+ã€é¢œè‰²ã€‘â€”&gt;ã€Partitionã€‘â€”&gt;ã€Modularity Classã€‘ï¼šåˆ©ç”¨åˆšåˆšçš„æ¨¡å—åŒ–æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚4.2 é€‰æ‹©ã€å¤–è§‚ã€‘â€”&gt;ã€èŠ‚ç‚¹ã€‘+ã€å¤§å°ã€‘â€”&gt;ã€Rankingã€‘â€”&gt;ã€Weightã€‘: ä½¿ç”¨ä¼ å…¥çš„ weightï¼ˆæ•…äº‹æ•°ï¼‰å†³å®šèŠ‚ç‚¹å¤§å°ã€‚4.3 é€‰æ‹©ã€æ•°æ®èµ„æ–™ã€‘â€”&gt;ã€æ•°æ®è¡¨æ ¼ã€‘â€”&gt;ã€å¤åˆ¶æ•°æ®åˆ°å…¶å®ƒåˆ—ã€‘ï¼šå°† name åˆ—å¤åˆ¶åˆ° Label åˆ—ã€‚4.4 é€‰æ‹©ã€å›¾ã€‘â€”&gt;ã€Tã€‘å³å¯æ˜¾ç¤ºæ ‡ç­¾ã€‚4.5 åœ¨ã€é¢„è§ˆã€‘å¤„è‡ªå·±è°ƒæ•´å¥½éœ€è¦çš„æ•°æ®åŽï¼Œç‚¹å‡»ã€SVG/PDF/PNGã€‘ï¼Œå³å¯å¯¼å‡ºå›¾å½¢ã€‚ 4.6 è¿˜æœ‰æ›´å¤šçš„ã€è¿‡æ»¤ã€‘ï¼Œã€ç»Ÿè®¡ã€‘ï¼Œã€é¢„è§ˆã€‘çš„ç›¸å…³åŠŸèƒ½ï¼Œå°±ç”±å¤§å®¶æ…¢æ…¢æŽ¢ç´¢äº†ã€‚ å›¾ç‰‡å‚è€ƒåŠéƒ¨åˆ† gephi è®¾ç½®ã€‚ èŠ‚ç‚¹æ•°æ®ï¼š è¾¹æ•°æ®ï¼š å¤–è§‚è®¾ç½®å½“åœ¨ã€ç»Ÿè®¡ã€‘å¤„ ç‚¹å‡»ã€è¿è¡Œæ¨¡å—åŒ–åŽã€‘ï¼Œæ­¤å¤„å‡ºçŽ°ã€Modularity Classã€‘ï¼š ã€layoutã€‘ï¼ˆå¸ƒå±€ï¼‰è®¾ç½®ï¼Œå¸ƒå±€ç®—æ³•é€‰æ‹©ï¼šã€Fruchterman Reingoldã€‘ï¼š ã€ç»Ÿè®¡ã€‘éƒ¨åˆ†-ç‚¹å‡»ã€è¿è¡Œæ¨¡å—åŒ–ã€‘ï¼š æ•ˆæžœå›¾]]></content>
      <tags>
        <tag>gephi</tag>
        <tag>marvel</tag>
        <tag>data visualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[çŸ¥è¯†å›¾è°±]]></title>
    <url>%2F2019%2F04%2F23%2Fknowledge-graph-intro%2F</url>
    <content type="text"><![CDATA[æŒ–ä¸ªå‘å…ˆã€‚æœ€è¿‘å¯¹çŸ¥è¯†å›¾è°±æœ‰ä¸ªå…¥é—¨çº§çš„äº†è§£ï¼Œå¸Œæœ›ä»¥åŽè¿˜ä¼šæœ‰é¡¹ç›®ç»§ç»­æŽ¢ç©¶ã€‚å¹´åˆå®žè·µäº†ä¸‹æ€Žä¹ˆæ­å»ºä¸€ä¸ªæœ€åŸºæœ¬çš„çŸ¥è¯†å›¾è°±ï¼Œé¡ºä¾¿å°è¯•äº†ä¸‹ä½¿ç”¨Neo4j æ­å»ºå›¾æ•°æ®åº“ï¼Œåœ¨æ­¤ç¨ä½œè®°å½•ã€‚å…¥é—¨é€‰æ‰‹å¯ä»¥çœ‹è¿™ç¯‡ç§‘æ™®å¸–ï¼šçŸ¥è¯†å›¾è°±çš„æŠ€æœ¯ä¸Žåº”ç”¨ ä»‹ç»çŸ¥è¯†å›¾è°±åˆ°åº•æ˜¯ä¸ªä»€ä¹ˆä¸œè¥¿ï¼Ÿè¯´åˆ°åº•å°±æ˜¯æ‹¥æœ‰ä¸€å®šçŸ¥è¯†ï¼Œå¯ä»¥è¿›è¡Œç›¸å…³çŸ¥è¯†é—®ç­”çš„ç³»ç»Ÿï¼Œè¿™ä¸ªç³»ç»Ÿå¯ä»¥ä¸åœåœ°è¾“å…¥çŸ¥è¯†ï¼Œä¹Ÿå¯ä»¥è¾“å‡ºçŸ¥è¯†ã€‚å…¶å®žï¼Œæ¯ä¸ªäººéƒ½æ˜¯ä¸€ä¸ªè¡Œèµ°çš„çŸ¥è¯†å›¾è°±ã€‚æˆ‘ä»¬ä»Žå°å°±è¢«é—®â€œå‡ å²å•¦ï¼Ÿâ€ï¼Œâ€œå«ä»€ä¹ˆåå­—å‘€ï¼Ÿâ€ï¼Œå½“æˆ‘ä»¬è¿›è¡Œå›žç­”æ—¶å°±æ˜¯çŸ¥è¯†è¾“å‡ºçš„è¿‡ç¨‹ã€‚ æœ¬è´¨ä¸ŠçŸ¥è¯†å›¾è°±çš„å»ºç«‹æœ‰å¦‚ä¸‹å‡ ä¸ªè¿‡ç¨‹ï¼šçŸ¥è¯†å­˜å‚¨ã€çŸ¥è¯†èžåˆã€çŸ¥è¯†éªŒè¯ã€çŸ¥è¯†è®¡ç®—ã€‚ çŸ¥è¯†å­˜å‚¨ï¼š ä¿¡æ¯èŽ·å–åŠå­˜å‚¨çš„è¿‡ç¨‹ï¼Œæ¯”å¦‚3å²çš„æ—¶å€™å­¦ä¼šèƒŒå”è¯—ï¼Œ8å²çš„æ—¶å€™å­¦ä¼šåŠ å‡ä¹˜é™¤ï¼Œ15å²å¼€å§‹å­¦ç‰©ç†ï¼Œå¯ä»¥æºæºä¸æ–­åœ°èŽ·å–ä¸åŒçš„çŸ¥è¯†ï¼Œè¯­æ–‡ã€æ•°å­¦ã€è‹±æ–‡ç­‰ç­‰ã€‚ çŸ¥è¯†èžåˆï¼š å¾ˆå¤šçŸ¥è¯†éƒ½æ˜¯ç›¸äº’å…³è”çš„ï¼Œæ¯”å¦‚æ–¹ç¨‹æ±‚è§£æ—¶ä¼šç”¨åˆ°å››åˆ™è¿ç®—ï¼Œæ±‚å¾®åˆ†ç§¯åˆ†ä¹Ÿä¼šç”¨åˆ°å››åˆ™è¿ç®—ï¼Œå¾ˆå¤šçŸ¥è¯†éƒ½éœ€è¦ä¸æ–­æ•´åˆã€‚ çŸ¥è¯†éªŒè¯ï¼š å¯¹çŸ¥è¯†çš„æ­£ç¡®æ€§ã€å®žæ—¶æ€§ã€æ•´ä¸ªä½“ç³»çš„é€»è¾‘ä¸€è‡´æ€§çš„æ£€éªŒã€‚æ¯”å¦‚2018å¹´å¯¹äºŽâ€œå°æœ‹å‹ä½ ä»Šå¹´å‡ å²å•¦ï¼Ÿâ€çš„å›žç­”æ˜¯3å²ï¼Œé‚£ä¹ˆåˆ°2019å¹´è¿™ä¸ªå›žç­”çš„çŸ¥è¯†è¦æ›´æ–°ä¸º4å²äº†ã€‚ çŸ¥è¯†è®¡ç®—ï¼š ä¸€ä¸ªçŸ¥è¯†ä½“ç³»æ˜¯å¾ˆåºžå¤§çš„ï¼Œæœ‰å¾ˆå¤šå¾ˆå¤šçš„å…³ç³»ã€‚æ¯”å¦‚â€œå°æœ‹å‹ä»Šå¹´å‡ å²äº†ï¼Ÿâ€ï¼Œéœ€è¦å¯¹è¯­ä¹‰è¿›è¡Œè®¡ç®—ï¼Œæ˜Žç™½é—®çš„é—®é¢˜æ˜¯å¹´é¾„ç›¸å…³ï¼Œè€Œä¸”æ˜¯å½“å‰å¹´é¾„ï¼Œéœ€è¦è®¡ç®—å½“å‰æ—¶é—´ä¸Žå‡ºç”Ÿæ—¶é—´çš„å·®å€¼ã€‚è®¡ç®—é—®é¢˜è¦çš„æ˜¯ä»€ä¹ˆæ ·çš„ç­”æ¡ˆï¼Œè€Œä¸æ˜¯é—®å¹´é¾„è¦æ•°å­—æ—¶è¿”å›žä¸€ä¸ªâ€œå¤©ç©ºçš„é¢œè‰²â€çš„ç­”æ¡ˆã€‚ å…ˆåˆ†äº«ä¸‹ä¸¤ä¸ªæœ‰è¶£çš„å¯è§†åŒ–å›¾è°±ï¼š æ¼«å¨è‹±é›„çš„çŸ¥è¯†å›¾è°±å¯è§†åŒ– [demo link] æ˜Ÿé™…æˆ˜äº‰çŸ¥è¯†å›¾è°±å¯è§†åŒ– [demo link] æ­å»ºå›¾æ•°æ®åº“è™½ç„¶è¿™ä¸¤demoå¾ˆå¤§ç¨‹åº¦æ˜¯å‰ç«¯å†™çš„å¥½ï¼Œå¯¹äºŽæˆ‘è¿™ç§å‰ç«¯ç™½ç—´ï¼Œå¸‚é¢ä¸Šå·²ç»æœ‰äº†ç›¸ä¼¼çš„å¾ˆå¥½ä¸Šæ‰‹çš„å¼€æºå·¥å…·ï¼ˆå›¾æ•°æ®åº“Neo4jï¼‰ï¼Œå¤šç”¨äºŽé‡‘èžçš„åæ¬ºè¯ˆï¼Œç¤¾äº¤å…³ç³»å›¾è°±ç­‰ç­‰ã€‚æŽ¥ä¸‹æ¥æˆ‘ä¹Ÿå°è¯•æ­å»ºä¸€ä¸ªåŸºç¡€çš„çŸ¥è¯†å›¾è°±ã€‚ éœ€è¦æ€è€ƒå‡ ä¸ªåŸºæœ¬é—®é¢˜ï¼š å“ªé‡Œæ¥çš„æ•°æ®ï¼Ÿ é€‰æ‹©å“ªç§æ•°æ®åº“ï¼Ÿæ•°æ®å­˜å‚¨æ ¼å¼ å¦‚ä½•å°†æ•°æ®å¯¼å…¥æ•°æ®åº“ï¼Ÿ å¦‚ä½•åšåˆ°çŸ¥è¯†èžåˆï¼Ÿè¿›è¡Œæ•°æ®åº“çš„å¢žåˆ æ”¹æŸ¥ï¼Ÿ å¦‚ä½•åšçŸ¥è¯†è®¡ç®—ï¼Ÿ æ•°æ®æ¥æºä»‹ç»ä¸€ä¸ªåŸºäºŽä¸­æ–‡çš„çŸ¥è¯†å›¾è°±å¼€æºç½‘ç«™ï¼šå¼€æ”¾çš„ä¸­æ–‡çŸ¥è¯†å›¾è°± openkg.cn è¿™ä¸ªç½‘ç«™é‡Œæœ‰å›¾è°±æ•°æ®ï¼Œå›¾è°±å·¥å…·ï¼Œèµ„æºæ¶µç›–ï¼šå¸¸è¯†ã€é‡‘èžã€å†œä¸šã€ç¤¾äº¤ã€ç‰©è”ç½‘ã€æ°”è±¡ã€ç”Ÿæ´»ã€å‡ºè¡Œã€ç§‘æ•™ã€åŒ»ç–—ç­‰ç­‰ã€‚æ€»ä¹‹æ˜¯æ¯”è¾ƒå…¨é¢çš„ä¸­æ–‡çŸ¥è¯†å›¾è°±å¼€æºåº“äº†ã€‚æ¯”å¦‚æˆ‘ç”¨è¿‡çš„æ˜¯å››å¤§åè‘—é‡Œçš„è¥¿æ¸¸è®°äººç‰©å…³ç³»æ•°æ®ï¼šä¸­å›½å››å¤§åè‘—äººç‰©å…³ç³»çŸ¥è¯†å›¾è°±å’ŒOWLæœ¬ä½“ ï¼Œåœ¨æ•°æ®ä¸Žèµ„æºé‚£è¾¹å¯è‡ªè¡Œä¸‹è½½ã€‚ç„¶åŽè§£åŽ‹å³å¯ï¼Œå°±æœ‰csvæ ¼å¼çš„äººç‰©å…³ç³»è¡¨æ ¼ã€‚ é¡ºä¾¿ä»‹ç»ä¸ªopenkgé‡Œçš„ä¸€ä¸ªæ¯”è¾ƒæˆåž‹çš„çŸ¥è¯†å›¾è°±å·¥å…·ï¼šæ€çŸ¥ï¼ˆOwnThinkï¼‰ ï¼Œä½“éªŒå…¥å£ï¼šæ€çŸ¥æœºå™¨äºº æ•°æ®åº“é€‰æ‹©è™½ç„¶æˆ‘ä¸€å¼€å§‹å·²ç»é€‰å®šäº†å›¾æ•°æ®åº“ ï¼Œè¿™é‡Œè¿˜æ˜¯å°†å„ç§ æ•°æ®ç»“æž„ å’Œ æ•°æ®åº“ åšä¸ªç®€å•çš„å¯¹æ¯”ï¼š æ•°æ®ç±»åž‹å¯èƒ½æ˜¯ï¼š å¤šå…ƒ ç»“æž„åŒ–è¡¨æ ¼ï¼š å°±åƒexcelè¡¨æ ¼ä¸€æ ·æ•´æ•´é½é½ JSON é”®å€¼ ï¼š key-value ç»„ RDF ä¸‰å…ƒç»„ï¼š ä¸‰å…ƒç»„å°±æ˜¯ä¸¤ä¸ªå®žä½“ï¼Œä¸­é—´æœ‰ä¸€ä¸ªå…³ç³»ã€‚æ¯”å¦‚â€œä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬â€ï¼Œä¸¤ä¸ªå®žä½“åˆ†åˆ«æ˜¯â€œä¸­å›½â€å’Œâ€œåŒ—äº¬â€ï¼Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»æ˜¯â€œçš„é¦–éƒ½â€ï¼Œè¿™ä¸¤ä¸ªå®žä½“åŠ ä¸€ä¸ªå…³ç³»å°±æ˜¯ä¸‰å…ƒç»„ã€‚ ä¸åŒçš„æ•°æ®åº“ï¼š MySQLï¼š é€‚åˆç»“æž„åŒ–æ•°æ®ï¼Œåƒexcelè¡¨æ ¼ MongoDBï¼š é€‚åˆjsonæ ¼å¼æ•°æ® Neo4jï¼š å›¾æ•°æ®åº“ï¼Œå¯è§†åŒ–ï¼Œé€‚åˆä¸‰å…ƒç»„ï¼Œå¹¶ä¸”å®žä½“å’Œå…³ç³»å¯ä»¥åŒ…å«å±žæ€§ã€‚ å› æ­¤ï¼Œé€‰æ‹©ä»€ä¹ˆæ ·çš„æ•°æ®åº“ï¼Œæ˜¯åŸºäºŽä½ çš„æ•°æ®æ‰“ç®—ä»¥ä»€ä¹ˆæ ·çš„æ ¼å¼å­˜å‚¨ã€‚é€‰æ‹©ä»€ä¹ˆæ ·çš„æ•°æ®åº“ä»¥åŠæ€Žä¹ˆè®¾è®¡ schemaã€‚é€‰å…³ç³»æ•°æ®åº“è¿˜æ˜¯NoSQL æ•°æ®åº“ï¼Ÿè¦ä¸è¦ç”¨å†…å­˜æ•°æ®åº“ï¼Ÿè¦ä¸è¦ç”¨å›¾æ•°æ®åº“ï¼Ÿè¿™äº›éƒ½éœ€è¦æ ¹æ®æ•°æ®åœºæ™¯æ…Žé‡é€‰æ‹©ã€‚è¥¿æ¸¸è®°çš„äººç‰©å…³ç³»æ•°æ®æ˜¯ä¸‰å…ƒç»„åˆæ ¼ï¼Œè¿™é‡Œæˆ‘ä»¬é€‰æ‹©å°è¯•ç”¨Neo4j å›¾æ•°æ®åº“ã€‚ å›¾æ•°æ®åº“ï¼šNeo4jNeo4j æ˜¯ä¸€ä¸ªå›¾æ•°æ®åº“ï¼Œä¸»è¦åŒ…æ‹¬èŠ‚ç‚¹å’Œå…³ç³»ã€‚èŠ‚ç‚¹å’Œå…³ç³»éƒ½å¯ä»¥åŒ…å«å±žæ€§ã€‚ä»‹ç»æ–‡æ¡£ [doc], ç¤¾åŒº[community]ã€‚ å®‰è£…ç¤¾åŒºç‰ˆä¸‹è½½é“¾æŽ¥ï¼šneo4j-community-3.5.2 è§£åŽ‹åŽåœ¨binç›®å½•ä¸‹è¿è¡Œå‘½ä»¤ï¼šneo4j console 12345678D:\&gt;cd D:\neo4j-community-3.5.2\binD:\neo4j-community-3.5.2\bin&gt;neo4j console2019-04-23 12:23:39.692+0000 INFO ======== Neo4j 3.5.2 ========2019-04-23 12:23:39.708+0000 INFO Starting...2019-04-23 12:23:42.950+0000 INFO Bolt enabled on 127.0.0.1:7687.2019-04-23 12:23:44.544+0000 INFO Started.2019-04-23 12:23:45.696+0000 INFO Remote interface available at http://localhost:7474/ æµè§ˆå™¨Chromeè®¿é—®ï¼šlocalhost:7474ï¼Œç¬¬ä¸€æ¬¡è®¿é—®ä¼šæç¤ºä¿®æ”¹å¯†ç ã€‚ ç¬¬ä¸€æ¬¡çŽ©å¯ä»¥å‚è€ƒè¿™ä¸ªå…¥é—¨å°æ•™ç¨‹ï¼šNeo4j ç®€å•å…¥é—¨æ›´è¯¦ç»†çš„å®‰è£…ä»¥åŠå…¶ä»–ä¿¡æ¯ï¼šneo4j å®‰è£… Cypher è¯­æ³•æ€»ç»“Cypherå…¥é—¨ï¼štutorial cypherå°±åƒMySQLçš„sqlè¯­è¨€ï¼Œdemoç¤ºä¾‹ï¼šneo4j_movie_graph.cypher ã€‚cypherå¯å¯¹æ•°æ®åº“åšå¢žåˆ æ”¹æŸ¥ã€‚æ›´å¤šçš„æ“ä½œå‚è€ƒï¼šCypher Basic ï¼Œå¯ä»¥è‡ªå·±è·‘å‡ è¡Œå‘½ä»¤æ‰¾æ‰¾æ„Ÿè§‰ã€‚ Py2neo [doc]æ¯•ç«Ÿå†™sqlæˆ–è€…å†™cypherä¸æ˜¯æˆ‘æ“…é•¿çš„äº‹ï¼Œæ‰€å¹¸å¯ä»¥é€šè¿‡Python API py2neo æ¥è®¿é—®neo4jã€‚ py2neoä½¿ç”¨æ•™ç¨‹-1py2neoä½¿ç”¨æ•™ç¨‹-2py2neoä½¿ç”¨æ•™ç¨‹-3 è´´ä¸€äº›ä½¿ç”¨è¿‡çš„ä»£ç ï¼Œè¦é…åˆåˆé€‚çš„æ•°æ®é›†ä¸€èµ·ä½¿ç”¨ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141import pandas as pdimport numpy as npimport itertoolsfrom py2neo import Graph,Node,Relationship,NodeMatcher# ç»ˆç«¯è¿è¡Œ neo4j consoleä¹‹åŽå°±å¯ä»¥ä»Žpy2neoè¿žæŽ¥neo4jæ•°æ®åº“graph = Graph("http://localhost:7474",username="neo4j",password="neo4j")matcher = NodeMatcher(graph)# graph.delete_all() # æ¸…ç©ºæœ¬åœ°æ•°æ®åº“çš„å‘½ä»¤ï¼Œæ…Žç”¨# ============================================================# Demo1: Product# ç›®çš„ï¼šæŠŠèƒ½å±•ç¤ºçš„åº”ç”¨éƒ½å±•ç¤ºå‡ºæ¥prod = pd.read_excel('type.xlsx')list(prod)# è®¾ç½®åŸºç¡€ä¿¡æ¯for i in np.unique(prod['åˆ†ç»„ç±»åž‹']): node_a = Node('Type', name=i) df = prod[prod['åˆ†ç»„ç±»åž‹'] == i] _p = np.unique(df['äº§å“ç±»åž‹']) for j in _p: node_b = Node('Product',name=j) rela = Relationship(node_a,'includes',node_b) graph.create(rela) # å› ä¸ºç‚¹Nodeéœ€è¦å®šä¹‰æ€§è´¨ï¼Œæ‰€ä»¥å»ºè®® key-value å’Œ rela-nodeåˆ†å¼€å»ºè¡¨# add property/relationshipæ—¶éœ€è¦ç¬¬ä¸€åˆ—çš„äº§å“å·²å­˜åœ¨ï¼Œå¦åˆ™éœ€è¦æ›´å¤šä¿¡æ¯# ------------------------------------------------------------# add propertyprop = pd.read_excel('data/add_property.xlsx')for _prod in np.unique(prop['product']): df = prop[prop['product'] == _prod].reset_index(drop=True) # å¦‚æžœåº“å†…æ²¡æœ‰è¯¥äº§å“ï¼Œè¿›è¡ŒNodeå»ºç«‹ if matcher.match(name=_prod).first() is None: node_target = Node('Product', name = _prod) # 'Product'æ˜¯å›ºå®šçš„ else: node_target = matcher.match(name=_prod).first() # 'Product'æ˜¯å›ºå®šçš„ for idx in range(len(df)): node_target[df['key'][idx]] = df['value'][idx] graph.push(node_target) # add relationshiprel = pd.read_excel('add_relationship.xlsx')# æ£€æŸ¥æ˜¯å¦å­˜åœ¨èŠ‚ç‚¹ !!!å¸Œæœ›åŽæœŸçš„åˆ¶è¡¨è§„åˆ™å®Œå–„åŽå¯ä»¥çœåŽ»æ£€æŸ¥è¿™ä¸€æ­¥# å¦‚æžœä¸å­˜åœ¨ï¼Œå°±åˆ›å»º# head ä¸Ž tail å¦‚ä½•åŒºåˆ†#nodes = list(itertools.chain.from_iterable([np.unique(rel['product']), np.unique(rel['node_name'])]))for ind in range(len(rel)): # å¦‚æžœä¸æƒ³é‡å¤éåŽ†ï¼Œå°±å»ºchunk df if matcher.match(name=rel['node_head'][ind]).first() is None: node_head = Node(rel['head_type'][ind], name = rel['node_head'][ind]) else: node_head = matcher.match(name=rel['node_head'][ind]).first() node_tail = matcher.match(name=rel['node_tail'][ind]).first() r = rel['rela'][ind] # æ£€æŸ¥å…³ç³»æ˜¯å¦å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨ï¼Œå»ºç«‹å…³ç³» cmd = "MATCH a=()-[:%s]-&gt;( &#123;name: '%s'&#125;) RETURN a" %(r,rel['node_tail'][ind]) if len(graph.run(cmd).data()) == 0: relatp = Relationship(node_head,r,node_tail) graph.create(relatp) # ==============================================================# Demo2: Roomroom = pd.read_excel('room.xlsx')for i in np.unique(room['owner']): node_a = Node('Owner',name=i) df = room[room['owner'] == i] _r = np.unique(df['room_name']) for j in _r: node_b = Node('Room',name=j) rela = Relationship(node_a,'lives',node_b) graph.create(rela) _df = df[df['room_name']==j] _p = np.unique(_df['product_id']) for k in _p: node_c = Node('Prod',name=k) relatp = Relationship(node_b,'has',node_c) graph.create(relatp) for i in np.unique(room['product_id']): # è¿™äº›ç‚¹å·²ç»å­˜åœ¨ï¼Œåªéœ€è¦åŒ¹é… node_a = matcher.match('Prod',name=i).first() df = room[room['product_id'] == i] _c = np.unique(df['control']) for j in _c: node_b = Node('Ctrl',name=j) # æŠŠnodeçš„ç­”æ¡ˆä½œä¸ºå±žæ€§å¡«å……ä¸ŠåŽ»ï¼Œç„¶åŽï¼Œæ–¹ä¾¿ç”¨ä½œcypheræŸ¥è¯¢ rela = Relationship(node_a,'controls',node_b) graph.create(rela)# ==============================================================# Demo3: è¥¿æ¸¸è®°xyj = pd.read_csv('./è¥¿æ¸¸è®°/triples.csv')#æ‰€æœ‰äººç‰©åˆ›å»ºå®Œï¼Œç›´æŽ¥æœç´¢å»ºå…³ç³»n = [np.unique(xyj['head']),np.unique(xyj['tail'])]for i in n: for p in i: node = Node('Person', name=p) graph.create(node)for idx in range(len(xyj)): print(idx) node_a = matcher.match('Person',name=xyj['tail'][idx]).first() node_b = matcher.match('Person',name=xyj['head'][idx]).first() relatp = Relationship(node_a,xyj['label'][idx],node_b) graph.create(relatp) # æŸ¥è¯¢ 'å­™æ‚Ÿç©ºçš„å¸ˆå‚…æ˜¯è°'len(graph.nodes)len(graph.nodes.match("Person"))# 'PERSON'æ ¹æ®å‘½åå®žä½“åŒ¹é…test = matcher.match('Person',name='å”åƒ§').first()for rel in matcher.match(start_node=test, rel_type="å¾’å¼Ÿ"): print(rel.end_node()["name"]) æ‹“å±•å¤§æ‰¹é‡æ•°æ®å¯¼å…¥å‚è€ƒï¼šcsvæ–‡ä»¶å¯¼å…¥Neo4j(åŒ…æ‹¬ç»“ç‚¹å’Œå…³ç³»çš„å¯¼å…¥)neo4jæ‰¹é‡å¯¼å…¥neo4j-importå¦‚ä½•å°†å¤§è§„æ¨¡æ•°æ®å¯¼å…¥Neo4j åˆ ç©ºå±žæ€§ä¸€èˆ¬èŠ‚ç‚¹å’Œå…³ç³»å¯ä»¥é€šè¿‡py2neoåˆ ç©ºï¼Œä½†æ˜¯å±žæ€§ä¼šå­˜ç•™ï¼šNeo4j - How to delete unused property keys from browser? åˆ«äººå¼€å‘çš„æœ‰è¶£çš„å›¾è°±æˆæžœ [refer] ä¸€å®¶åšNLPçš„å…¬å¸ï¼šwisers AI lab zhishi.me Acemap äº¤å¤§ è´¼æœ‰æ„æ€çš„ç½‘ç«™ä»¥åŠéšä¾¿ç‚¹å¼€çš„é“¾æŽ¥ï¼šacemap.info main page 2018 NeurIPS 2018 https://www.acemap.info/topic?topicID=0304C748 https://www.acemap.info/paper-map?topicID=0304C748 æœ‰è¶£çš„ç®—æ³•ä»‹ç»ï¼šhttps://www.acemap.info/acenap/algorithms CN-DBpedia å¤æ—¦å¤§å­¦ï¼Œæˆ‘çŽ°åœ¨ç‚¹å¼€å±žäºŽç½‘ç«™ç»´æŠ¤ä¸­ï¼Œæ‰€ä»¥è¿™å—ä»¥åŽå†è¡¥å……æ ·ä¾‹æ•°æ®æ–‡ä»¶æ˜¯txtæ ¼å¼ï¼Œæ¯è¡Œä¸€æ¡æ•°æ®ï¼Œæ¯æ¡æ•°æ®æ˜¯ä¸€ä¸ª(å®žä½“åç§°ï¼Œå±žæ€§åç§°ï¼Œå±žæ€§å€¼)çš„ä¸‰å…ƒç»„ï¼Œä¸­é—´ç”¨tabåˆ†éš”ï¼Œå…·ä½“å¦‚ä¸‹æ‰€ç¤ºã€‚ã€å¤æ—¦å¤§å­¦ ç®€ç§° å¤æ—¦ã€‘åŒ…å«900ä¸‡+çš„ç™¾ç§‘å®žä½“ä»¥åŠ6700ä¸‡+çš„ä¸‰å…ƒç»„å…³ç³»ã€‚å…¶ä¸­mention2entityä¿¡æ¯110ä¸‡+ï¼Œæ‘˜è¦ä¿¡æ¯400ä¸‡+ï¼Œæ ‡ç­¾ä¿¡æ¯1980ä¸‡+ï¼Œinfoboxä¿¡æ¯4100ä¸‡+ã€‚å¤æ—¦å¤§å­¦è¿˜æœ‰ä¸ªknowledge works http://kw.fudan.edu.cn/ å¯ä»¥ä¸‹è½½æ•°æ®é›†çš„ç½‘ç«™ï¼šgrouplens ä¸€ä¸ªä¸­è‰è¯çš„çŸ¥è¯†æœåŠ¡ç³»ç»Ÿï¼šhttp://zcy.ckcest.cn/tcm/ é€†å¤©http://zcy.ckcest.cn/tcm/qaos/profilenet NLPIRï¼šhttp://ictclas.nlpir.org/nlpir/]]></content>
      <tags>
        <tag>knowledge graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[convert-jupyter-to-other-formats]]></title>
    <url>%2F2019%2F04%2F23%2Fconvert-jupyter-to-other-formats%2F</url>
    <content type="text"><![CDATA[è‡ªä»Žåœ¨Githubä¸Šç”¨Jupyter notebookå†™ç¬”è®°å’Œæ•™ç¨‹è¶Šç”¨è¶Šé¡ºæ‰‹åŽï¼Œå¼€å§‹å¯¹Jupyteræœ‰äº†è¶Šæ¥è¶Šå¤šçš„æœŸå¾…ã€‚æœ¬ç¯‡ä»‹ç»å¦‚ä½•å°†Jupyteræ–‡ä»¶ï¼ˆ.ipynbï¼‰è½¬æ¢æˆä¸‹é¢çš„æ ¼å¼ï¼š html latex markdown slides rst python åœ¨ç»ˆç«¯å¯¹åº”çš„æ–‡ä»¶ç›®å½•ä¸‹ï¼Œå‘½ä»¤è¡Œé”®å…¥ï¼š 1$ ipython nbconvert --to FORMAT notebook.ipynb è¿™é‡Œnotebook.ipynb å°±æ˜¯å¾…è½¬æ¢æ ¼å¼çš„jupyter notebookæ–‡ä»¶ï¼Œ--to FORMAT ä»£è¡¨è½¬æ¢åŽçš„æ ¼å¼ï¼Œå¯ä»¥æœ‰ä»¥ä¸‹é€‰æ‹©ï¼š --to html --template full (default) A full static HTML render of the notebook. This looks very similar to the interactive view. --template basic Simplified HTML, useful for embedding in webpages, blogs, etc. This excludes HTML headers. --to latex Latex export. This generates NOTEBOOK_NAME.tex file, ready for export. You can automatically run latex on it to generate a PDF by adding --post PDF. --template article (default) Latex article, derived from Sphinxâ€™s howto template. --template book Latex book, derived from Sphinxâ€™s manual template. --template basic Very basic latex output - mainly meant as a starting point for custom templates. --to slides This generates a Reveal.js HTML slideshow. It must be served by an HTTP server. The easiest way to get this is to add --postserve on the command-line. --to markdown Simple markdown output. Markdown cells are unaffected, and code cells are placed in triple-backtick (```) blocks. --to rst Basic reStructuredText output. Useful as a starting point for embedding notebooks in Sphinx docs. --to python Convert a notebook to an executable Python script. This is the simplest way to get a Python script out of a notebook. If there were any magics in the notebook, this may only be executable from an IPython session. ç¤ºä¾‹å¦‚æžœæƒ³å°†Jupyteræ–‡ä»¶ï¼ˆtest.ipynbï¼‰è½¬æ¢æˆmarkdownæ ¼å¼ï¼Œè¿è¡Œç»ˆç«¯ï¼Œåœ¨è¯¥æ–‡ä»¶çš„ç›®å½•ä¸‹ï¼Œé”®å…¥å‘½ä»¤ï¼Œå°±ä¼šç”Ÿæˆæ–°markdownæ–‡ä»¶ï¼š 1$ ipython nbconvert --to markdown test.ipynb å‚è€ƒï¼šConverting notebooks to other formats]]></content>
      <tags>
        <tag>jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kerasåˆ†è¯å™¨ Tokenizer]]></title>
    <url>%2F2019%2F04%2F23%2Fkeras-tokenizer%2F</url>
    <content type="text"><![CDATA[ä»‹ç»ä¸€ä¸‹kerasçš„åˆ†è¯å™¨Tokenizerï¼Œä»¥åŠæˆ‘åœ¨ä¸Šé¢åƒè¿‡çš„äºã€‚ TokenizerTokenizeræ˜¯ä¸€ä¸ªå°†æ–‡æœ¬å‘é‡åŒ–ï¼Œè½¬æ¢æˆåºåˆ—çš„ç±»ã€‚ç”¨æ¥æ–‡æœ¬å¤„ç†çš„åˆ†è¯ã€åµŒå…¥ ã€‚ 1234567keras.preprocessing.text.Tokenizer(num_words=None, filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~\t\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0) å‚æ•°è¯´æ˜Žï¼š num_words: é»˜è®¤æ˜¯Noneå¤„ç†æ‰€æœ‰å­—è¯ï¼Œä½†æ˜¯å¦‚æžœè®¾ç½®æˆä¸€ä¸ªæ•´æ•°ï¼Œé‚£ä¹ˆæœ€åŽè¿”å›žçš„æ˜¯æœ€å¸¸è§çš„ã€å‡ºçŽ°é¢‘çŽ‡æœ€é«˜çš„num_wordsä¸ªå­—è¯ã€‚ä¸€å…±ä¿ç•™ num_words-1 ä¸ªè¯ã€‚ filters: è¿‡æ»¤ä¸€äº›ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤ä¸Šæ–‡çš„å†™æ³•å°±å¯ä»¥äº†ã€‚ lower: æ˜¯å¦å…¨éƒ¨è½¬ä¸ºå°å†™ã€‚ split: åˆ†è¯çš„åˆ†éš”ç¬¦å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸ºç©ºæ ¼ã€‚å› ä¸ºè‹±æ–‡åˆ†è¯åˆ†éš”ç¬¦å°±æ˜¯ç©ºæ ¼ã€‚ char_level: åˆ†å­—ã€‚ oov_token: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls ç›¸å…³çš„ç±»æ–¹æ³• æ–¹æ³• å‚æ•° è¿”å›žå€¼ fit_on_texts(texts) textsï¼šè¦ç”¨ä»¥è®­ç»ƒçš„æ–‡æœ¬åˆ—è¡¨ - texts_to_sequences(texts) textsï¼šå¾…è½¬ä¸ºåºåˆ—çš„æ–‡æœ¬åˆ—è¡¨ åºåˆ—çš„åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­æ¯ä¸ªåºåˆ—å¯¹åº”äºŽä¸€æ®µè¾“å…¥æ–‡æœ¬ texts_to_sequences_generator(texts) textsï¼šå¾…è½¬ä¸ºåºåˆ—çš„æ–‡æœ¬åˆ—è¡¨ æœ¬å‡½æ•°æ˜¯texts_to_sequencesçš„ç”Ÿæˆå™¨å‡½æ•°ç‰ˆï¼Œè¿”å›žæ¯æ¬¡è°ƒç”¨è¿”å›žå¯¹åº”äºŽä¸€æ®µè¾“å…¥æ–‡æœ¬çš„åºåˆ— texts_to_matrix(texts, mode) textsï¼šå¾…å‘é‡åŒ–çš„æ–‡æœ¬åˆ—è¡¨ï¼›modeï¼šâ€˜binaryâ€™ï¼Œâ€˜countâ€™ï¼Œâ€˜tfidfâ€™ï¼Œâ€˜freqâ€™ä¹‹ä¸€ï¼Œé»˜è®¤ä¸ºâ€˜binaryâ€™ å½¢å¦‚(len(texts), nb_words)çš„numpy array fit_on_sequences(sequences) sequencesï¼šè¦ç”¨ä»¥è®­ç»ƒçš„åºåˆ—åˆ—è¡¨ - sequences_to_matrix(sequences) sequencesï¼šå¾…å‘é‡åŒ–çš„åºåˆ—åˆ—è¡¨ï¼› modeï¼šåŒä¸Š è¿”å›žå€¼ï¼šå½¢å¦‚(len(sequences), nb_words)çš„numpy array å±žæ€§ word_counts: å­—å…¸ï¼Œå°†å•è¯ï¼ˆå­—ç¬¦ä¸²ï¼‰æ˜ å°„ä¸ºå®ƒä»¬åœ¨è®­ç»ƒæœŸé—´å‡ºçŽ°çš„æ¬¡æ•°ã€‚ä»…åœ¨è°ƒç”¨fit_on_textsä¹‹åŽè®¾ç½®ã€‚ word_docs: å­—å…¸ï¼Œå°†å•è¯ï¼ˆå­—ç¬¦ä¸²ï¼‰æ˜ å°„ä¸ºå®ƒä»¬åœ¨è®­ç»ƒæœŸé—´æ‰€å‡ºçŽ°çš„æ–‡æ¡£æˆ–æ–‡æœ¬çš„æ•°é‡ã€‚ä»…åœ¨è°ƒç”¨fit_on_textsä¹‹åŽè®¾ç½®ã€‚ word_index: å­—å…¸ï¼Œå°†å•è¯ï¼ˆå­—ç¬¦ä¸²ï¼‰æ˜ å°„ä¸ºå®ƒä»¬çš„æŽ’åæˆ–è€…ç´¢å¼•ã€‚ä»…åœ¨è°ƒç”¨fit_on_textsä¹‹åŽè®¾ç½®ã€‚ document_count: æ•´æ•°ã€‚åˆ†è¯å™¨è¢«è®­ç»ƒçš„æ–‡æ¡£ï¼ˆæ–‡æœ¬æˆ–è€…åºåˆ—ï¼‰æ•°é‡ã€‚ä»…åœ¨è°ƒç”¨fit_on_textsæˆ–fit_on_sequencesä¹‹åŽè®¾ç½® 123456789101112131415161718192021222324252627282930313233from keras.preprocessing.text import Tokenizer# Using TensorFlow backend.# åˆ›å»ºåˆ†è¯å™¨ Tokenizer å¯¹è±¡tokenizer = Tokenizer()# texttext = ["ä»Šå¤© åŒ—äº¬ ä¸‹ é›¨ äº†", "æˆ‘ ä»Šå¤© åŠ ç­"]# fit_on_texts æ–¹æ³•tokenizer.fit_on_texts(text)# word_countså±žæ€§tokenizer.word_counts# OrderedDict([('ä»Šå¤©', 2),# ('åŒ—äº¬', 1),# ('ä¸‹', 1),# ('é›¨', 1),# ('äº†', 2),# ('æˆ‘', 1),# ('åŠ ç­', 1)])# word_docså±žæ€§tokenizer.word_docs# defaultdict(int, &#123;'ä¸‹': 1, 'åŒ—äº¬': 1, 'ä»Šå¤©': 2, 'é›¨': 1, 'äº†': 2, 'æˆ‘': 1, 'åŠ ç­': 1&#125;)# word_indexå±žæ€§tokenizer.word_index# &#123;'ä»Šå¤©': 1, 'äº†': 2, 'åŒ—äº¬': 3, 'ä¸‹': 4, 'é›¨': 5, 'æˆ‘': 6, 'åŠ ç­': 7&#125;# document_countå±žæ€§tokenizer.document_count# 2 éœ€è¦æ³¨æ„çš„ç‚¹æ˜¯ï¼Œç”±äºŽä¹¦å†™ä¹ æƒ¯ï¼Œè‹±æ–‡æ–‡æœ¬çš„å•è¯ä¹‹é—´æ˜¯ç”¨ç©ºæ ¼éš”å¼€çš„ï¼Œsplit=&#39; &#39; è¿™ä¸ªå‚æ•°å¯ä»¥ç›´æŽ¥å¯¹è‹±æ–‡æ–‡æœ¬è¿›è¡Œç©ºæ ¼åˆ†è¯ã€‚ä½†æ˜¯å¯¹ä¸­æ–‡ä¸è¡Œï¼Œå› æ­¤ä½¿ç”¨ tokenizer.fit_on_texts(text) æ—¶ï¼Œtextå¦‚æžœæ˜¯è‹±æ–‡æ–‡æœ¬ï¼Œå¯ä»¥ç›´æŽ¥ text = [&quot;Today is raining.&quot;, &quot;I feel tired today.&quot;] ï¼Œä½†æ˜¯textæ˜¯ä¸­æ–‡æ–‡æœ¬çš„è¯ï¼Œéœ€è¦å…ˆå°†ä¸­æ–‡æ–‡æœ¬åˆ†è¯å†ä½œä¸ºè¾“å…¥textï¼š text = [&quot;ä»Šå¤© åŒ—äº¬ ä¸‹ é›¨ äº†&quot;, &quot;æˆ‘ ä»Šå¤© åŠ ç­&quot;] è¿™é‡Œå°±æ˜¯æˆ‘è¸©è¿‡çš„å‘äº†ï¼Œä¹‹å‰æ‹·ä»£ç ä¸‹æ¥è·‘çš„æ—¶å€™ï¼Œåˆ«äººç”¨çš„æ˜¯è‹±æ–‡æ–‡æœ¬ï¼Œæ²¡é—®é¢˜ï¼Œä½†æ˜¯æˆ‘çš„è¾“å…¥æ˜¯ä¸­æ–‡æ–‡æœ¬ï¼Œå¯¼è‡´åˆ†è¯æ­¥éª¤åˆ©ç”¨ç©ºæ ¼å¯¹ä¸­æ–‡åˆ†è¯ï¼Œä¼šå°†æ•´å¥è¯å½“ä½œä¸€ä¸ªtokenï¼Œè€Œä¸”æ˜¯å­—å…¸é‡Œæ‰¾ä¸åˆ°çš„tokenï¼Œè¿™æ ·ä¼šé€ æˆå¤§é‡çš„ç›¸åŒçš„åµŒå…¥è¡¨è¾¾å’Œç›¸åŒçš„é¢„æµ‹åˆ†æ•°ã€‚ å› æ­¤ï¼Œkerasçš„Tokenizerå¯¹äºŽè‹±æ–‡æ–‡æ¡£å¯ä»¥åšåˆ†è¯+åµŒå…¥ ä¸¤æ­¥ï¼Œå¯¹äºŽä¸­æ–‡çš„è¯ï¼Œå…¶å®žåªæœ‰åµŒå…¥è¿™æ­¥ã€‚ åµŒå…¥ç¤ºä¾‹123456789101112131415161718192021222324from keras.preprocessing.text import Tokenizerfrom keras.preprocessing.sequence import pad_sequences# 1. åˆ›å»ºåˆ†è¯å™¨ Tokenizer å¯¹è±¡tokenizer = Tokenizer() # é‡Œé¢çš„å‚æ•°å¯ä»¥è‡ªå·±æ ¹æ®å®žé™…æƒ…å†µæ›´æ”¹# 2. æ•´ç†æ•´ä½“è¯­æ–™ï¼Œä¸­æ–‡éœ€ç©ºæ ¼åˆ†è¯text = ["ä»Šå¤© åŒ—äº¬ ä¸‹ é›¨ äº†", "æˆ‘ ä»Šå¤© åŠ ç­"]# 3. å°†Tokenizeræ‹Ÿåˆè¯­æ–™ï¼Œç”Ÿæˆå­—å…¸ï¼Œå½¢æˆæ–°çš„tokenizertokenizer.fit_on_texts(text)# 4. ä¿å­˜tokenizerï¼Œé¿å…é‡å¤å¯¹åŒä¸€è¯­æ–™è¿›è¡Œæ‹Ÿåˆimport joblibjoblib.dump(tokenizer, save_path)# 5. æ•´åˆéœ€è¦åšåµŒå…¥çš„æ–‡æœ¬ï¼Œä¸­æ–‡éœ€è¦ç©ºæ ¼åˆ†è¯new_text = ["ä»Šå¤© å›žå®¶ åƒé¥­", "æˆ‘ ä»Šå¤© ç”Ÿç—… äº†"]# 6. å°†æ–‡æœ¬å‘é‡åŒ–list_tokenized = tokenizer.text_to_sequence(new_text)# 7. ç”Ÿæˆè®­ç»ƒæ•°æ®çš„åºåˆ—X_train = pad_sequences(list_tokenized, maxlen=200) å†™åœ¨æœ€åŽå½“é‡åˆ°ä¸€ä¸ªä¸é”™çš„APIï¼Œä¸å¦¨æŠŠå‚æ•°çœ‹å…¨ï¼Œå…å¾—æ ½åœ¨ä¸€ä¸ªå°å‚æ•°ä¸Šã€‚]]></content>
      <tags>
        <tag>keras</tag>
        <tag>tokenizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è§£å†³GitHub commitä¸æ˜¾ç¤ºç»¿è‰²çš„é—®é¢˜]]></title>
    <url>%2F2019%2F04%2F10%2Fgithub-commit-not-green%2F</url>
    <content type="text"><![CDATA[ä¹‹å‰å¥½ä¸å®¹æ˜“ä¸‹å†³å¿ƒç«‹ä¸ªflagï¼ša commit a day, green wall repay. æ˜¯ä¸æ˜¯æœ—æœ—ä¸Šå£ï¼Œä½†æ˜¯è‡ªå·±æ˜Žæ˜Žcommitäº†å¾ˆå¤šæ¬¡ï¼ŒGitHubå¢™æ€Žä¹ˆä¹Ÿä¸ç»¿ã€‚ Contributionsæœªè¢«Githubè®¡å…¥çš„å‡ ä¸ªå¸¸è§åŽŸå›  è¿›è¡ŒCommitsçš„ç”¨æˆ·æ²¡æœ‰è¢«å…³è”åˆ°ä½ çš„Githubå¸å·ä¸­ã€‚ ä¸æ˜¯åœ¨è¿™ä¸ªç‰ˆæœ¬åº“çš„é»˜è®¤åˆ†æ”¯è¿›è¡Œçš„Commitã€‚ è¿™ä¸ªä»“åº“æ˜¯ä¸€ä¸ªForkä»“åº“ï¼Œè€Œä¸æ˜¯ç‹¬ç«‹ä»“åº“ã€‚ æˆ‘çš„æ˜¯ç‹¬ç«‹ä»“åº“ï¼ŒæŸ¥äº†ä¸€ä¸‹å‘çŽ°æˆ‘çš„gitè®°å½•æŒ‡å‘å¦ä¸€ä¸ªè´¦å·ï¼ŒåŽŸæ¥æ˜¯commitçš„é‚®ç®±å’Œç”¨æˆ·åä¸å¯¹ã€‚å¯ä»¥ç”¨git show å‘çŽ°é‚®ç®±é‚£é‡Œè·Ÿgithubçš„è´¦å·é‚®ç®±ä¸ä¸€æ ·ã€‚è§£å†³æ­¥éª¤ï¼š ä½¿ç”¨git show æŸ¥çœ‹æœ¬åœ°ç«¯çš„é‚®ç®± åœ¨GitHubçš„ä¸ªäººè´¦å·çš„è®¾ç½®é‡Œï¼Œæ‰¾åˆ°Emailï¼Œæ‰¾åˆ° Add email addressï¼ŒæŠŠæœ¬åœ°é‚®ç®±å¡«ä¸ŠåŽ» æ·»åŠ å¹¶ç»‘å®šéªŒè¯ï¼Œåˆ·æ–°ï¼Œç»¿è‰²æ ¼å­å°±å‡ºæ¥äº†ï¼ å½“ç„¶ä¹Ÿå¯ä»¥ä¿®æ”¹æœ¬åœ°gité…ç½® git config â€”global user.name â€œusernameâ€git config â€”global user.email â€œusername@mail.comâ€]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kaggle - Bag of Words Meets Bags of Popcorn]]></title>
    <url>%2F2019%2F03%2F12%2Fkaggle-movie-reviews%2F</url>
    <content type="text"><![CDATA[è¯´èµ·sentiment analysisï¼Œå°±ä¸å¾—ä¸è¯´èµ·NLPé€‰æ‰‹å¿…åšé¢˜ï¼šBag of Words Meets Bags of Popcorn ï¼Œä»¥ä¸‹ç®€ç§°â€œå½±è¯„åˆ†æžé¢˜â€ã€‚å¿…é¡»è´Ÿè´£ä»»çš„è¯´ï¼Œè¿™æ˜¯ä¸€é“å¾ˆç®€å•çš„é¢˜ï¼Œå°±æ˜¯å¯¹ä¸€æ®µå½±è¯„è¿›è¡Œæƒ…æ„Ÿå€¾å‘é¢„æµ‹ï¼ˆpositive/negativeï¼‰ã€‚æ•°æ®ä¸ºè‹±æ–‡æ–‡æœ¬ï¼Œæ•°æ®é›†è‡ªå·±ä¸‹è½½ï¼šðŸ”—data æ–‡æœ¬æ¸…æ´—æŠ€å·§1. reï¼šæ­£åˆ™è¡¨è¾¾å¼ Regular Expressionä¸­æ–‡å¤„ç†ï¼šé™¤äº†ä¸­æ–‡ï¼Œå…¶ä»–å­—ç¬¦å…¨éƒ¨åŽ»æŽ‰ã€‚è¿™æ ·å¤„ç†åŽçš„outputåªå‰©ä¸‹ä¸­æ–‡å’Œè¿žæŽ¥æ–­å¥çš„é€—å·ã€‚è¿™ä¸ªæ˜¯æŠŠéžä¸­æ–‡çš„å­—ç¬¦æ¯”å¦‚æ•°å­—ã€è‹±æ–‡ã€æ ‡ç‚¹ç¬¦å·ã€htmlæ–‡æœ¬ç­‰å…¨éƒ¨æ´—æŽ‰äº†ï¼Œæ¯”è¾ƒç‹ ã€‚è¿™ä¸ªæŠ€èƒ½åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šä¼šæ¯”è¾ƒæœ‰ç”¨ã€‚ 123456789101112131415161718import redef only_chinese(comment): line = comment.strip() # åŽ»é™¤é¦–å°¾ç©ºæ ¼ p2 = re.compile(u'[^\u4e00-\u9fa5]') # ä¸­æ–‡çš„ç¼–ç èŒƒå›´æ˜¯ï¼š\u4e00åˆ°\u9fa5 zh = " ".join(p2.split(line)).strip() outStr = ",".join(zh.split()) # æ‰€æœ‰çš„æ–­å¥å…¨éƒ¨ç”¨é€—å·è¿žæŽ¥ return outStrcomment = " æ­¦æž—å¤–ä¼ çš„æƒ…èŠ‚è®¾è®¡åŸºæœ¬æ²¡ä»€ä¹ˆbugï¼â•­(â—ï½€âˆ€Â´â—)â•¯!!\ çœ‹äº†10å¹´éƒ½çœ‹ä¸è…»~é€ä½ ä¸ªç½‘pané“¾æŽ¥ï¼š\ http://fakewebsite.com"test = only_chinese(comment)print(test)# output# æ­¦æž—å¤–ä¼ çš„æƒ…èŠ‚è®¾è®¡åŸºæœ¬æ²¡ä»€ä¹ˆ,çœ‹äº†,å¹´éƒ½çœ‹ä¸è…»,é€ä½ ä¸ªç½‘,é“¾æŽ¥ è‹±æ–‡å¤„ç†ï¼šé™¤äº†è‹±æ–‡å­—æ¯ï¼Œå…¶ä»–å­—ç¬¦å…¨éƒ¨æ›¿æ¢ä¸ºç©ºæ ¼ã€‚æ­£åˆ™è¡¨è¾¾å¼å°±æ˜¯ä¸“æ²»å„ç§ä¸æœã€‚ 123456import recomment = 'æœ€å–œæ¬¢çš„è¯æ˜¯Coding is the new SEXY!'review_text = re.sub("[^a-zA-Z]"," ",comment)print(review_text)# Coding is the new SEXY 2. BeautifulSoupï¼šæ¸…æ´—HTMLã€åžƒåœ¾ç¬¦å·å¾ˆå¤šç½‘ä¸Šçˆ¬ä¸‹æ¥çš„è¯„ä»·å†…å®¹ä¼šå¸¦æœ‰HTMLç±»åž‹çš„æ–‡æœ¬ï¼Œéƒ½æ˜¯æ— æ•ˆä¿¡æ¯éœ€è¦åˆ é™¤ï¼Œé¦–å…ˆâ€pip install beautifulsoup4â€œ 1234567from bs4 import BeautifulSoupreview = '&lt;br /&gt;&lt;br /&gt;\"Elvira, Mistress of the Dark\"'review_text = BeautifulSoup(review).get_text()print(review_text)# "Elvira, Mistress of the Dark" è¿™æ ·æ´—å‡ºæ¥çš„æ–‡æœ¬â€Elvira, Mistress of the Darkâ€å°±æ˜¯çœŸæ­£æœ‰æ•ˆçš„ä¿¡æ¯äº†ã€‚ äº†è§£æ–‡æœ¬æ¸…æ´—æŠ€å·§ä¹‹åŽï¼Œå›žåˆ°å½±è¯„æƒ…æ„Ÿåˆ†æžé¢˜æœ¬èº«ï¼ŒæŽ¥ä¸‹æ¥å¼€å§‹æ­£æ–‡ã€‚ ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ–‡æœ¬æ¸…æ´—å‡½æ•°å½±è¯„åˆ†æžé¢˜çš„æ–‡æœ¬æ˜¯è‹±æ–‡çš„ï¼Œé¦–å…ˆè¦åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå°†è¯„è®ºæ¸…ç†æˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„æ ¼å¼ã€‚æˆ‘ä»¬åªæƒ³è¦åŽŸå§‹æ–‡æœ¬ï¼Œè€Œä¸æ˜¯å…¶ä»–ç›¸å…³çš„HTMLï¼Œæˆ–å…¶ä»–åžƒåœ¾ç¬¦å·ã€‚ 12345678910111213141516171819import refrom bs4 import BeautifulSoup def review_to_wordlist(review): ''' Meant for converting each of the IMDB reviews into a list of words. ''' # First remove the HTML. review_text = BeautifulSoup(review).get_text() # Use regular expressions to only include words. review_text = re.sub("[^a-zA-Z]"," ", review_text) # Convert words to lower case and split them into separate words. words = review_text.lower().split() # Return a list of words # return(words) return(" ".join(words)) åŠ è½½æ•°æ®ï¼ŒæŒ‰ç…§ä¸Šé¢çš„å‡½æ•°å°†æ ·æœ¬æ•°æ®è¿›è¡Œæ¸…æ´—ï¼š 1234567891011import pandas as pd# load datadata = pd.read_csv('data/labeledTrainData.tsv',delimiter="\t")# clean dataclean_data = []for rv in data['review']: clean_data.append(review_to_wordlist(rv)) data['clean_review'] = clean_data æˆ‘è¿™é‡ŒæŠŠæ•´ä¸ªlabeled train dataåšä¸ºæ•´ä¸ªæ•°æ®é›†ï¼Œå°†å…¶æ‹†åˆ†æˆè®­ç»ƒé›†ã€æµ‹è¯•é›†ï¼š 1234from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(\ data['clean_review'],data['sentiment'], test_size=0.2, random_state=1) ç¬¬äºŒæ­¥ï¼šç”Ÿæˆè¯å‘é‡å…ˆçœ‹ä¸€ä¸‹å½±è¯„çš„å¹³å‡æ–‡æœ¬é•¿åº¦ï¼š 12data['clean_review'].apply(lambda x: len(x.split(" "))).mean()# 236.82856 ä½¿ç”¨kerasçš„Tokenizerè¿›è¡Œåˆ†è¯ï¼š 12345678910from keras.preprocessing.text import Tokenizerfrom keras.preprocessing.sequence import pad_sequencesmax_features = 6000 # å­—å…¸æœ€å¤§æ•°tokenizer = Tokenizer(num_words=max_features)tokenizer.fit_on_texts(X_train)list_tokenized_train = tokenizer.texts_to_sequences(X_train)maxlen = 130 # å¥å­æœ€å¤§é•¿åº¦X_tr = pad_sequences(list_tokenized_train, maxlen=maxlen) ç¬¬ä¸‰æ­¥ï¼š åˆ›å»º åˆ†ç±»å™¨/æ¨¡åž‹å¤©åº•ä¸‹çš„åˆ†ç±»å™¨åƒåƒä¸‡ï¼Œå¯ä»¥éšè‡ªå·±é€‰å‡ ä¸ªå°è¯•ä¸€ä¸‹ã€‚ BiLSTM Classifier 12345678910111213141516171819from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flattenfrom keras.layers import Bidirectional, GlobalMaxPool1Dfrom keras.models import Model, Sequentialfrom keras.layers import Convolution1Dfrom keras import initializers, regularizers, constraints, optimizers, layersembed_size = 256model = Sequential()model.add(Embedding(max_features, embed_size))model.add(Bidirectional(LSTM(32, return_sequences = True)))model.add(GlobalMaxPool1D())model.add(Dense(20, activation="relu"))model.add(Dropout(0.05))model.add(Dense(1, activation="sigmoid"))model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])batch_size = 100epochs = 5model.fit(X_tr,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2) éœ€è¦ä¸€ç‚¹æ—¶é—´ï¼Œè¿è¡Œæ—¶ä¼šå‡ºçŽ°ï¼š 123456789101112Train on 16000 samples, validate on 4000 samplesEpoch 1/516000/16000 [==============================] - 31s 2ms/step - loss: 0.4832 - acc: 0.7597 - val_loss: 0.3214 - val_acc: 0.8608Epoch 2/516000/16000 [==============================] - 30s 2ms/step - loss: 0.2633 - acc: 0.8929 - val_loss: 0.3142 - val_acc: 0.8642Epoch 3/516000/16000 [==============================] - 30s 2ms/step - loss: 0.1876 - acc: 0.9292 - val_loss: 0.3474 - val_acc: 0.8557Epoch 4/516000/16000 [==============================] - 29s 2ms/step - loss: 0.1211 - acc: 0.9593 - val_loss: 0.4179 - val_acc: 0.8560Epoch 5/516000/16000 [==============================] - 29s 2ms/step - loss: 0.0760 - acc: 0.9754 - val_loss: 0.5393 - val_acc: 0.8440&lt;keras.callbacks.History at 0x2684c124940&gt; ç¬¬å››æ­¥ï¼š æ¨¡åž‹è¯„ä¼°123456789list_sentences_test = X_testlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)prediction = model.predict(X_te)y_pred = (prediction &gt; 0.5)from sklearn.metrics import f1_score, confusion_matrixprint('F1-score: &#123;0&#125;'.format(f1_score(y_pred, y_test)))print('Confusion matrix:')confusion_matrix(y_pred, y_test) è¯„ä¼°ç»“æžœï¼š 1234F1-score: 0.8357478065700876Confusion matrix:array([[2147, 449], [ 356, 2048]], dtype=int64) æ•´ä¸ªæµç¨‹å°±å·®ä¸å¤šç®—å®Œæˆå•¦ï¼ŒæŽ¥ä¸‹æ¥å°±æ˜¯è¿›è¡Œæ¨¡åž‹ä¼˜åŒ–ï¼Œæˆ–è€…æ›´æ¢å…¶å®ƒåˆ†ç±»å™¨ã€‚ å†™åœ¨æœ€åŽè¿™ç¯‡å°±æ˜¯ç®€å•èµ°äº†ä¸ªæµç¨‹ï¼Œä»…ä½œç¤ºä¾‹ã€‚é™¤äº†æœ¬æ–‡æåˆ°çš„æŠ€å·§ï¼Œè¿˜æœ‰å¾ˆå¤šç»†èŠ‚å¯ä»¥å¡«å……ï¼Œæ¯”å¦‚åŽ»æŽ‰åœç”¨è¯ç­‰ï¼›è¿˜æœ‰ç»†èŠ‚å¯ä»¥ä¼˜åŒ–ï¼Œæ¯”å¦‚è°ƒæ•´åµŒå…¥ç»´åº¦ç­‰ã€‚ä¹‹åŽæœ‰ç©ºè¿˜ä¼šç»§ç»­ç»´æŠ¤æœ¬ç¯‡ï¼Œå¡«å……æ›´å¤šæœ‰æ•ˆå†…å®¹ã€‚ æ›´å¤šå‚è€ƒï¼š Random Forest Classifier Natural Language Processing in a Kaggle Competition: Movie Reviews https://nbviewer.jupyter.org/github/jmsteinw/ https://www.kaggle.com/jatinmittal0001/word2vec Kaggle - Bag of Words Meets Bags of Popcorn]]></content>
      <tags>
        <tag>kaggle</tag>
        <tag>NLP</tag>
        <tag>sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POS tagging è¯æ€§æ ‡æ³¨ ä¹‹ æ­¦æž—å¤–ä¼ ç‰ˆ]]></title>
    <url>%2F2019%2F03%2F12%2Fpos%2F</url>
    <content type="text"><![CDATA[è¯ç±»ï¼ŒPOS(Part Of Speech)ï¼Œå°±æ˜¯æ‰€è°“çš„åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€ä»£è¯ã€ä»‹è¯ç­‰è¯æ€§åˆ†ç±»ã€‚è¯æ€§æ ‡æ³¨ï¼ˆPOS taggingï¼‰å°±æ˜¯æ ‡è®°/åˆ¤æ–­ä¸€ä¸ªè¯å±žäºŽä»€ä¹ˆè¯æ€§ã€‚è¯ç±»æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š åˆ†å¸ƒç‰¹å¾(Distributional)å•è¯èƒ½å¤Ÿå‡ºçŽ°åœ¨ç›¸ä¼¼çš„çŽ¯å¢ƒä¸­å•è¯æœ‰ç›¸ä¼¼çš„åŠŸèƒ½ å½¢æ€ç‰¹å¾(Morphological)å•è¯æœ‰ç›¸åŒçš„å‰ç¼€åŽç¼€(è¯ç¼€å…·æœ‰ç›¸ä¼¼çš„åŠŸèƒ½)åœ¨å¥æ³•ç»“æž„ä¸­æœ‰ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡çŽ¯å¢ƒ æ— å…³äºŽå«ä¹‰(meaning)ï¼Œä¹Ÿæ— å…³äºŽè¯­æ³•(å¯ä»¥æ˜¯ä¸»è¯­/å®¾è¯­ï¼Œç­‰ç­‰) è¯çš„å±žæ€§å¯ä»¥æä¾›å¾ˆå¤šä¿¡æ¯ï¼š å½¢å®¹è¯ åŽé¢è·Ÿçš„å¾€å¾€æ˜¯ä¸€ä¸ªåè¯ï¼›ä¸€å¥è¯é‡Œ åè¯ é€šå¸¸æ˜¯æ¯”è¾ƒé‡è¦çš„ä¿¡æ¯ï¼Œè€Œ ä»‹è¯ å¯èƒ½æ¯”è¾ƒä¸é‡è¦ã€‚æ¯”å¦‚ â€œåŒç¦å®¢æ ˆçš„æŽŒæŸœæ˜¯è°ï¼Ÿâ€ è¿™å¥è¯é‡Œï¼Œé‡è¦çš„è¯æœ‰ åè¯ï¼šâ€œå®¢æ ˆâ€ã€â€œæŽŒæŸœâ€ ç–‘é—®è¯ï¼šâ€œè°â€ è¯ç±»æ ‡æ³¨æ˜¯æ­§ä¹‰æ¶ˆè§£(disambiguation) çš„ä¸€ä¸ªé‡è¦æ–¹é¢ã€‚å¾ˆå¤šæ¬¡ä¸ä»…ä»…æœ‰ä¸€ä¸ªè¯æ€§ï¼Œå½“ä¸åŒè¯æ€§æ—¶ä»£è¡¨çš„æ„æ€ä¸åŒã€‚æ¯”å¦‚â€œæŽ’å±±å€’æµ·â€ï¼ŒåŽŸå…ˆæ˜¯ä¸€ä¸ªå½¢å®¹è¯ï¼Œç”¨æ¥å½¢å®¹å£°åŠ¿æµ©å¤§ã€‚ä½†æ˜¯å¦‚æžœæ˜¯å‡ºçŽ°åœ¨éƒ­èŠ™è“‰çš„å˜´é‡Œï¼Œé‚£åŸºæœ¬è¡¨ç¤ºä¸€ä¸ªæ‹›å¼åç§°ï¼Œæ˜¯ä¸€ä¸ªåè¯ã€‚ ä¸­æ–‡ POS tagging ä½“éªŒé“¾æŽ¥ï¼šç™¾åº¦ ã€è…¾è®¯ Python Packagejieba ðŸ”—jiebaæ˜¯ä¼˜ç§€çš„ä¸­æ–‡åˆ†è¯å·¥å…·ï¼ŒåŒæ ·ä¹Ÿæœ‰è¯æ€§æ ‡æ³¨çš„åŠŸèƒ½ã€‚é¦–å…ˆè¯·ç¡®ä¿â€pip install jiebaâ€ï¼Œæ¥ä¸ªå•æ¡queryï¼š 1234import jieba.posseg as psegwords = pseg.cut("ä½ŸæŽŒæŸœå–œæ¬¢çš„äººæ˜¯è°ï¼Ÿ")for word, flag in words: print(word, flag) output: 12345678ä½Ÿ nræŽŒæŸœ nå–œæ¬¢ vçš„ ujäºº næ˜¯ vè° rï¼Ÿ w å¯ä»¥æ ¹æ®ä¸‹é¢çš„jiebaçš„è¯æ€§å¯¹ç…§è¡¨ï¼ˆä¼šæœ‰å˜åŠ¨ï¼‰å¯¹ä¸Šé¢çš„outputè¿›è¡Œè§£è¯»ï¼š jiebaè¯æ€§å¯¹ç…§è¡¨ ç¼–ç  åç§° æ³¨è§£ ag å½¢è¯­ç´  å½¢å®¹è¯æ€§è¯­ç´ ã€‚å½¢å®¹è¯ä»£ç ä¸º aï¼Œè¯­ç´ ä»£ç ï½‡å‰é¢ç½®ä»¥Aã€‚ a å½¢å®¹è¯ å–è‹±è¯­å½¢å®¹è¯ adjectiveçš„ç¬¬1ä¸ªå­—æ¯ã€‚ ad å‰¯å½¢è¯ ç›´æŽ¥ä½œçŠ¶è¯­çš„å½¢å®¹è¯ã€‚å½¢å®¹è¯ä»£ç  aå’Œå‰¯è¯ä»£ç då¹¶åœ¨ä¸€èµ·ã€‚ an åå½¢è¯ å…·æœ‰åè¯åŠŸèƒ½çš„å½¢å®¹è¯ã€‚å½¢å®¹è¯ä»£ç  aå’Œåè¯ä»£ç nå¹¶åœ¨ä¸€èµ·ã€‚ b åŒºåˆ«è¯ å–æ±‰å­—â€œåˆ«â€çš„å£°æ¯ã€‚ c è¿žè¯ å–è‹±è¯­è¿žè¯ conjunctionçš„ç¬¬1ä¸ªå­—æ¯ã€‚ dg å‰¯è¯­ç´  å‰¯è¯æ€§è¯­ç´ ã€‚å‰¯è¯ä»£ç ä¸º dï¼Œè¯­ç´ ä»£ç ï½‡å‰é¢ç½®ä»¥Dã€‚ d å‰¯è¯ å– adverbçš„ç¬¬2ä¸ªå­—æ¯ï¼Œå› å…¶ç¬¬1ä¸ªå­—æ¯å·²ç”¨äºŽå½¢å®¹è¯ã€‚ e å¹è¯ å–è‹±è¯­å¹è¯ exclamationçš„ç¬¬1ä¸ªå­—æ¯ã€‚ f æ–¹ä½è¯ å–æ±‰å­—â€œæ–¹â€ g è¯­ç´  ç»å¤§å¤šæ•°è¯­ç´ éƒ½èƒ½ä½œä¸ºåˆæˆè¯çš„â€œè¯æ ¹â€ï¼Œå–æ±‰å­—â€œæ ¹â€çš„å£°æ¯ã€‚ h å‰æŽ¥æˆåˆ† å–è‹±è¯­ headçš„ç¬¬1ä¸ªå­—æ¯ã€‚ i æˆè¯­ å–è‹±è¯­æˆè¯­ idiomçš„ç¬¬1ä¸ªå­—æ¯ã€‚ j ç®€ç§°ç•¥è¯­ å–æ±‰å­—â€œç®€â€çš„å£°æ¯ã€‚ k åŽæŽ¥æˆåˆ† l ä¹ ç”¨è¯­ ä¹ ç”¨è¯­å°šæœªæˆä¸ºæˆè¯­ï¼Œæœ‰ç‚¹â€œä¸´æ—¶æ€§â€ï¼Œå–â€œä¸´â€çš„å£°æ¯ã€‚ m æ•°è¯ å–è‹±è¯­ numeralçš„ç¬¬3ä¸ªå­—æ¯ï¼Œnï¼Œuå·²æœ‰ä»–ç”¨ã€‚ ng åè¯­ç´  åè¯æ€§è¯­ç´ ã€‚åè¯ä»£ç ä¸º nï¼Œè¯­ç´ ä»£ç ï½‡å‰é¢ç½®ä»¥Nã€‚ n åè¯ å–è‹±è¯­åè¯ nounçš„ç¬¬1ä¸ªå­—æ¯ã€‚ nr äººå åè¯ä»£ç  nå’Œâ€œäºº(ren)â€çš„å£°æ¯å¹¶åœ¨ä¸€èµ·ã€‚ ns åœ°å åè¯ä»£ç  nå’Œå¤„æ‰€è¯ä»£ç så¹¶åœ¨ä¸€èµ·ã€‚ nt æœºæž„å›¢ä½“ â€œå›¢â€çš„å£°æ¯ä¸º tï¼Œåè¯ä»£ç nå’Œtå¹¶åœ¨ä¸€èµ·ã€‚ nz å…¶ä»–ä¸“å â€œä¸“â€çš„å£°æ¯çš„ç¬¬ 1ä¸ªå­—æ¯ä¸ºzï¼Œåè¯ä»£ç nå’Œzå¹¶åœ¨ä¸€èµ·ã€‚ o æ‹Ÿå£°è¯ å–è‹±è¯­æ‹Ÿå£°è¯ onomatopoeiaçš„ç¬¬1ä¸ªå­—æ¯ã€‚ p ä»‹è¯ å–è‹±è¯­ä»‹è¯ prepositionalçš„ç¬¬1ä¸ªå­—æ¯ã€‚ q é‡è¯ å–è‹±è¯­ quantityçš„ç¬¬1ä¸ªå­—æ¯ã€‚ r ä»£è¯ å–è‹±è¯­ä»£è¯ pronounçš„ç¬¬2ä¸ªå­—æ¯,å› på·²ç”¨äºŽä»‹è¯ã€‚ s å¤„æ‰€è¯ å–è‹±è¯­ spaceçš„ç¬¬1ä¸ªå­—æ¯ã€‚ tg æ—¶è¯­ç´  æ—¶é—´è¯æ€§è¯­ç´ ã€‚æ—¶é—´è¯ä»£ç ä¸º t,åœ¨è¯­ç´ çš„ä»£ç gå‰é¢ç½®ä»¥Tã€‚ t æ—¶é—´è¯ å–è‹±è¯­ timeçš„ç¬¬1ä¸ªå­—æ¯ã€‚ u åŠ©è¯ å–è‹±è¯­åŠ©è¯ auxiliary vg åŠ¨è¯­ç´  åŠ¨è¯æ€§è¯­ç´ ã€‚åŠ¨è¯ä»£ç ä¸º vã€‚åœ¨è¯­ç´ çš„ä»£ç gå‰é¢ç½®ä»¥Vã€‚ v åŠ¨è¯ å–è‹±è¯­åŠ¨è¯ verbçš„ç¬¬ä¸€ä¸ªå­—æ¯ã€‚ vd å‰¯åŠ¨è¯ ç›´æŽ¥ä½œçŠ¶è¯­çš„åŠ¨è¯ã€‚åŠ¨è¯å’Œå‰¯è¯çš„ä»£ç å¹¶åœ¨ä¸€èµ·ã€‚ vn ååŠ¨è¯ æŒ‡å…·æœ‰åè¯åŠŸèƒ½çš„åŠ¨è¯ã€‚åŠ¨è¯å’Œåè¯çš„ä»£ç å¹¶åœ¨ä¸€èµ·ã€‚ w æ ‡ç‚¹ç¬¦å· x éžè¯­ç´ å­— éžè¯­ç´ å­—åªæ˜¯ä¸€ä¸ªç¬¦å·ï¼Œå­—æ¯ xé€šå¸¸ç”¨äºŽä»£è¡¨æœªçŸ¥æ•°ã€ç¬¦å·ã€‚ y è¯­æ°”è¯ å–æ±‰å­—â€œè¯­â€çš„å£°æ¯ã€‚ z çŠ¶æ€è¯ å–æ±‰å­—â€œçŠ¶â€çš„å£°æ¯çš„å‰ä¸€ä¸ªå­—æ¯ã€‚ un æœªçŸ¥è¯ ä¸å¯è¯†åˆ«è¯åŠç”¨æˆ·è‡ªå®šä¹‰è¯ç»„ã€‚å–è‹±æ–‡Unknowné¦–ä¸¤ä¸ªå­—æ¯ã€‚(éžåŒ—å¤§æ ‡å‡†ï¼ŒCSWåˆ†è¯ä¸­å®šä¹‰) å†æ¥ä¸ªå¤šæ¡queryçš„ä¾‹å­ï¼š 12345678query = ['ç§€æ‰æœ€å–œæ¬¢è¯´çš„è¯æ˜¯ä»€ä¹ˆ','è€ç™½çš„å¤–å·æ˜¯ä»€ä¹ˆ','éƒ­èŠ™è“‰çš„æƒ…æ•Œæ˜¯è°']for q in query: d = &#123;&#125; words = pseg.cut(q) for word, flag in words: d[word] = flag print(d) outputï¼š 123&#123;'å–œæ¬¢': 'v', 'è¯´': 'v', 'æ˜¯': 'v', 'çš„è¯': 'u', 'æœ€': 'd', 'ç§€æ‰': 'n', 'ä»€ä¹ˆ': 'r'&#125;&#123;'çš„': 'uj', 'æ˜¯': 'v', 'è€ç™½': 'nr', 'ä»€ä¹ˆ': 'r', 'å¤–å·': 'n'&#125;&#123;'æ˜¯': 'v', 'éƒ­': 'nr', 'èŠ™è“‰': 'n', 'çš„': 'uj', 'æƒ…æ•Œ': 'n', 'è°': 'r'&#125; åŸºæœ¬ä¸Šè¿˜æ˜¯æŒºå‡†çš„ã€‚ HanLP ðŸ”—HanLPå®žé™…ä¸Šæ˜¯Javaå†™çš„ï¼Œpyhanlpæ‰æ˜¯pythonæŽ¥å£ï¼Œå› æ­¤ä¸‹è½½æ˜¯â€pip install pyhanlpâ€ 12345from pyhanlp import *print(HanLP.segment('è€ç™½çš„çœŸå®žèº«ä»½æ˜¯ä»€ä¹ˆ'))for term in HanLP.segment('è€ç™½çš„çœŸå®žèº«ä»½æ˜¯ä»€ä¹ˆ'): print('&#123;&#125;\t&#123;&#125;'.format(term.word, term.nature)) # èŽ·å–å•è¯ä¸Žè¯æ€§ outputï¼š 1234567[è€ç™½/nz, çš„/ude1, çœŸå®ž/a, èº«ä»½/n, æ˜¯/vshi, ä»€ä¹ˆ/ry]è€ç™½ nzçš„ ude1çœŸå®ž aèº«ä»½ næ˜¯ vshiä»€ä¹ˆ ry HanLPçš„è¯æ€§å¯¹ç…§è¡¨ ç¼–ç  åç§° ç¼–ç  åç§° ç¼–ç  åç§° P 1 =========== === ========================== ===== ============== a å½¢å®¹è¯ gc åŒ–å­¦ç›¸å…³è¯æ±‡ nf é£Ÿå“ï¼Œæ¯”å¦‚â€œè–¯ç‰‡â€ ad å‰¯å½¢è¯ gg åœ°ç†åœ°è´¨ç›¸å…³è¯æ±‡ ng åè¯æ€§è¯­ç´  ag å½¢å®¹è¯æ€§è¯­ç´  gi è®¡ç®—æœºç›¸å…³è¯æ±‡ nh åŒ»è¯ç–¾ç—…ç­‰å¥åº·ç›¸å…³åè¯ al å½¢å®¹è¯æ€§æƒ¯ç”¨è¯­ gm æ•°å­¦ç›¸å…³è¯æ±‡ nhd ç–¾ç—… an åå½¢è¯ gp ç‰©ç†ç›¸å…³è¯æ±‡ nhm è¯å“ b åŒºåˆ«è¯ h å‰ç¼€ ni æœºæž„ç›¸å…³ï¼ˆä¸æ˜¯ç‹¬ç«‹æœºæž„åï¼‰ bg åŒºåˆ«è¯­ç´  i æˆè¯­ nic ä¸‹å±žæœºæž„ bl åŒºåˆ«è¯æ€§æƒ¯ç”¨è¯­ j ç®€ç§°ç•¥è¯­ nis æœºæž„åŽç¼€ c è¿žè¯ k åŽç¼€ nit æ•™è‚²ç›¸å…³æœºæž„ cc å¹¶åˆ—è¿žè¯ l ä¹ ç”¨è¯­ nl åè¯æ€§æƒ¯ç”¨è¯­ d å‰¯è¯ m æ•°è¯ nm ç‰©å“å dg è¾„,ä¿±,å¤ä¹‹ç±»çš„å‰¯è¯ mg æ•°è¯­ç´  nmc åŒ–å­¦å“å dl è¿žè¯­ Mg ç”²ä¹™ä¸™ä¸ä¹‹ç±»çš„æ•°è¯ nn å·¥ä½œç›¸å…³åè¯ e å¹è¯ mq æ•°é‡è¯ nnd èŒä¸š end ä»…ç”¨äºŽç»ˆ##ç»ˆ n åè¯ nnt èŒåŠ¡èŒç§° f æ–¹ä½è¯ nb ç”Ÿç‰©å nr äººå g å­¦æœ¯è¯æ±‡ nba åŠ¨ç‰©å nr1 å¤å§“ gb ç”Ÿç‰©ç›¸å…³è¯æ±‡ nbc åŠ¨ç‰©çº²ç›® nr2 è’™å¤å§“å gbc ç”Ÿç‰©ç±»åˆ« nbp æ¤ç‰©å nrf éŸ³è¯‘äººå P 2 =========== === ========================== ===== ============== nrj æ—¥è¯­äººå qg é‡è¯è¯­ç´  ud åŠ©è¯ ns åœ°å qt æ—¶é‡è¯ ude1 çš„ åº• nsf éŸ³è¯‘åœ°å qv åŠ¨é‡è¯ ude2 åœ° nt æœºæž„å›¢ä½“å r ä»£è¯ ude3 å¾— ntc å…¬å¸å rg ä»£è¯æ€§è¯­ç´  udeng ç­‰ ç­‰ç­‰ äº‘äº‘ ntcb é“¶è¡Œ Rg å¤æ±‰è¯­ä»£è¯æ€§è¯­ç´  udh çš„è¯ ntcf å·¥åŽ‚ rr äººç§°ä»£è¯ ug è¿‡ ntch é…’åº—å®¾é¦† ry ç–‘é—®ä»£è¯ uguo è¿‡ nth åŒ»é™¢ rys å¤„æ‰€ç–‘é—®ä»£è¯ uj åŠ©è¯ nto æ”¿åºœæœºæž„ ryt æ—¶é—´ç–‘é—®ä»£è¯ ul è¿žè¯ nts ä¸­å°å­¦ ryv è°“è¯æ€§ç–‘é—®ä»£è¯ ule äº† å–½ ntu å¤§å­¦ rz æŒ‡ç¤ºä»£è¯ ulian è¿ž ï¼ˆâ€œè¿žå°å­¦ç”Ÿéƒ½ä¼šâ€ï¼‰ nx å­—æ¯ä¸“å rzs å¤„æ‰€æŒ‡ç¤ºä»£è¯ uls æ¥è®² æ¥è¯´ è€Œè¨€ è¯´æ¥ nz å…¶ä»–ä¸“å rzt æ—¶é—´æŒ‡ç¤ºä»£è¯ usuo æ‰€ o æ‹Ÿå£°è¯ rzv è°“è¯æ€§æŒ‡ç¤ºä»£è¯ uv è¿žè¯ p ä»‹è¯ s å¤„æ‰€è¯ uyy ä¸€æ · ä¸€èˆ¬ ä¼¼çš„ èˆ¬ pba ä»‹è¯â€œæŠŠâ€ t æ—¶é—´è¯ uz ç€ pbei ä»‹è¯â€œè¢«â€ tg æ—¶é—´è¯æ€§è¯­ç´  uzhe ç€ q é‡è¯ u åŠ©è¯ uzhi ä¹‹ P 3 =========== === ========================== ===== ============== v åŠ¨è¯ wb ç™¾åˆ†å·åƒåˆ†å·ï¼Œå…¨è§’ï¼šï¼… â€° åŠè§’ï¼š% wt å¹å·ï¼Œå…¨è§’ï¼šï¼ vd å‰¯åŠ¨è¯ wd é€—å·ï¼Œå…¨è§’ï¼šï¼Œ åŠè§’ï¼š, ww é—®å·ï¼Œå…¨è§’ï¼šï¼Ÿ vf è¶‹å‘åŠ¨è¯ wf åˆ†å·ï¼Œå…¨è§’ï¼šï¼› åŠè§’ï¼š ; wyy å³å¼•å·ï¼Œå…¨è§’ï¼šâ€ â€™ ã€ vg åŠ¨è¯æ€§è¯­ç´  wh å•ä½ç¬¦å·ï¼Œå…¨è§’ï¼šï¿¥ ï¼„ ï¿¡ Â° â„ƒ åŠè§’ï¼š$ wyz å·¦å¼•å·ï¼Œå…¨è§’ï¼šâ€œ â€˜ ã€Ž vi ä¸åŠç‰©åŠ¨è¯ï¼ˆå†…åŠ¨è¯ï¼‰ wj å¥å·ï¼Œå…¨è§’ï¼šã€‚ x å­—ç¬¦ä¸² vl åŠ¨è¯æ€§æƒ¯ç”¨è¯­ wky å³æ‹¬å·ï¼Œå…¨è§’ï¼šï¼‰ ã€• ï¼½ ï½ ã€‹ ã€‘ ã€— ã€‰ åŠè§’ï¼š ) ] { &gt; xu ç½‘å€URL vn ååŠ¨è¯ wkz å·¦æ‹¬å·ï¼Œå…¨è§’ï¼šï¼ˆ ã€” ï¼» ï½› ã€Š ã€ ã€– ã€ˆ åŠè§’ï¼š( [ { &lt; xx éžè¯­ç´ å­— vshi åŠ¨è¯â€œæ˜¯â€ wm å†’å·ï¼Œå…¨è§’ï¼šï¼š åŠè§’ï¼š : y è¯­æ°”è¯(delete yg) vx å½¢å¼åŠ¨è¯ wn é¡¿å·ï¼Œå…¨è§’ï¼šã€ yg è¯­æ°”è¯­ç´  vyou åŠ¨è¯â€œæœ‰â€ wp ç ´æŠ˜å·ï¼Œå…¨è§’ï¼šâ€”â€” ï¼ï¼ â€”â€”ï¼ åŠè§’ï¼šâ€” â€”- z çŠ¶æ€è¯ w æ ‡ç‚¹ç¬¦å· ws çœç•¥å·ï¼Œå…¨è§’ï¼šâ€¦â€¦ â€¦ zg çŠ¶æ€è¯ å†æ¥ä¸ªå¤šqueryä¾‹å­ï¼š 12345testCases = ['è€ç™½çš„çœŸå®žèº«ä»½æ˜¯ä»€ä¹ˆ','ç›—åœ£æ˜¯è°']for sentence in testCases: print(HanLP.segment(sentence)) # [è€ç™½/nz, çš„/ude1, çœŸå®ž/a, èº«ä»½/n, æ˜¯/vshi, ä»€ä¹ˆ/ry]# [ç›—/vg, åœ£/ag, æ˜¯/vshi, è°/ry] è¿™é‡Œçš„â€œç›—åœ£â€è¢«åˆ†å¼€äº†ï¼Œåˆ†æˆâ€œç›—â€å’Œâ€œåœ£â€ï¼Œä¸€ä¸ªåŠ¨è¯ä¸€ä¸ªå½¢å®¹è¯ï¼Œæ²¡æœ‰è¢«è¯†åˆ«æˆä¸“æœ‰åè¯ï¼Œæ˜¯å› ä¸ºè®­ç»ƒçš„æ—¶å€™æ²¡æœ‰è¿™ä¸ªæ ·æœ¬ï¼Œçš„ç¡®ç›—åœ£è¿™ä¸ªä¹Ÿå¾ˆå°‘åœ¨å…¶ä»–åœºæ™¯/å°è¯´/ç”µè§†å‰§ç­‰åœ°æ–¹å‡ºçŽ°ã€‚ é¡ºä¾¿å†æ¥çœ‹çœ‹HanLPçš„å…¶ä»–åŠŸèƒ½ï¼š å…³é”®è¯æå– 123456document = 'è€ç™½çš„çœŸå®žèº«ä»½æ˜¯ä»€ä¹ˆ'print(HanLP.extractKeyword(document, 2))# output:[è€ç™½, çœŸå®ž]print(HanLP.extractKeyword(document, 3))# output:[è€ç™½, èº«ä»½, çœŸå®ž] è‡ªåŠ¨æ‘˜è¦ è¿™é‡Œçš„è‡ªåŠ¨æ‘˜è¦ä¹Ÿæ˜¯æ¯”è¾ƒé‡è¦çš„åŠŸèƒ½ï¼Œå› ä¸ºæ¯”å†™è®ºæ–‡æ›´å¤´ç–¼çš„æ˜¯è¿˜è¦å†™æ‘˜è¦ï¼Œå¦‚æžœè‡ªåŠ¨æ‘˜è¦æŠ€æœ¯æˆç†ŸåŽï¼Œè®ºæ–‡çš„æ‘˜è¦å°±å¯ä»¥è‡ªåŠ¨ç”Ÿæˆäº†ã€‚åŒ…æ‹¬è¯»é•¿æ–‡ç« å°±å¯ä»¥å…ˆçœ‹æ‘˜è¦å†å†³å®šè¦ä¸è¦æ·±å…¥çœ‹ä¸‹åŽ»ã€‚ 1234print(HanLP.extractSummary('ä¸€ä¸ªæœˆé»‘é£Žé«˜çš„æ€äººå¤œï¼Œä¼ è¯´ä¸­çš„é›Œé›„åŒç…žä»Žå¤©è€Œé™ï¼Œæ‰“ä¹±äº†åŒç¦å®¢æ ˆçš„å®‰ç¨³æ—¥å­ã€‚å®¶ä¸–æ˜¾èµ«ã€ä»Žå°å¨‡ç”Ÿæƒ¯å…»çš„éƒ­èŠ™è“‰ï¼Œçˆ¶äº²æ˜¯ä¸€ä»£å¤§ä¾ ï¼Œå§‹ç»ˆæŠŠå¥¹ç¬¼ç½©åœ¨é˜´å½±ä¹‹ä¸‹ã€‚ä»Žå°äº‰èƒœå¥½èƒœçš„å¥¹ï¼Œæ¯…ç„¶é€‰æ‹©äº†ä¸€æ¡ç¦»å®¶å‡ºèµ°ç‹¬é—¯æ±Ÿæ¹–çš„è·¯ï¼Œå´åœ¨ç¬¬ä¸€ç«™ï¼Œè¢«æ‰£åœ¨äº†åŒç¦å®¢æ ˆï¼Œä»Žæ­¤å¼€å§‹äº†è‰°è‹¦å“ç»çš„æ‚å½¹ç”Ÿæ¶¯â€¦â€¦', 1))# output# [è¢«æ‰£åœ¨äº†åŒç¦å®¢æ ˆ] ä¾å­˜å¥æ³•åˆ†æž 123456789HanLP.parseDependency("è€ç™½çš„çœŸå®žèº«ä»½æ˜¯ä»€ä¹ˆ")# output# 1 è€ç™½ è€ç™½ nh nr _ 4 å®šä¸­å…³ç³» _ _# 2 çš„ çš„ u u _ 1 å³é™„åŠ å…³ç³» _ _# 3 çœŸå®ž çœŸå®ž a a _ 4 å®šä¸­å…³ç³» _ _# 4 èº«ä»½ èº«ä»½ n n _ 5 ä¸»è°“å…³ç³» _ _# 5 æ˜¯ æ˜¯ v v _ 0 æ ¸å¿ƒå…³ç³» _ _# 6 ä»€ä¹ˆ ä»€ä¹ˆ r r _ 5 åŠ¨å®¾å…³ç³» _ _ è¯æ€§æ ‡æ³¨çš„ä¸€ä¸ªåº”ç”¨æ–¹å‘æ˜¯åœ¨çŸ¥è¯†å›¾è°±é‡Œï¼Œå½“ä½ ç¡®å®šè¯æ€§æ—¶ï¼Œæ›´åŠ èƒ½æ–¹ä¾¿æŠŠå¯èƒ½çš„ç­”æ¡ˆæªå‡ºæ¥ã€‚]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sentiment analysis]]></title>
    <url>%2F2019%2F03%2F11%2Fsentiment-analysis%2F</url>
    <content type="text"><![CDATA[Sentiment Analysisï¼ˆæƒ…æ„Ÿåˆ†æžï¼‰ï¼Œåˆç§°å€¾å‘æ€§åˆ†æžï¼Œæ„è§æŠ½å–ï¼ˆOpinion extractionï¼‰ï¼Œæ„è§æŒ–æŽ˜ï¼ˆOpinion miningï¼‰ï¼Œæƒ…æ„ŸæŒ–æŽ˜ï¼ˆSentiment miningï¼‰ï¼Œä¸»è§‚åˆ†æžï¼ˆSubjectivity analysisï¼‰ï¼Œå®ƒæ˜¯å¯¹å¸¦æœ‰æƒ…æ„Ÿè‰²å½©çš„ä¸»è§‚æ€§æ–‡æœ¬è¿›è¡Œåˆ†æžã€å¤„ç†ã€å½’çº³å’ŒæŽ¨ç†çš„è¿‡ç¨‹ï¼Œæ˜¯NLPä¸­ä¸€ç§å¸¸è§åº”ç”¨ã€‚ å¤§è‡´æœ‰ä»¥ä¸‹å‡ ç§åº”ç”¨åœºæ™¯ï¼š å•†å“è¯„ä»· åœ¨ä¸Šç½‘è´­ç‰©å¸ç©ºè§æƒ¯çš„æ—¶ä»£ï¼Œè´­ç‰©è¯„ä»·é“ºå¤©ç›–åœ°ï¼Œæ·˜å®/äº¬ä¸œ/å¤§ä¼—ç‚¹è¯„/â€¦ï¼Œå®¢æˆ·çš„è¯„ä»·ï¼ˆå¥½è¯„positive/å·®è¯„negativeï¼‰å˜å¾—å¯¹æ¶ˆè´¹è€…ã€åº—å®¶å’Œappå¹³å°è¶Šæ¥è¶Šé‡è¦ã€‚å¾ˆå¤šå•†æˆ·éœ€è¦åŠæ—¶åˆ†æžè‡ªå®¶äº§å“çš„è¯„ä»·ï¼Œå‘çŽ°é—®é¢˜è¿›è¡Œäº§å“ä¼˜åŒ–ã€‚ ç¤¾äº¤è´¦å·ç•™è¨€ æ¯”å¦‚åœ¨å¾®åšä¸Šæœ‰å¾ˆå¤šå¤§Væˆ–æ˜Žæ˜Ÿè¦åšè¯„æŽ§ï¼Œæ‰€ä»¥çŸ­æ—¶é—´å†…å¤„ç†å¤§é‡è¯„è®ºæˆä¸ºäº†éœ€æ±‚ã€‚ä¾‹å¦‚åé»‘ä¸€ç›´æ˜¯é¥­åœˆæ¯”è¾ƒå¤´ç—›çš„äº‹æƒ…ï¼Œç›®å‰çš„åšæ³•æ˜¯é äººå·¥è¿›è¡Œè¿‡æ»¤ï¼ŒSentiment Analysiså°±èƒ½æ™ºèƒ½åé»‘ï¼Œç”¨æ•°æ®ç§‘å­¦ç¢¾åŽ‹ç«žäº‰è€…ã€‚ èˆ†æƒ…åˆ†æž åˆ©ç”¨å…¬å…±ä¿¡æ¯æ¥åˆ¤æ–­è¶‹åŠ¿ï¼Œæ¯”å¦‚æ–°æ”¿ç­–çš„å‡ºå°æ˜¯å¦å—æ°‘ä¼—å–œæ¬¢ï¼Œå…¬ä¼—å¯¹é€‰ä¸¾å€™é€‰äººçš„çœ‹æ³•å¦‚ä½•ã€‚ è¶‹åŠ¿é¢„æµ‹ æ ¹æ®èˆ†è®ºé¢„æµ‹é€‰ä¸¾ç»“æžœï¼›æ ¹æ®æ¶ˆè´¹è€…ä¿¡æ¯æŒ‡æ•°é¢„æµ‹å¸‚åœºè¶‹åŠ¿ï¼›æ—©åœ¨2010å¹´ï¼Œå°±æœ‰å­¦è€…æŒ‡å‡ºï¼Œä¾é Twitterå…¬å¼€ä¿¡æ¯çš„æƒ…æ„Ÿåˆ†æžæ¥é¢„æµ‹è‚¡å¸‚çš„æ¶¨è½ï¼Œå‡†ç¡®çŽ‡é«˜è¾¾87.6%ï¼ Sentiment Analysisçš„æœ¬è´¨æ˜¯è¡¨è¾¾ä¸€ç§æ€åº¦/è§‚ç‚¹ï¼ˆattitudeï¼‰ ï¼Œè¡¨è¾¾è§‚ç‚¹åŒ…å«ä»¥ä¸‹3ä¸ªå…ƒç´ ï¼š Holder / source of attitudeï¼ˆ = æŒæœ‰è§‚ç‚¹çš„äºº ï¼‰ Aspect / target of attitudeï¼ˆ = è§‚ç‚¹çš„è§’åº¦ ï¼‰ Type of attitudeï¼ˆ = è§‚ç‚¹çš„æƒ…ç»ª ï¼‰ positive, negative star rating (â­ ~ â­â­â­â­â­) hate, dislike, donâ€™t care, Like, love (make choice) Sentiment Analysis çš„å¸¸è§å¤„ç†æ–¹å¼æœ‰ä»¥ä¸‹å‡ ç§ï¼š docs -basedï¼š æ•´ä¸ªè§‚ç‚¹æ–‡æœ¬ç›´æŽ¥åˆ¤æ–­ sentences / phrases -basedï¼š å°†æ–‡æœ¬æ‹†æˆå¥å­æˆ–çŸ­è¯­å†è¿›è¡Œè§‚ç‚¹å’Œæƒ…ç»ªåˆ¤æ–­ aspects/targets/attributes -basedï¼š å…ˆåšè§‚ç‚¹æŠ½å–ï¼Œå†åšæƒ…ç»ªåˆ¤æ–­ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬é¢ä¸´çš„æƒ…æ„Ÿåˆ†æžä»»åŠ¡åŒ…æ‹¬å¦‚ä¸‹å‡ ç±»ï¼š Simplest task: Is the attitude of this text positive or negative? åŸºäºŽæ•´æ®µæ–‡å­—/å¥å­/çŸ­è¯­ ç›´æŽ¥åˆ¤æ–­æ­£è´Ÿå‘æƒ…ç»ª More complex: Rank the attitude of this text from 1 to 5 åŸºäºŽæ•´æ®µæ–‡å­—/å¥å­/çŸ­è¯­ ç›´æŽ¥åˆ¤æ–­æƒ…ç»ªå±‚æ¬¡ Advanced (eg. Aspect-based): Detect the target, source, or complex attitude types å…ˆåˆ¤æ–­å­˜åœ¨å“ªäº›è§‚ç‚¹çš„è§’åº¦ï¼Œå†åˆ¤æ–­æƒ…ç»ªå€¾å‘ ç›®å‰NLPè¢«ç ”ç©¶çš„æœ€å¤šçš„ä¸¤ç§è¯­è¨€æ˜¯è‹±æ–‡å’Œä¸­æ–‡ï¼Œè‹±æ–‡åœ¨ç½‘ä¸Šæœæœ‰å¾ˆå¤šå¥½ç”¨çš„å·¥å…·åœ¨æ­¤ä¸åšä»‹ç»ã€‚è¿™é‡Œä¸»è¦ä»‹ç»å¯¹ä¸­æ–‡æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†æžã€‚ æƒ…æ„Ÿå€¾å‘åˆ†æžä¸€èˆ¬æˆ‘ä»¬çœ‹è¯„ä»·çš„æ—¶å€™ï¼Œå¦‚æžœä¸€ä¸ªå•†å“å¥½è¯„å¤šï¼Œæˆ‘ä»¬å¯èƒ½ä¹°çš„æ›´æ”¾å¿ƒã€‚ä¸‹é¢æ˜¯äº›å…³äºŽæŸç”µå½±çš„è¯„ä»·ï¼š å…¬å¼€çš„sentiment analysisä½“éªŒæŽ¥å£ï¼Œå¯ä»¥éšæ„æ„Ÿå—ä¸€ä¸‹ï¼š è…¾è®¯ ç™¾åº¦ ï¼ˆæŽ¨èðŸ‘è™½ç„¶ä¸åƒæˆ‘çš„é£Žæ ¼ï¼Œä½†è¿™å—ç™¾åº¦åšçš„è¿˜å¯ä»¥ï¼‰ å¼€æº/å¥½ç”¨çš„å·¥å…·SnowNLP ï¼ˆðŸ”—é“¾æŽ¥ï¼‰GitHubä¸Šæœ‰å¼€æºåšæƒ…æ„Ÿåˆ†æžçš„å¥½ç”¨çš„pythonåŒ…ï¼Œæˆ‘ä»¬æ¥æ„Ÿå—ä¸€ä¸‹ã€‚é¦–å…ˆè¯·ç¡®ä¿è‡ªå·±å·²ç» â€˜pip install snownlpâ€™ ï¼Œå¯¹äºŽnlpé€‰æ‰‹ï¼Œè¿™ä¸ªåŒ…åº”è¯¥å¾ˆç†Ÿæ‚‰äº†ã€‚ä¸ä»…å¯ä»¥åšæƒ…æ„Ÿåˆ†æžï¼Œè¿˜å¯ä»¥åšä¸­æ–‡åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€æå–å…³é”®å­—ç­‰ç­‰ã€‚ 1234from snownlp import SnowNLPs = SnowNLP(u'è¿™ä¸ªä¸œè¥¿çœŸå¿ƒå¾ˆèµž')print(s.sentiments) # 0.9769551298267365 positiveçš„æ¦‚çŽ‡ éžå¸¸ç®€å•ï¼Œå°±3è¡Œcodeè§£å†³ã€‚å†æ¥è¯•è¯•å…¶ä»–ï¼š 12345678s = SnowNLP(u'è¿™ä¸ªä¸œè¥¿çœŸå¿ƒå¾ˆèµžï¼Œé€Ÿåº¦éžå¸¸å¿«ï¼Œéš”å¤©åˆ°ï¼Œé€è´§åˆ°å®¶ï¼Œå¿«é€’å‘˜æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œå€¼å¾—æŽ¨èï¼Œç‚¹ä¸ªèµžï¼')print(s.sentiments) # 0.9998864907454681 positiveçš„æ¦‚çŽ‡s = SnowNLP(u'å¸ƒæ–™ä¸å¥½ï¼Œä¸å»ºè®®è´­ä¹°')print(s.sentiments) # 0.11886826035520526 positiveçš„æ¦‚çŽ‡s = SnowNLP(u'é£Ÿç‰©å¾ˆç¾Žå‘³ï¼Œä½†æ˜¯æœåŠ¡å‘˜çš„æ€åº¦å¾ˆå·®åŠ²ï¼æ€»ä½“è¿˜å¯ä»¥å§ã€‚')print(s.sentiments) # 0.42427513441222864 positiveçš„æ¦‚çŽ‡ ç¬¬äºŒæ¯”ç¬¬ä¸€å¥çš„æ­£å‘è¯„è®ºæ›´å¤šï¼Œå› æ­¤positiveçš„æ¦‚çŽ‡ä¹Ÿæ›´å¤§äº†ã€‚ç¬¬ä¸‰å¥å±žäºŽè´Ÿé¢è¯„ä»·ï¼Œå› æ­¤åˆ†æ•°è¶ŠæŽ¥è¿‘0ä¸ºè´Ÿé¢ï¼ŒæŽ¥è¿‘1ä¸ºæ­£é¢ã€‚ SnowNLPçš„ä¼˜ç‚¹æ˜¯å¯ä»¥æ‹¿è‡ªå·±çš„æ•°æ®åŽ»è®­ç»ƒè¿™ä¸ªåˆ†ç±»å™¨ï¼Œç¼ºç‚¹æ˜¯åœ¨é•¿æ–‡æœ¬åˆ†æžæ—¶ï¼Œå°¤å…¶æ˜¯å‡ºçŽ°å¤šä¸ªè§‚ç‚¹æ—¶ï¼Œä¼šæœ‰é¢„æµ‹ä¸å‡†çš„æƒ…å†µã€‚ å½“è¯„ä»·æ˜¯é•¿æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥å…ˆåšåˆ†å¥ï¼Œå†å¯¹æ¯ä¸ªå¥å­è¿›è¡Œæƒ…æ„Ÿåˆ†æžï¼š 123456789101112s = SnowNLP(u'é£Ÿç‰©å¾ˆç¾Žå‘³ï¼Œä½†æ˜¯æœåŠ¡å‘˜çš„æ€åº¦å¾ˆå·®åŠ²ï¼æ€»ä½“è¿˜å¯ä»¥å§ã€‚')for sentence in s.sentences: print(sentence) sentc = SnowNLP(sentence) print(sentc.sentiments) # é£Ÿç‰©å¾ˆç¾Žå‘³# 0.7625799472162259# ä½†æ˜¯æœåŠ¡å‘˜çš„æ€åº¦å¾ˆå·®åŠ²# 0.059144753698306296# æ€»ä½“è¿˜å¯ä»¥å§# 0.8182792359070481 ç™¾åº¦APIè¿™é‡Œçš„APP_IDã€API_KEYã€SECRET_KEYéœ€è¦è‡ªå·±ç”³è¯·ï¼Œä¼ é€é—¨ï¼šå¼€å‘æŒ‡å— ã€‚è¿™æ ·å°±å¯ä»¥å…è´¹ç”¨ç™¾åº¦æä¾›çš„æŽ¥å£äº†ï¼Œç›®å‰è¿˜æŒºå‡†çš„ï¼Œè€Œä¸”æœ€å¤§å¯æŽ¥å—2048å­—èŠ‚ï¼Œç›®å‰ç¼ºç‚¹æ˜¯æœ‰QPS=5é™åˆ¶ï¼Œä½†ä¹Ÿæ¯”äººåˆ¤æ–­å¿«å¤šäº†ã€‚ 123456789101112from aip import AipNlp""" ä½ çš„ APPID AK SK """APP_ID = '15593833'API_KEY = 'PBW2w1dveS7x3YMecKSZW'SECRET_KEY = 'AOE75EWZqeI6kM7WMXKesq8i6FzQ'client = AipNlp(APP_ID, API_KEY, SECRET_KEY)result=client.sentimentClassify('æˆ‘çœŸçš„è§‰å¾—æ­¦æž—å¤–ä¼ è¶…çº§å¥½çœ‹ï¼æ¯ä¸ªè§’è‰²éƒ½è¶…çº§å–œæ¬¢ï¼ä¸€å¹´æœ€å°‘çœ‹ä¸‰éï¼è€ç™½è¯´ä¸Šä¸€å¥æˆ‘å°±èƒ½æŽ¥ä¸‹ä¸€å¥ï¼')print(result)# &#123;'log_id': 4474874689828720427, 'text': 'æˆ‘çœŸçš„è§‰å¾—æ­¦æž—å¤–ä¼ è¶…çº§å¥½çœ‹ï¼æ¯ä¸ªè§’è‰²éƒ½è¶…çº§å–œæ¬¢ï¼ä¸€å¹´æœ€å°‘çœ‹ä¸‰éï¼è€ç™½è¯´ä¸Šä¸€å¥æˆ‘å°±èƒ½æŽ¥ä¸‹ä¸€å¥ï¼', 'items': [&#123;'negative_prob': 0.0638882, 'sentiment': 2, 'positive_prob': 0.936112, 'confidence': 0.858026&#125;]&#125; è¿™é‡Œçš„è¿”å›žç»“æžœæœ‰negative_probï¼Œè¡¨ç¤ºè¿™æ®µè¯ä¸ºè´Ÿé¢æƒ…ç»ªçš„æ¦‚çŽ‡ã€‚sentiment=2è¡¨ç¤ºåˆ¤æ–­ç»“æžœä¸ºæ­£é¢æƒ…ç»ªï¼Œä¸º0è¡¨ç¤ºä¸ºè´Ÿé¢ï¼Œä¸º1è¡¨ç¤ºä¸ºä¸­ç«‹æƒ…ç»ªã€‚ å¦‚æžœæ˜¯æ‰¹é‡è¯·æ±‚ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/usr/bin/env python# -*- coding: utf-8 -*-from aip import AipNlpimport pandas as pdimport time""" ä½ çš„ APPID AK SK """APP_ID = '155934'API_KEY = 'PBW2w1dveS7x3YcKSZW0V7'SECRET_KEY = 'AOE75EWZqeI6kM7Kesq8i6FzQruDI'client = AipNlp(APP_ID, API_KEY, SECRET_KEY)# è¯·æ±‚æ–‡ä»¶source_file = "è¯·æ±‚æ–‡ä»¶è·¯å¾„"source_df = pd.read_excel(source_file)comments = []neg_probs = []pos_probs = []confidences = []sentiments = []complete_count = 0# è¯·æ±‚é”™è¯¯ç»Ÿè®¡err_count = 0err_comment = []start_time = time.time()# å¾ªçŽ¯è¯·æ±‚i = 0while i &lt; len(source_df): comment = source_df["comment_content"][i] try: query_result = client.sentimentClassify(comment[:1024]) except Exception as e: print("query_result:&#123;&#125;".format(query_result)) print("#######è¯·æ±‚è¿‡ç¨‹å­˜åœ¨é—®é¢˜#######") err_count += 1 err_comment.append(comment) i += 1 continue try: result = query_result['items'][0] neg_prob = result['negative_prob'] pos_prob = result['positive_prob'] confidence = result['confidence'] sentiment = result['sentiment'] except KeyError as e: print("#######è¯·æ±‚QPSé™åˆ¶#######") print("i=&#123;&#125;".format(i)) continue i += 1 comments.append(comment) neg_probs.append(neg_prob) pos_probs.append(pos_prob) confidences.append(confidence) sentiments.append(sentiment) complete_count += 1 print("æ€»å…±ï¼š&#123;&#125;æ¡".format(len(source_df))) print("è¯·æ±‚å®Œæˆ: &#123;&#125;æ¡".format(complete_count)) print("å®Œæˆè¿›åº¦ï¼š&#123;&#125;%".format(round(complete_count / len(source_df) * 100, 2))) cost_mins = (time.time() - start_time) / 60 print("ç´¯è®¡ç”¨æ—¶ï¼š&#123;&#125;åˆ†é’Ÿ".format(round(cost_mins, 2))) avg_query_time = complete_count / cost_mins # print("æ¯æ¡è¯·æ±‚å¹³å‡ç”¨æ—¶ï¼š&#123;&#125;".format(avg_query_time)) left_mins = (len(source_df) - complete_count - err_count) / avg_query_time print("é¢„è®¡è¿˜éœ€ï¼š&#123;&#125;åˆ†é’Ÿ".format(round(left_mins, 2))) print("\n")print("æ‰€æœ‰è¯·æ±‚å®Œæˆï¼")print("è¯·æ±‚æ€»æ•°é‡ï¼š&#123;&#125;".format(len(source_df)))print("è¯·æ±‚è¿‡ç¨‹ä¸­å­˜åœ¨é—®é¢˜çš„æ•°é‡ï¼š&#123;&#125;".format(err_count))# ä¿å­˜ç»“æžœ# è¯·æ±‚æˆåŠŸçš„ç»“æžœä¿å­˜desti_df = pd.DataFrame()desti_df["comment"] = commentsdesti_df["neg_probs"] = neg_probsdesti_df["pos_probs"] = pos_probsdesti_df["confidences"] = confidencesdesti_df["sentiments"] = sentimentsdesti_file = "è¯·æ±‚ç»“æžœä¿å­˜è·¯å¾„"desti_df.to_excel(desti_file, engine='xlsxwriter')# è¯·æ±‚å¤±è´¥çš„ç»“æžœä¿å­˜err_df = pd.DataFrame()err_file = "è¯·æ±‚ç»“æžœæŠ¥é”™ä¿å­˜è·¯å¾„"err_df["comment"] = err_commenterr_df.to_excel(err_file, engine='xlsxwriter') # å¦‚æžœè¯·æ±‚æŽ¥å£é‡Œæœ‰å¥‡æ€ªå­—ç¬¦ï¼Œä¿å­˜æ–‡ä»¶æ—¶å°±ä½¿ç”¨, engine='xlsxwriter' baseline modelå…³äºŽè¯„ä»·åˆ†æžæœ€ç»å…¸çš„å°±æ˜¯kaggleçš„ç”µå½±è¯„ä»·é¢˜ï¼ŒBag of Words Meets Bags of Popcorn ï¼Œé¢„æµ‹ä¸€æ®µè¯„è®ºæ˜¯positiveè¿˜æ˜¯negativeã€‚ç¨åŽä¼šä¸“é—¨ä¸ºè¿™ä¸ªæ¯”èµ›å†™ä¸€ç¯‡ã€‚ å¯æƒœæ•´ä¸ªæ•°æ®é›†éƒ½æ˜¯è‹±æ–‡çš„ã€‚ä¸‹é¢å‡ ä¸ªä¸ºä¸­æ–‡çš„æ•°æ®é›†ï¼Œéžå¸¸çè´µï¼Œä¾›å‚è€ƒï¼š nlp_chinese_corpusè‡ªç„¶è¯­è¨€å¤„ç†ä¸Žä¿¡æ¯æ£€ç´¢å…±äº«å¹³å°çˆ¬å–å•†å“è¯„è®ºå¹¶å¯¹å•†å“è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†ç±» å…¶ä»–å¼€æºé¡¹ç›®å‚è€ƒï¼š baidu çš„ Senta SentimentPolarityAnalysis vaderSentiment è¯„è®ºè§‚ç‚¹æŠ½å– Aspect-Basedå¾ˆå¤šæ—¶å€™ï¼Œè§‚ç‚¹ç»å¸¸æ˜¯å¤šè§’åº¦çš„ã€‚ä¸€æ®µè¯„ä»·å¯èƒ½åŒæ—¶æœ‰æ­£é¢æƒ…ç»ªå’Œè´Ÿé¢æƒ…ç»ªï¼ŒåŒæ—¶è¡¨è¾¾äº†å¤šä¸ªè§‚ç‚¹ï¼š é£Ÿç‰©å¾ˆç¾Žå‘³ï¼Œä½†æ˜¯æœåŠ¡å‘˜çš„æ€åº¦å¾ˆå·®åŠ²ï¼æ€»ä½“è¿˜å¯ä»¥å§ã€‚ è¿™æ¡è¯„ä»·é‡Œå¯¹é£Ÿç‰©çš„è§‚ç‚¹æ˜¯positiveçš„ï¼Œå¯¹æœåŠ¡çš„è§‚ç‚¹æ˜¯negativeçš„ï¼ŒæåŠäº†ä¸¤ä¸ªç»´åº¦ã€‚æ­¤æ—¶å¯¹æ•´ä¸ªè¯„ä»·åšç®€å•çš„åˆ†ç±»æ˜¯ä¸å¤Ÿçš„ã€‚ Aspect / target of attitudeï¼ˆ = è§‚ç‚¹çš„è§’åº¦ ï¼‰ single dimension å•ä¸ªè§’åº¦ many dimensions å¤šä¸ªè§’åº¦ ï¼ˆfood, price, service,â€¦â€¦æ ¹æ®ä¸åŒçš„äº§å“ä¼šæœ‰ä¸åŒçš„è§’åº¦ï¼‰ ä¸‹é¢æ˜¯å…³äºŽè´­ç‰©è¯„ä»·çš„ Aspects-based sentiment analysisï¼š è¿™å¼ å›¾å†…æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼Œé¦–å…ˆè¿›è¡Œè§‚ç‚¹æŠ½å–ï¼Œè¿™é‡Œæœ‰å…³äºŽä»·æ ¼ã€æœåŠ¡ã€å£å‘³ç­‰è§‚ç‚¹ï¼›å…¶æ¬¡å†è¿›è¡Œè§‚ç‚¹çš„æ­£è´Ÿé¢é¢„æµ‹ï¼Œåœ¨æœ€å³ä¾§ä¸€æ çš„ç»“æžœå±•ç¤ºé‡Œã€‚ ç»™ä¸€ä¸ªç™¾åº¦çš„è§‚ç‚¹æŠ½å–ä½“éªŒæŽ¥å£ï¼šä¼ é€é—¨ é‡Œé¢å·²ç»æœ‰13ç±»è¡Œä¸šçš„å„ç§è§‚ç‚¹é¢„æ–™è®­ç»ƒå‡ºæ¥çš„æ¨¡åž‹ï¼Œæ¯”å¦‚æœ‰é…’åº—ã€ç¾Žé£Ÿã€æˆ¿äº§ã€æ±½è½¦ç­‰è¡Œä¸šã€‚å½“ä½ æŠŠè¯„ä»·è¾“å…¥è¿›åŽ»ä¹‹åŽï¼Œåˆ†æžç»“æžœä¼šæŠŠå„è§‚ç‚¹ä»¥åŠæƒ…ç»ªéƒ½æ€»ç»“å‡ºæ¥ã€‚ AI Challenger å…¨çƒAIæŒ‘æˆ˜èµ›2018çš„AI Challengeræœ‰ä¸€ä¸ªæ˜¯å…³äºŽè§‚ç‚¹æŠ½å–çš„ï¼šç»†ç²’åº¦ç”¨æˆ·è¯„è®ºæƒ…æ„Ÿåˆ†æžã€‚ å®˜ç½‘ï¼šhttps://challenger.ai/ å®˜æ–¹çš„baselineï¼šAI_Challenger_2018 å¼€æºçš„è§£å†³æ–¹æ¡ˆï¼šAI Challenger 2018 æ–‡æœ¬æŒ–æŽ˜ç±»ç«žèµ›ç›¸å…³è§£å†³æ–¹æ¡ˆåŠä»£ç æ±‡æ€» æ•°æ®é›†ä¸‹è½½ï¼šä¼ é€é—¨]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown-tutorial]]></title>
    <url>%2F2019%2F02%2F11%2Fmarkdown-tutorial%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘è¿™ä¸¤å¤©åˆæŠ˜è…¾äº†ä¸‹ä¸ªäººç½‘ç«™ï¼Œè¶ç€è¿˜æ²¡å¿˜è®°ï¼ŒæŠŠè¯¥è®°ä¸‹çš„è®°ä¸€ä¸‹ã€‚ æŠŠæ•´ä¸ªä¸»é¢˜æ¢æˆäº†Nextï¼Œç¨åŽåšè¡¥å…… Markdown tips å†’å·ï¼šåŽé¢é¡»æœ‰ç©ºæ ¼æ‰èƒ½å¡«å†™å†…å®¹ å¢žåŠ ç©ºæ ¼ï¼š ä¸€ä¸ªç©ºæ ¼è¾“å…¥â€œ\&nbsp;â€å³å¯ å¤šæ ‡ç­¾ï¼š [tag1, tag2, â€¦] æ¢è¡Œå¤šæŒ‰ä¿©ç©ºæ ¼ http://www.markdown.cn/#overview Front matter title date categories tags image æ˜¾ç¤ºé¦–é¡µï¼Œæ‘˜è¦é‡Œçš„å›¾ banner_img é¡¶ç½®çš„å›¾ç‰‡ mathjax è¦ç”¨å…¬å¼ comments æ˜¯å¦æœ‰è¯„è®º]]></content>
      <tags>
        <tag>tutorial</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®ºæ–‡ç¬”è®° - Convolutional Sequence to Sequence Learning]]></title>
    <url>%2F2018%2F11%2F01%2F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Convolutional-Sequence-to-Sequence-Learning%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘è¯»äº†ä¸¤ç¯‡2017å¹´å¾ˆå‡ºåçš„è®ºæ–‡ï¼Œåˆ†åˆ«æ˜¯Facebookçš„ã€ŠConvolutional Sequence to Sequence Learningã€‹å’ŒGoogleçš„ã€ŠAttention is All You Needã€‹ã€‚è¿™ä¸¤ç¯‡è®©æˆ‘æ”¹è§‚å¼€å§‹å˜å¾—å¯¹è¯»è®ºæ–‡æœ‰å¥½æ„Ÿï¼Œå¯æƒ³è€ŒçŸ¥ç²¾å½©ç¨‹åº¦ã€‚æœ¬è´¨ä¸Šéƒ½æ˜¯æŠ›å¼ƒäº†RNNåšSeq2Seqçš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚æœ¬ç¯‡å…ˆè¿›è¡Œã€ŠConvolutional Sequence to Sequence Learningã€‹çš„è®ºæ–‡èµ°è¯»ã€‚ é¢å‘è¯»è€…ï¼šæœ¬èº«å°±å¯¹è¿™ç¯‡è®ºæ–‡æ„Ÿå…´è¶£ï¼Œä½†è¿˜æ˜¯æœ‰ä¸ç†è§£çš„åœ°æ–¹ã€‚ è®ºæ–‡æ‘˜è¦ æŠ›å¼ƒRNNï¼Œåªç”¨CNNåšSeq2Seqï¼ˆæœºå™¨ç¿»è¯‘ï¼‰ä»»åŠ¡ã€‚ä½†æ˜¯å…¨ç¯‡å°‘ä¸äº†CNNä¸ŽRNNçš„å¯¹æ¯”æè¿°ã€‚ RNNæ˜¯é“¾å¼ç»“æž„ï¼ˆChain Structureï¼‰ï¼Œä¸èƒ½å¹¶è¡Œè®­ç»ƒï¼ŒCNN(Hierarchical Structure)å¯ä»¥ï¼Œå¹¶ä¸”å¤§å¤§é™ä½Žè®¡ç®—å¤æ‚åº¦ã€‚ è®ºæ–‡ç›®å½•å¦‚ä¸‹ï¼š 1234567891011121314151617181920211. Introduction2. Recurrent Sequence to Sequence Learning3. A Convolutional Architecture |- 3.1 Position Embedding |- 3.2 Convolutional Block Structure |- 3.3 Multi-step Attention |- 3.4 Normalization Strategy |- 3.5 Initialization4. Experimental Setup |- 4.1 Datasets |- 4.2 Model Parameters and Optimization |- 4.3 Evaluation5. Results |- 5.1 Recurrent vs. Convolutional Models |- 5.2 Ensemble Results |- 5.3 Generation Speed |- 5.4 Position Embedding |- 5.5 Multi-step Attention |- 5.6 Kernel size and Depth |- 5.7 Summarization6. Conclusion and Future Work ï¼ˆä¸»è¦è¯¦ç»†å±•å¼€ç¬¬ä¸‰èŠ‚çš„å†…å®¹ã€‚æœ¬ç¯‡ä»¥èµ°è¯»çš„å½¢å¼ï¼Œä¸€å¥è¯quoteå‡ºæ¥ï¼Œå†è§£é‡Šç†è§£ï¼Œå†™å®Œæ•´ç†åˆ ï¼ŒæŒ‘æˆ‘è§‰å¾—é‡è¦çš„å¥å­è®²ï¼Œè¯»è¿™ç¯‡æ–‡ç« æœ€å¥½æ˜¯å¯¹è¿™ç¯‡è®ºæ–‡æœ‰ä¸€å®šäº†è§£ï¼Œæˆ–è€…æ˜¯å·¦è¾¹æ˜¯è®ºæ–‡ï¼Œå³è¾¹æ˜¯è¿™ç¯‡æ–‡ç« ï¼Œä¸‹é¢å¼•ç”¨çš„å¥å­éƒ½æ˜¯è®ºæ–‡é‡Œçš„æ–‡å­—ï¼Œä¹‹å‰è¯»è¿‡å¾ˆå¤šè®ºæ–‡ï¼Œä¹Ÿæ˜¯åœ¨çº¸ä¸Šç”»ç”»ï¼Œè¿™ä¸ªæ˜¯ç¬¬ä¸€ç¯‡å†™è®ºæ–‡ç¬”è®°ï¼‰ 1. IntroductionCNN vs RNN é“¾å¼/å±‚çº§ ç»“æž„ï¼Œå¹¶è¡Œè¿ç®— ä¸Šä¸‹æ–‡ç›¸å…³ è¾“å…¥è¾“å‡ºå›ºå®šé•¿åº¦ è®¡ç®—å¤æ‚åº¦ RNNæ˜¯é“¾å¼ç»“æž„ï¼ŒCNNæ˜¯å±‚çº§ç»“æž„ Compared to recurrent layers, convolutions create representations for fixed size contexts, however, the effective context size of the network can easily be made larger by stacking several layers on top of each other. This allows to precisely control the maximum length of dependencies to be modeled. CNNå¤„ç†çš„inputå—é™ï¼Œéœ€è¦æ˜¯ç­‰é•¿çš„æ–‡æœ¬ï¼Œfixed sizeï¼Œä½†æ˜¯åªè¦å¾€ä¸Šå †å å±‚æ•°å°±èƒ½å¤„ç†æ›´é•¿çš„æ–‡æœ¬ã€‚ è¿™æ ·çš„ç»“æž„èƒ½æŽ§åˆ¶æœ€å¤§é•¿åº¦æ•°å€¼ã€‚ Convolutional networks do not depend on the computations of the previoustime step and therefore allow parallelization over every element in a sequence. This contrasts with RNNs which maintain a hidden state of the entire past that prevents parallel computation within a sequence. å¥å­ä¸­çš„æ¯ä¸ªå•è¯å¹¶è¡Œè¿ç®—ï¼Œä¸ä¾èµ–å‰ä¸ªè¯çš„è®¡ç®—ã€‚ä¸ŽRNNçš„éšè—çŠ¶æ€ç›¸åã€‚ Multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower layers while distant elements interact at higher layers. ä¸ºäº†èŽ·å–æ›´å¤šçš„ä¿¡æ¯ï¼Œä½¿ç”¨çš„æ˜¯å¤šå±‚çš„CNNç»“æž„ã€‚è¶Šä½Žå±‚çš„å¤„ç†çš„è¯åœ¨å¥å­ä¸­æ›´è¿‘ï¼Œè¶Šé«˜å±‚å¤„ç†çš„è¯è·æ›´å¤§ã€‚ Hierarchical structure provides a shorter path to capture long-range dependencies compared to the chain structure modeled by recurrent networks, e.g. we can obtain a feature representation capturing relationships within a window of n words by applying only O(n/k) convolutional operations for kernels of width k, compared to a linear number O(n) for recurrent neural networks. è¿™ç§å±‚çº§ç»“æž„ç¼©çŸ­äº†èŽ·å–é•¿è·ç¦»çš„ä¿¡æ¯æ­¥éª¤ã€‚ Inputs to a convolutional network are fed through a constant number of kernels and non-linearities, whereas recurrent networks apply up to n operations and non-linearities to the first word and only a single set of operations to the last word. CNNå¤„ç†æ•°æ®ï¼šå¸¸é‡ä¸ªkernelå’Œéžçº¿æ€§å¤„ç† RNNï¼šå˜é‡ä¸ªnæ­¥éª¤ï¼Œç¬¬ä¸€ä¸ªè¯éžçº¿æ€§ï¼Œæœ€åŽä¸€ä¸ªè¯åšå•ä¸€å¤„ç† In this paper we propose an architecture for sequence to sequence modeling that is entirely convolutional. Our model is equipped with gated linear units (Dauphin et al., 2016) and residual connections (He et al., 2015a).We also use attention in every decoder layer and demonstrate that each attention layer only adds a negligible amount of overhead. å®Œå…¨é å·ç§¯ é¡ºå¸¦GLU/residual connections/attention 2. Recurrent Sequence to Sequence Learningä»‹ç»RNNçš„è¿ç®—è¿‡ç¨‹ï¼Œå¾ˆç®€çŸ­çš„å¸¦è¿‡ï¼Œå› ä¸ºä¹‹å‰æœ‰ä¸“é—¨ä»‹ç» Input Sequence &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x = (x_1,â€¦,x_m)$ Encoder Embedding &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w = (w_!,â€¦,w_m)$ State Representation &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $z = (z_1,â€¦,z_m)$ ================================================= [Encoder] Conditional Input &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $c = (c_1,â€¦,c_i,â€¦)$ ================================================= [Decoder] Hidden State &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $h = (h_1,â€¦,h_n)$ Decoder Embedding &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$g = (g_1,â€¦,g_n)$ Output Sequence &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $y = (y_1,â€¦,y_n)$ å› ä¸ºæ˜¯Encoder-Decoderç»“æž„ï¼Œæ‰€ä»¥è¿ç®—éƒ½æ˜¯ä¸Šä¸‹å¯¹ç§°çš„ã€‚ wå’Œgåˆ†åˆ«ä¸ºinput sequenceå’Œoutput sequenceçš„ 3. A Convolutional Architectureæ­£æ–‡ä»Žè¿™ä¸ªæ‰åˆšåˆšå¼€å§‹ã€‚ å°½é‡ç”¨å›¾ç¤ºè¡¨ç¤º]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>NLP</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLPç¬”è®° - LSTM ç»“æž„è¯¦è§£]]></title>
    <url>%2F2018%2F11%2F01%2FNLP%E7%AC%94%E8%AE%B0-LSTM%2F</url>
    <content type="text"><![CDATA[LSTMï¼ˆLong Short Term Memory networksï¼‰ï¼Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œæ˜¯ç»å…¸RNNçš„å‡çº§ç‰ˆï¼ŒåŽŸåˆ™ä¸Šæ˜¯RNNçš„ä¸€ç§ã€‚ç”¨æ¥è§£å†³ long-term dependencyé—®é¢˜ï¼ŒçŽ°è¢«å¹¿æ³›ä½¿ç”¨ã€‚ Long-Term Dependenciesä¼ ç»ŸRNNçš„é“¾å¼ç»“æž„å¦‚ä¸‹ï¼š RNNæœ€å¤§ä¼˜ç‚¹å°±æ˜¯èƒ½åˆ©ç”¨ä¹‹å‰çš„ä¿¡æ¯æ¥é¢„æµ‹å½“å‰å†…å®¹ã€‚æ¯”å¦‚è¦å¯¹å¥å­ â€œäº‘æœµæ¼‚æµ®åœ¨å¤©ç©ºâ€ çš„æœ€åŽä¸€ä¸ªè¯â€œå¤©ç©ºâ€è¿›è¡Œé¢„æµ‹ã€‚å…¶å®žï¼Œå…‰é â€œäº‘æœµâ€è¿™ä¸ªè¯å°±èƒ½æ˜Žæ˜¾çš„çŸ¥é“é¢„æµ‹è¯æ˜¯â€œå¤©ç©ºâ€ã€‚å¦‚ä¸‹å›¾ï¼Œâ€œäº‘æœµâ€å’Œâ€œå¤©ç©ºâ€çš„ä½ç½®æ˜¯å¾ˆè¿‘çš„ã€‚è¿™æ ·çš„æ“ä½œåœ¨çŸ­æ–‡æœ¬é‡Œæ˜¯æ¯”è¾ƒæœ‰æ•ˆå®žç”¨çš„ã€‚ ä½†æ˜¯è¿˜æœ‰å¾ˆå¤šæ˜¯é•¿æ–‡æœ¬çš„é¢„æµ‹ã€‚æ¯”å¦‚ï¼Œâ€œæˆ‘ ä»Žå°åœ¨ä¸­å›½é•¿å¤§ï¼Œæˆ‘å¾ˆå–œæ¬¢é‚£é‡Œçš„ç¾Žé£Ÿâ€¦â€¦æˆ‘èƒ½è¯´å¾ˆæµåˆ©çš„ä¸­æ–‡ã€‚â€ å¯¹è¿™ä¸ªé•¿æ–‡æœ¬è¿›è¡Œæœ€åŽä¸€ä¸ªè¯â€œä¸­æ–‡â€çš„é¢„æµ‹ã€‚æ ¹æ®é™„è¿‘çš„æ–‡å­—â€œè¯´å¾ˆæµåˆ©çš„â€ï¼Œé¢„æµ‹è¯å¾ˆå¯èƒ½æ˜¯ä¸€ä¸ªè¯­ç§ï¼Œè‡³äºŽæ˜¯ä»€ä¹ˆè¯­ç§è¿˜éœ€è¦å›žå½’åˆ°æœ€åˆçš„æ–‡æœ¬æ‰èƒ½ç¡®å®šä¸‹æ¥ï¼Œå¦‚ä¸‹å›¾ã€‚ç„¶è€Œè¿™ä¸ªé¢„æµ‹è¯ä¸Žæœ€åˆçš„æ–‡æœ¬çš„è·ç¦»å¯èƒ½æ˜¯éžå¸¸å¤§çš„ã€‚ ç„¶è€Œï¼Œé¢„æµ‹è¯ä¸Žç›¸å…³æ–‡æœ¬çš„é—´è·è¶Šå¤§ï¼ŒRNNå°±è¶Šéš¾åŽ»èŽ·å–ç›¸å…³ä¿¡æ¯ã€‚è¿™å°±æ˜¯Long-Term Dependencies é—®é¢˜ã€‚ å› æ­¤å°±æœ‰äº†LSTMï¼Œæ¥è§£å†³è¿™ä¸ªLong-Term Dependenciesé—®é¢˜ã€‚ ä¸‹é¢æ˜¯ä¼ ç»Ÿçš„RNNçš„é“¾å¼å†…éƒ¨ç»“æž„ï¼Œå¾ˆç®€å•æ˜Žäº†ï¼Œæ¯”å¦‚æœ‰ä¸€ä¸ªtanhå±‚ã€‚ LSTMåœ¨å¤§ä½“ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªé“¾å¼ç»“æž„ï¼Œä½†åœ¨å†…éƒ¨ç¨åšæ”¹è¿›ã€‚ä¸Žä¸Šé¢ä¸åŒçš„æ˜¯æœ‰4å±‚ç½‘ç»œç»“æž„ã€‚ å…ˆä¸è¦è§‰å¾—è¿™ä¸ªå†…éƒ¨ç»“æž„å¤æ‚ï¼ŒæŽ¥ä¸‹æ¥ä¼šæ‹†è§£å¼€æ¥ä¸€æ­¥æ­¥è¯´æ˜Žã€‚ä¸‹å›¾æ˜¯ä¸€äº›æ ‡è®°ç¬¦å·ï¼š LSTM æ ¸å¿ƒç»“æž„LSTMç»“æž„ä¸»çº¿æ˜¯ä¸‹é¢çš„ C_{t-1}â†’C_{t} çš„è¿™æ¡æ°´å¹³çº¿ã€‚ä»¥ä¸‹ç§°ä¸ºCçº¿ã€‚è¿™æ¡çº¿å°±åƒä¸ªä¼ é€å¸¦è´¯ç©¿æ•´ä¸ªé“¾å¼ç»“æž„ã€‚è¿™æ ·ä½¿å¾—ä¿¡æ¯æµé€šä¸”ä¿å…¨ä¿¡æ¯å†…å®¹ã€‚ å¯¹äºŽè¿™æ¡Cçº¿ï¼ŒLSTMæœ¬èº«æ²¡æœ‰åˆ å‡æ–‡æœ¬ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä¿¡æ¯é‡çš„åˆ å‡æ˜¯ç”±å„ç§â€œé—¨â€ï¼ˆgatesï¼‰è°ƒæŽ§çš„ã€‚ é—¨æ˜¯å¯¹ä¿¡æ¯ç­›é€‰çš„ä¸€ç§æ“ä½œï¼Œæœ‰é€‰æ‹©æ€§çš„è®©ä¿¡æ¯æµè¿‡ã€‚â€œé—¨â€ç”±ä¸€ä¸ªsigmoidå±‚ï¼ˆa sigmoid neural net layerï¼‰å’Œä¸€ä¸ªç‚¹ä¹˜ç»„æˆï¼ˆpointwise multiplication operationï¼‰ã€‚ sigmoidå±‚ï¼šè¾“å‡ºä¸º0-1çš„æ•°ï¼Œç”¨æ¥æè¿°ç­›é€‰ä¿¡æ¯çš„ç¨‹åº¦ã€‚0æ„å‘³ç€ä¸è®©ä¿¡æ¯æµé€šï¼Œ1æ„å‘³ç€è®©æ‰€æœ‰ä¿¡æ¯æµé€šã€‚ ä¸€ä¸ªLSTMæœ‰3ä¸ªè¿™æ ·çš„é—¨ï¼Œæ¥ä¿æŠ¤å’ŒæŽ§åˆ¶è¿™æ¡Cçº¿ä¸Šçš„ä¿¡æ¯æµã€‚ LSTM æ‹†è§£GATE 1ç¬¬ä¸€ä¸ªé—¨æ˜¯ç”¨æ¥å†³å®šåœ¨Cçº¿ä¸Šä¸¢å¼ƒå“ªäº›ä¿¡æ¯ï¼Œç”±ä¸€ä¸ªå«â€œforget gate layerâ€çš„sigmoidå±‚æž„æˆï¼Œå–å†³äºŽh_{tâˆ’1}å’Œx_{t}ï¼Œå®ƒçš„è¾“å‡ºæ˜¯ä¸€ä¸ª0-1çš„æ•°å€¼ã€‚æ¯”å¦‚åœ¨æ–‡æœ¬ä¸­ï¼Œå‰ä¸€ä¸ªå¥å­çš„ä¸»è§’æ˜¯ä¸ªç”·æ€§ï¼ŒåŽä¸€ä¸ªå¥å­çš„ä¸»è§’æ˜¯ä¸ªå¥³æ€§ï¼Œé‚£ä¹ˆåœ¨å‰ä¸€ä¸ªå¥å­é‡ŒèŽ·å–çš„æ€§åˆ«ä¿¡æ¯åº”è¯¥èˆå¼ƒã€‚ GATE 2ç¬¬äºŒä¸ªé—¨æ˜¯ç”¨æ¥ç¡®å®šä¿ç•™å“ªäº›ä¿¡æ¯ã€‚è¿™é‡Œç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼Œç¬¬ä¸€ä¸ªæ˜¯ç”±ä¸€ä¸ªå«â€œinput gate layerâ€çš„sigmoidå±‚æ¥å†³å®šæ›´æ–°å“ªäº›æ•°å€¼ï¼Œç¬¬äºŒä¸ªæ˜¯ä¸€ä¸ªtanhå±‚æ¥åˆ›å»ºä¸€ä¸ªå€™é€‰Cå‘é‡ï¼Œè¿™ä¸ªå‘é‡å¯ä»¥è¢«åŠ åˆ°Cçº¿ä¸­ã€‚è¿™ä¸¤ä¸ªçš„ç»„åˆå°±å½¢æˆäº†Cçº¿ä¿¡æ¯çš„æ›´æ–°ã€‚å°±åƒæ–‡æœ¬ä¸»è§’çš„æ€§åˆ«ä¿¡æ¯æ›´æ–°ã€‚ GATE 3çŽ°åœ¨æ¥æ›´æ–°C_{t-1}â†’C_{t}ã€‚f_{t}Ã—C_{t-1}æ˜¯ä¸¢å¼ƒä¿¡æ¯æ“ä½œï¼Œi_{t}Ã—C'_{t}æ˜¯ä¿¡æ¯æ›´æ–°æ“ä½œã€‚è¿™æ ·å°±å®Œæˆäº†ä¸»è§’æ€§åˆ«ä¿¡æ¯çš„æ›´æ–°ã€‚ æœ€åŽæˆ‘ä»¬éœ€è¦æ ¹æ®Cçº¿å†³å®šè¾“å‡ºå†…å®¹ã€‚é¦–å…ˆæ˜¯ä¸€ä¸ªsigmoidå±‚è¿›è¡Œè¾“å‡ºå†…å®¹ç­›é€‰ã€‚å…¶æ¬¡ä½¿å…¶ç»è¿‡tanhå¤„ç†å†å¯¹sigmoidçš„è¾“å‡ºè¿›è¡Œç‚¹ä¹˜ï¼Œå°±è¾“å‡ºæˆ‘ä»¬å†³å®šè¾“å‡ºçš„å†…å®¹ã€‚æ¯”å¦‚æ›´æ–°çš„æ˜¯æ€§åˆ«ä¿¡æ¯æ˜¯ä¸€ä¸ªåè¯ï¼Œä½†é¢„æµ‹è¯è¦æ±‚å¯èƒ½æ˜¯ä¸€ä¸ªä»£è¯ï¼Œå°±éœ€è¦è¿›è¡Œå°†â€œç”·/å¥³â€ä¿¡æ¯è½¬æ¢æˆâ€œä»–/å¥¹â€çš„é¢„æµ‹ã€‚ LSTM å˜ç§ä»¥ä¸Šä»‹ç»çš„æ˜¯æœ€æ™®é€šçš„LSTMï¼Œè¿˜æœ‰å…¶å®ƒçš„LSTMçš„å˜ç§ï¼Œå¯¹å†…éƒ¨ç»“æž„åšè½»å¾®çš„æ”¹å˜ã€‚ peephole connections Gated Recurrent Unit Deep LSTM å†™åœ¨æœ€åŽåœ¨ä¸€ç³»åˆ—RNNå–å¾—çš„ä¼˜ç§€æˆæžœèƒŒåŽï¼Œæœ‰ç›¸å½“å¤šä½¿ç”¨çš„æ˜¯LSTMç»“æž„ã€‚RNNåœ¨è®­ç»ƒæ—¶ä¼šé‡åˆ°gradient explode/vanishingï¼Œå¯¼è‡´æ— æ³•è®­ç»ƒã€‚å®žé™…ä¸­ï¼Œä½¿ç”¨æ›´å¤šçš„æ˜¯æ”¹è‰¯åŽçš„LSTMï¼ŒGRUç­‰ã€‚å¸Œæœ›ä¸Šé¢çš„æ‹†è§£ä½¿å¾—LSTMçœ‹èµ·æ¥å¹³æ˜“è¿‘äººã€‚ LSTMæ˜¯ä¸€ä¸ªé‡å¤§çš„è¿›æ­¥ï¼Œç»§LSTMä¹‹åŽï¼Œè¿˜æœ‰ä¸€ä¸ªé‡å¤§çš„è¿›æ­¥ï¼Œå°±æ˜¯Attentionï¼Œæ³¨æ„åŠ›æœºåˆ¶ã€‚æŽ¥ä¸‹æ¥è¿˜ä¼šæ›´å…·ä½“åœ°ä»‹ç»Attentionï¼Œæ•¬è¯·æœŸå¾…ï¼ðŸ˜˜ å‚è€ƒ Understanding LSTM Networks]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLPç¬”è®° - RNN ç»“æž„è¯¦è§£]]></title>
    <url>%2F2018%2F10%2F31%2FNLP%E7%AC%94%E8%AE%B0-RNN%2F</url>
    <content type="text"><![CDATA[é¢å‘è¯»è€…ï¼šæ·±åº¦å­¦ä¹  / NLPå…¥é—¨é€‰æ‰‹ NLPé‡Œæœ€å¸¸ç”¨æœ€ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¨¡åž‹å°±æ˜¯å¾ªçŽ¯ç¥žç»ç½‘ç»œ RNNï¼ˆRecurrent Neural Networkï¼‰ã€‚è¿™ä¸ªæ¨¡åž‹çš„å‘½åå·²ç»è¯´æ˜Žäº†æ•°æ®å¤„ç†æ–¹æ³•ï¼Œæ˜¯æŒ‰é¡ºåºæŒ‰æ­¥éª¤è¯»å–çš„ã€‚ä¸Žäººç±»ç†è§£æ–‡å­—çš„é“ç†å·®ä¸å¤šï¼Œçœ‹ä¹¦éƒ½æ˜¯ä¸€ä¸ªå­—ä¸€ä¸ªå­—ï¼Œä¸€å¥è¯ä¸€å¥è¯åŽ»ç†è§£çš„ã€‚ æœ¬æ–‡å°†ä»‹ç»RNNåŠå…¶ä¸€äº›é‡è¦çš„RNNå˜ç§æ¨¡åž‹ï¼š RNN Encoder-Decoderï¼ˆ = Seq-to-Seq ï¼‰ Attention Mechanism RNN å¤šç»“æž„è¯¦è§£CNN vs RNN CNN éœ€è¦å›ºå®šé•¿åº¦çš„è¾“å…¥ã€è¾“å‡ºï¼ŒRNN çš„è¾“å…¥å’Œè¾“å‡ºå¯ä»¥æ˜¯ä¸å®šé•¿ä¸”ä¸ç­‰é•¿çš„ CNN åªæœ‰ one-to-one ä¸€ç§ç»“æž„ï¼Œè€Œ RNN æœ‰å¤šç§ç»“æž„ï¼Œå¦‚ä¸‹å›¾ï¼š 1. one-to-one æœ€åŸºæœ¬çš„å•å±‚ç½‘ç»œï¼Œè¾“å…¥æ˜¯xï¼Œç»è¿‡å˜æ¢Wx+bå’Œæ¿€æ´»å‡½æ•°få¾—åˆ°è¾“å‡ºyã€‚ 2. one-to-nè¾“å…¥ä¸æ˜¯åºåˆ—è€Œè¾“å‡ºä¸ºåºåˆ—çš„æƒ…å†µï¼Œåªåœ¨åºåˆ—å¼€å§‹è¿›è¡Œè¾“å…¥è®¡ç®—ï¼š å›¾ç¤ºä¸­è®°å·çš„å«ä¹‰æ˜¯ï¼š åœ†åœˆæˆ–æ–¹å—è¡¨ç¤ºçš„æ˜¯å‘é‡ã€‚ ä¸€ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹è¯¥å‘é‡åšä¸€æ¬¡å˜æ¢ã€‚å¦‚ä¸Šå›¾ä¸­h_{0}å’Œxåˆ†åˆ«æœ‰ä¸€ä¸ªç®­å¤´è¿žæŽ¥ï¼Œå°±è¡¨ç¤ºå¯¹h_{0}å’Œxå„åšäº†ä¸€æ¬¡å˜æ¢ã€‚ è¿˜æœ‰ä¸€ç§ç»“æž„æ˜¯æŠŠè¾“å…¥ä¿¡æ¯Xä½œä¸ºæ¯ä¸ªé˜¶æ®µçš„è¾“å…¥ï¼š ä¸‹å›¾çœç•¥äº†ä¸€äº›Xçš„åœ†åœˆï¼Œæ˜¯ä¸€ä¸ªç­‰ä»·è¡¨ç¤ºï¼š è¿™ç§ one-to-n çš„ç»“æž„å¯ä»¥å¤„ç†çš„é—®é¢˜æœ‰ï¼š ä»Žå›¾åƒç”Ÿæˆæ–‡å­—ï¼ˆimage captionï¼‰ï¼Œæ­¤æ—¶è¾“å…¥çš„Xå°±æ˜¯å›¾åƒçš„ç‰¹å¾ï¼Œè€Œè¾“å‡ºçš„yåºåˆ—å°±æ˜¯ä¸€æ®µå¥å­ï¼Œå°±åƒçœ‹å›¾è¯´è¯ç­‰ ä»Žç±»åˆ«ç”Ÿæˆè¯­éŸ³æˆ–éŸ³ä¹ç­‰ 3. n-to-næœ€ç»å…¸çš„RNNç»“æž„ï¼Œè¾“å…¥ã€è¾“å‡ºéƒ½æ˜¯ç­‰é•¿çš„åºåˆ—æ•°æ®ã€‚ å‡è®¾è¾“å…¥ä¸ºX=(x_{1}, x_{2}, x_{3}, x_{4})ï¼Œæ¯ä¸ªxæ˜¯ä¸€ä¸ªå•è¯çš„è¯å‘é‡ã€‚ ä¸ºäº†å»ºæ¨¡åºåˆ—é—®é¢˜ï¼ŒRNNå¼•å…¥äº†éšçŠ¶æ€hï¼ˆhidden stateï¼‰çš„æ¦‚å¿µï¼Œhå¯ä»¥å¯¹åºåˆ—å½¢çš„æ•°æ®æå–ç‰¹å¾ï¼ŒæŽ¥ç€å†è½¬æ¢ä¸ºè¾“å‡ºã€‚å…ˆä»Žh_{1}çš„è®¡ç®—å¼€å§‹çœ‹ï¼š h_{2}çš„è®¡ç®—å’Œh_{1}ç±»ä¼¼ã€‚è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®¡ç®—æ—¶ï¼Œæ¯ä¸€æ­¥ä½¿ç”¨çš„å‚æ•°Uã€Wã€béƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æ¯ä¸ªæ­¥éª¤çš„å‚æ•°éƒ½æ˜¯å…±äº«çš„ï¼Œè¿™æ˜¯RNNçš„é‡è¦ç‰¹ç‚¹ï¼Œä¸€å®šè¦ç‰¢è®°ã€‚ ä¾æ¬¡è®¡ç®—å‰©ä¸‹æ¥çš„ï¼ˆä½¿ç”¨ç›¸åŒçš„å‚æ•°Uã€Wã€bï¼‰ï¼š è¿™é‡Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œåªç”»å‡ºåºåˆ—é•¿åº¦ä¸º4çš„æƒ…å†µï¼Œå®žé™…ä¸Šï¼Œè¿™ä¸ªè®¡ç®—è¿‡ç¨‹å¯ä»¥æ— é™åœ°æŒç»­ä¸‹åŽ»ã€‚å¾—åˆ°è¾“å‡ºå€¼çš„æ–¹æ³•å°±æ˜¯ç›´æŽ¥é€šè¿‡hè¿›è¡Œè®¡ç®—ï¼š æ­£å¦‚ä¹‹å‰æ‰€è¯´ï¼Œä¸€ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹å¯¹åº”çš„å‘é‡åšä¸€æ¬¡ç±»ä¼¼äºŽf(Wx+b)çš„å˜æ¢ï¼Œè¿™é‡Œçš„è¿™ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹h_{1}è¿›è¡Œä¸€æ¬¡å˜æ¢ï¼Œå¾—åˆ°è¾“å‡ºy_{1}ã€‚ å‰©ä¸‹çš„è¾“å‡ºç±»ä¼¼è¿›è¡Œï¼ˆä½¿ç”¨å’Œy_{1}åŒæ ·çš„å‚æ•°Vå’Œcï¼‰ï¼š è¿™å°±æ˜¯æœ€ç»å…¸çš„RNNç»“æž„ï¼Œå®ƒçš„è¾“å…¥æ˜¯x_{1}, x_{2}, .....x_{n}ï¼Œè¾“å‡ºä¸ºy_{1}, y_{2}, ...y_{n}ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¾“å…¥å’Œè¾“å‡ºåºåˆ—å¿…é¡»è¦æ˜¯ç­‰é•¿çš„ã€‚ç”±äºŽè¿™ä¸ªé™åˆ¶çš„å­˜åœ¨ï¼Œç»å…¸RNNçš„é€‚ç”¨èŒƒå›´æ¯”è¾ƒå°ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›é—®é¢˜é€‚åˆç”¨ç»å…¸çš„RNNç»“æž„å»ºæ¨¡ï¼Œå¦‚ï¼š è®¡ç®—è§†é¢‘ä¸­æ¯ä¸€å¸§çš„åˆ†ç±»æ ‡ç­¾ã€‚å› ä¸ºè¦å¯¹æ¯ä¸€å¸§è¿›è¡Œè®¡ç®—ï¼Œå› æ­¤è¾“å…¥å’Œè¾“å‡ºåºåˆ—ç­‰é•¿ã€‚ è¾“å…¥ä¸ºå­—ç¬¦ï¼Œè¾“å‡ºä¸ºä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚çŽ‡ã€‚è¿™å°±æ˜¯è‘—åçš„Char RNNï¼ˆè¯¦ç»†ä»‹ç»è¯·å‚è€ƒï¼šThe Unreasonable Effectiveness of Recurrent Neural Networksï¼ŒChar RNNå¯ä»¥ç”¨æ¥ç”Ÿæˆæ–‡ç« ï¼Œè¯—æ­Œï¼Œç”šè‡³æ˜¯ä»£ç ï¼Œéžå¸¸æœ‰æ„æ€ï¼‰ã€‚ 4. n-to-oneè¦å¤„ç†çš„é—®é¢˜è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªå•ç‹¬çš„å€¼è€Œä¸æ˜¯åºåˆ—ï¼Œåº”è¯¥æ€Žæ ·å»ºæ¨¡å‘¢ï¼Ÿå®žé™…ä¸Šï¼Œæˆ‘ä»¬åªåœ¨æœ€åŽä¸€ä¸ªhä¸Šè¿›è¡Œè¾“å‡ºå˜æ¢å°±å¯ä»¥äº†ï¼š è¿™ç§ç»“æž„é€šå¸¸ç”¨æ¥å¤„ç†åºåˆ—åˆ†ç±»é—®é¢˜ã€‚å¦‚è¾“å…¥ä¸€æ®µæ–‡å­—åˆ¤åˆ«å®ƒæ‰€å±žçš„ç±»åˆ«ï¼Œè¾“å…¥ä¸€ä¸ªå¥å­åˆ¤æ–­å…¶æƒ…æ„Ÿå€¾å‘ï¼Œè¾“å…¥ä¸€æ®µè§†é¢‘å¹¶åˆ¤æ–­å®ƒçš„ç±»åˆ«ç­‰ç­‰ã€‚ Encoder-Decodern-to-mè¿˜æœ‰ä¸€ç§æ˜¯ n-to-mï¼Œè¾“å…¥ã€è¾“å‡ºä¸ºä¸ç­‰é•¿çš„åºåˆ—ã€‚ è¿™ç§ç»“æž„æ˜¯Encoder-Decoderï¼Œä¹Ÿå«Seq2Seqï¼Œæ˜¯RNNçš„ä¸€ä¸ªé‡è¦å˜ç§ã€‚åŽŸå§‹çš„n-to-nçš„RNNè¦æ±‚åºåˆ—ç­‰é•¿ï¼Œç„¶è€Œæˆ‘ä»¬é‡åˆ°çš„å¤§éƒ¨åˆ†é—®é¢˜åºåˆ—éƒ½æ˜¯ä¸ç­‰é•¿çš„ï¼Œå¦‚æœºå™¨ç¿»è¯‘ä¸­ï¼Œæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„å¥å­å¾€å¾€å¹¶æ²¡æœ‰ç›¸åŒçš„é•¿åº¦ã€‚ä¸ºæ­¤ï¼ŒEncoder-Decoderç»“æž„å…ˆå°†è¾“å…¥æ•°æ®ç¼–ç æˆä¸€ä¸ªä¸Šä¸‹æ–‡è¯­ä¹‰å‘é‡cï¼š è¯­ä¹‰å‘é‡cå¯ä»¥æœ‰å¤šç§è¡¨è¾¾æ–¹å¼ï¼Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯æŠŠEncoderçš„æœ€åŽä¸€ä¸ªéšçŠ¶æ€èµ‹å€¼ç»™cï¼Œè¿˜å¯ä»¥å¯¹æœ€åŽçš„éšçŠ¶æ€åšä¸€ä¸ªå˜æ¢å¾—åˆ°cï¼Œä¹Ÿå¯ä»¥å¯¹æ‰€æœ‰çš„éšçŠ¶æ€åšå˜æ¢ã€‚ æ‹¿åˆ°cä¹‹åŽï¼Œå°±ç”¨å¦ä¸€ä¸ªRNNç½‘ç»œå¯¹å…¶è¿›è¡Œè§£ç ï¼Œè¿™éƒ¨åˆ†RNNç½‘ç»œè¢«ç§°ä¸ºDecoderã€‚Decoderçš„RNNå¯ä»¥ä¸ŽEncoderçš„ä¸€æ ·ï¼Œä¹Ÿå¯ä»¥ä¸ä¸€æ ·ã€‚å…·ä½“åšæ³•å°±æ˜¯å°†cå½“åšä¹‹å‰çš„åˆå§‹çŠ¶æ€h_{0}è¾“å…¥åˆ°Decoderä¸­ï¼š è¿˜æœ‰ä¸€ç§åšæ³•æ˜¯å°†cå½“åšæ¯ä¸€æ­¥çš„è¾“å…¥ï¼š Encoder-Decoder åº”ç”¨ç”±äºŽè¿™ç§Encoder-Decoderç»“æž„ä¸é™åˆ¶è¾“å…¥å’Œè¾“å‡ºçš„åºåˆ—é•¿åº¦ï¼Œå› æ­¤åº”ç”¨çš„èŒƒå›´éžå¸¸å¹¿æ³›ï¼Œæ¯”å¦‚ï¼š æœºå™¨ç¿»è¯‘ï¼šEncoder-Decoderçš„æœ€ç»å…¸åº”ç”¨ï¼Œäº‹å®žä¸Šè¿™ç»“æž„å°±æ˜¯åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸæœ€å…ˆæå‡ºçš„ã€‚ æ–‡æœ¬æ‘˜è¦ï¼šè¾“å…¥æ˜¯ä¸€æ®µæ–‡æœ¬åºåˆ—ï¼Œè¾“å‡ºæ˜¯è¿™æ®µæ–‡æœ¬åºåˆ—çš„æ‘˜è¦åºåˆ—ã€‚ é˜…è¯»ç†è§£ï¼šå°†è¾“å…¥çš„æ–‡ç« å’Œé—®é¢˜åˆ†åˆ«ç¼–ç ï¼Œå†å¯¹å…¶è¿›è¡Œè§£ç å¾—åˆ°é—®é¢˜çš„ç­”æ¡ˆã€‚ è¯­éŸ³è¯†åˆ«ï¼šè¾“å…¥æ˜¯è¯­éŸ³ä¿¡å·åºåˆ—ï¼Œè¾“å‡ºæ˜¯æ–‡å­—åºåˆ—ã€‚ Encoder-Decoder æ¡†æž¶Encoder-Decoder ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„æ¨¡åž‹ï¼Œæ˜¯ä¸€ç§æ¡†æž¶ã€‚ Encoderï¼šå°† inputåºåˆ— â†’è½¬æˆâ†’ å›ºå®šé•¿åº¦çš„å‘é‡ Decoderï¼šå°† å›ºå®šé•¿åº¦çš„å‘é‡ â†’è½¬æˆâ†’ outputåºåˆ— Encoder ä¸Ž Decoder å¯ä»¥å½¼æ­¤ç‹¬ç«‹ä½¿ç”¨ï¼Œå®žé™…ä¸Šç»å¸¸ä¸€èµ·ä½¿ç”¨ å› ä¸ºæœ€æ—©å‡ºçŽ°çš„æœºå™¨ç¿»è¯‘é¢†åŸŸï¼Œæœ€æ—©å¹¿æ³›ä½¿ç”¨çš„è½¬ç æ¨¡åž‹æ˜¯RNNã€‚å…¶å®žæ¨¡åž‹å¯ä»¥æ˜¯ CNN /RNN /BiRNN /LSTM /GRU /â€¦ Encoder-Decoder ç¼ºç‚¹ æœ€å¤§çš„å±€é™æ€§ï¼šç¼–ç å’Œè§£ç ä¹‹é—´çš„å”¯ä¸€è”ç³»æ˜¯å›ºå®šé•¿åº¦çš„è¯­ä¹‰å‘é‡c ç¼–ç è¦æŠŠæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯åŽ‹ç¼©è¿›ä¸€ä¸ªå›ºå®šé•¿åº¦çš„è¯­ä¹‰å‘é‡c è¯­ä¹‰å‘é‡cæ— æ³•å®Œå…¨è¡¨è¾¾æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ å…ˆè¾“å…¥çš„å†…å®¹æºå¸¦çš„ä¿¡æ¯ï¼Œä¼šè¢«åŽè¾“å…¥çš„ä¿¡æ¯ç¨€é‡ŠæŽ‰ï¼Œæˆ–è€…è¢«è¦†ç›–æŽ‰ è¾“å…¥åºåˆ—è¶Šé•¿ï¼Œè¿™æ ·çš„çŽ°è±¡è¶Šä¸¥é‡ï¼Œè¿™æ ·ä½¿å¾—åœ¨Decoderè§£ç æ—¶ä¸€å¼€å§‹å°±æ²¡æœ‰èŽ·å¾—è¶³å¤Ÿçš„è¾“å…¥åºåˆ—ä¿¡æ¯ï¼Œè§£ç æ•ˆæžœä¼šæ‰“æŠ˜æ‰£ å› æ­¤ï¼Œä¸ºäº†å¼¥è¡¥åŸºç¡€çš„ Encoder-Decoder çš„å±€é™æ€§ï¼Œæå‡ºäº†attentionæœºåˆ¶ã€‚ Attention Mechanismæ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰æ˜¯å¯¹åŸºç¡€Encoder-Decoderçš„æ”¹è‰¯ã€‚Attentionæœºåˆ¶é€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´è¾“å…¥ä¸åŒçš„cæ¥è§£å†³é—®é¢˜ï¼Œä¸‹å›¾æ˜¯å¸¦æœ‰Attentionæœºåˆ¶çš„Decoderï¼š æ¯ä¸€ä¸ªcä¼šè‡ªåŠ¨åŽ»é€‰å–ä¸Žå½“å‰æ‰€è¦è¾“å‡ºçš„yæœ€åˆé€‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨a_{ij}è¡¡é‡Encoderä¸­ç¬¬jé˜¶æ®µçš„h_{j}å’Œè§£ç æ—¶ç¬¬ié˜¶æ®µçš„ç›¸å…³æ€§ï¼Œæœ€ç»ˆDecoderä¸­ç¬¬ié˜¶æ®µçš„è¾“å…¥çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ c_{i}å°±æ¥è‡ªäºŽæ‰€æœ‰h_{j}å¯¹ a_{ij} çš„åŠ æƒå’Œã€‚ ä»¥æœºå™¨ç¿»è¯‘ä¸ºä¾‹ï¼ˆå°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼‰ï¼š è¾“å…¥çš„åºåˆ—æ˜¯â€œæˆ‘çˆ±ä¸­å›½â€ï¼Œå› æ­¤ï¼ŒEncoderä¸­çš„h_{1}ã€h_{2}ã€h_{3}ã€h_{4}å°±å¯ä»¥åˆ†åˆ«çœ‹åšæ˜¯â€œæˆ‘â€ã€â€œçˆ±â€ã€â€œä¸­â€ã€â€œå›½â€æ‰€ä»£è¡¨çš„ä¿¡æ¯ã€‚åœ¨ç¿»è¯‘æˆè‹±è¯­æ—¶ï¼Œç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡c_{1}åº”è¯¥å’Œ â€œæˆ‘â€ è¿™ä¸ªå­—æœ€ç›¸å…³ï¼Œå› æ­¤å¯¹åº”çš„ a_{11} å°±æ¯”è¾ƒå¤§ï¼Œè€Œç›¸åº”çš„ a_{12}ã€a_{13}ã€a_{14} å°±æ¯”è¾ƒå°ã€‚c_{2}åº”è¯¥å’Œâ€œçˆ±â€æœ€ç›¸å…³ï¼Œå› æ­¤å¯¹åº”çš„ a_{22}å°±æ¯”è¾ƒå¤§ã€‚æœ€åŽçš„c_{3}å’Œh_{3}ã€h_{4}æœ€ç›¸å…³ï¼Œå› æ­¤a_{33}ã€a_{34}çš„å€¼å°±æ¯”è¾ƒå¤§ã€‚ è‡³æ­¤ï¼Œå…³äºŽAttentionæ¨¡åž‹ï¼Œåªå‰©æœ€åŽä¸€ä¸ªé—®é¢˜äº†ï¼Œé‚£å°±æ˜¯ï¼šè¿™äº›æƒé‡ a_{ij} æ˜¯æ€Žä¹ˆæ¥çš„ï¼Ÿ äº‹å®žä¸Šï¼Œa_{ij}åŒæ ·æ˜¯ä»Žæ¨¡åž‹ä¸­å­¦å‡ºçš„ï¼Œå®ƒå®žé™…å’ŒDecoderçš„ç¬¬i-1é˜¶æ®µçš„éšçŠ¶æ€ã€Encoderç¬¬jä¸ªé˜¶æ®µçš„éšçŠ¶æ€æœ‰å…³ã€‚ åŒæ ·è¿˜æ˜¯æ‹¿ä¸Šé¢çš„æœºå™¨ç¿»è¯‘ä¸¾ä¾‹ï¼Œa_{1j}çš„è®¡ç®—ï¼ˆæ­¤æ—¶ç®­å¤´å°±è¡¨ç¤ºå¯¹hâ€™å’Œ h_{j}åŒæ—¶åšå˜æ¢ï¼‰ï¼š a_{2j} çš„è®¡ç®—ï¼š a_{3j} çš„è®¡ç®—ï¼š ä»¥ä¸Šå°±æ˜¯å¸¦æœ‰Attentionçš„Encoder-Decoderæ¨¡åž‹è®¡ç®—çš„å…¨è¿‡ç¨‹ã€‚ Attention çš„ä¼˜ç‚¹ï¼š åœ¨æœºå™¨ç¿»è¯‘æ—¶ï¼Œè®©ç”Ÿè¯ä¸åªæ˜¯å…³æ³¨å…¨å±€çš„è¯­ä¹‰å‘é‡cï¼Œå¢žåŠ äº†â€œæ³¨æ„åŠ›èŒƒå›´â€ã€‚è¡¨ç¤ºæŽ¥ä¸‹æ¥è¾“å‡ºçš„è¯è¦é‡ç‚¹å…³æ³¨è¾“å…¥åºåˆ—ç§çš„å“ªäº›éƒ¨åˆ†ã€‚æ ¹æ®å…³æ³¨çš„åŒºåŸŸæ¥äº§ç”Ÿä¸‹ä¸€ä¸ªè¾“å‡ºã€‚ ä¸è¦æ±‚ç¼–ç å™¨å°†æ‰€æœ‰ä¿¡æ¯å…¨è¾“å…¥åœ¨ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ä¸­ã€‚ å°†è¾“å…¥ç¼–ç æˆä¸€ä¸ªå‘é‡çš„åºåˆ—ï¼Œè§£ç æ—¶ï¼Œæ¯ä¸€æ­¥é€‰æ‹©æ€§çš„ä»Žåºåˆ—ä¸­æŒ‘ä¸€ä¸ªå­é›†è¿›è¡Œå¤„ç†ã€‚ åœ¨æ¯ä¸€ä¸ªè¾“å‡ºæ—¶ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨è¾“å…¥æºå¸¦çš„ä¿¡æ¯ï¼Œæ¯ä¸ªè¯­ä¹‰å‘é‡c_{i}ä¸ä¸€æ ·ï¼Œæ³¨æ„åŠ›ç„¦ç‚¹ä¸ä¸€æ ·ã€‚ Attention çš„ç¼ºç‚¹ éœ€è¦ä¸ºæ¯ä¸ªè¾“å…¥è¾“å‡ºç»„åˆåˆ†åˆ«è®¡ç®—attentionã€‚50ä¸ªå•è¯çš„è¾“å‡ºè¾“å‡ºåºåˆ—éœ€è¦è®¡ç®—2500ä¸ªattentionã€‚ attentionåœ¨å†³å®šä¸“æ³¨äºŽæŸä¸ªæ–¹é¢ä¹‹å‰éœ€è¦éåŽ†ä¸€éè®°å¿†å†å†³å®šä¸‹ä¸€ä¸ªè¾“å‡ºæ˜¯ä»¥ä»€ä¹ˆã€‚ Attentionçš„å¦ä¸€ç§æ›¿ä»£æ–¹æ³•æ˜¯å¼ºåŒ–å­¦ä¹ ï¼Œæ¥é¢„æµ‹å…³æ³¨ç‚¹çš„å¤§æ¦‚ä½ç½®ã€‚ä½†å¼ºåŒ–å­¦ä¹ ä¸èƒ½ç”¨åå‘ä¼ æ’­ç®—æ³•ç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚ Attentionå¯è§†åŒ–Attentionçš„å¯è§†åŒ–æ˜¯ä¸€ä»¶éžå¸¸coolçš„äº‹ï¼ðŸ˜ä¸‹é¢æŽ¨èä¸€äº›ä¼˜ç§€çš„attentionç›¸å…³è®ºæ–‡ã€‚ paper: Attention Is All You Needä¸‹å›¾æ˜¯å¥å­çš„å•è¯ä¹‹é—´çš„attentionè”ç³»ï¼Œé¢œè‰²æ·±æµ…è¡¨ç¤ºå¤§å°ã€‚ paper: Show, Attend and Tellæ³¨æ„æ¯ä¸ªå¥å­çš„ä¸åŒå•è¯åœ¨å›¾ç‰‡ä¸­çš„attentionæ ‡æ³¨ï¼š paper: Convolutional Sequence to Sequence Learningæœºå™¨ç¿»è¯‘ä¸­attentionæ•°å€¼çš„å¯è§†åŒ–ï¼Œä¸€èˆ¬æ˜¯éšç€é¡ºåºå¼ºç›¸å…³çš„ã€‚ å…¶å®ƒpaperï¼š [Grammar as a Foreign Language] [Teaching Machines to Read and Comprehend] ä¸‹é¢è¿™å¼ Attentionçƒ­åŠ›å›¾è¡¨è¾¾å‡ºäº†ï¼Œå½“äººåœ¨é˜…è¯»çš„æ—¶å€™ï¼Œæ³¨æ„åŠ›å¤§å¤šæ•°æ”¾åœ¨äº†æ–‡å­—æ ‡é¢˜æˆ–è€…æ®µè½ç¬¬ä¸€å¥è¯ï¼Œè¿˜æ˜¯è›®ç¬¦åˆçŽ°å®žæƒ…å†µçš„ã€‚ Attention Mechanismæ˜¯å¾ˆæ—©çš„æ¦‚å¿µäº†ï¼Œä½†éšç€2017å¹´è°·æ­Œçš„ä¸€ç¯‡ã€ŠAttention Is All You Needã€‹è¢«å®Œå…¨å¼•çˆ†ã€‚éšåŽä¼šå†™è¿™ç¯‡è®ºæ–‡çš„è¯¦ç»†èµ°è¯»å’Œè¿‡ç¨‹æŽ¨å¯¼ðŸ˜Žï¼Œæ•¬è¯·æœŸå¾…~ å‚è€ƒ å®Œå…¨å›¾è§£RNNã€RNNå˜ä½“ã€Seq2Seqã€Attentionæœºåˆ¶RNNæ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸çš„åŽŸå› ]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLPå®žæˆ˜ - åŸºäºŽSimNetçš„Quoraé—®å¥è¯­ä¹‰åŒ¹é…]]></title>
    <url>%2F2018%2F09%2F09%2FNLP%E5%AE%9E%E6%88%98-%E5%9F%BA%E4%BA%8ESimNet%E7%9A%84Quora%E9%97%AE%E5%8F%A5%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[Quora Question Pairsæ˜¯kaggleé‡Œçš„é—®å¥è¯­ä¹‰åŒ¹é…æ¯”èµ›ã€‚è¿™åœºæ¯”èµ›å¯¹äºŽnlpé€‰æ‰‹åº”è¯¥ä¸é™Œç”Ÿäº†ï¼Œæ•°æ®é›†ä¹Ÿæ˜¯å¤§å®¶å…¥é—¨nlpå¿…å¤‡ã€‚æœ¬æ–‡åœ¨æ·±åº¦è¯­ä¹‰åŒ¹é…ä½¿ç”¨çš„æ˜¯ç™¾åº¦å¼€æºçš„è¯­ä¹‰åŒ¹é…æ¡†æž¶AnyQé‡Œçš„SimNetã€‚ çŽ¯å¢ƒè¯´æ˜Ž Linux python 2.7 TensorFlow 1.7.0 CPU Jupyter Notebook ä¸‹è½½AnyQé¦–å…ˆéœ€è¦æœ‰gitï¼Œå¦‚æžœæ²¡æœ‰å¯ä»¥ç‚¹è¿™é‡Œä¸‹è½½ã€‚åœ¨LinuxçŽ¯å¢ƒé€‰å®šè·¯å¾„ä¸‹æ•²å…¥git clone https://github.com/baidu/AnyQè¿›è¡ŒAnyQçš„ä¸‹è½½ã€‚ ä¸‹è½½å®Œï¼ŒæŸ¥çœ‹SimNetçš„è·¯å¾„æ˜¯AnyQ/tools/simnet/train/tf/ SimNet çš„ç»“æž„å¦‚ä¸‹ï¼š 123456789simnet |-tf |- date //ç¤ºä¾‹æ•°æ® |- examples //ç¤ºä¾‹é…ç½®æ–‡ä»¶ |- layers //ç½‘ç»œä¸­ä½¿ç”¨æ“ä½œå±‚çš„å®žçŽ° |- losses //æŸå¤±å‡½æ•°å®žçŽ° |- nets //ç½‘ç»œç»“æž„å®žçŽ° |- tools //æ•°æ®è½¬åŒ–åŠè¯„ä»·å·¥å…· |- util //å·¥å…·ç±» â¤å¦å¤–å·²ç»ä¸“é—¨å†™äº†ä¸€ç¯‡å…³äºŽSimNetçš„ä»£ç èµ°è¯»ï¼Œå¼ºçƒˆæŽ¨èæ‰“å¼€é‚£ç¯‡æ–‡ç« æ”¾åœ¨æ—è¾¹ä¸Žè¿™ç¯‡ä¸€èµ·çœ‹ï¼Œç‚¹è¿™é‡ŒæŸ¥çœ‹ã€‚ å¦å¤–ï¼Œåœ¨LinuxæŸ¥çœ‹/ä¿®æ”¹ä»£ç ï¼ŒæŽ¨èjupyter notebookã€‚ å…¶å®ƒè¯´æ˜Žï¼š ä¿å­˜æ¨¡åž‹æ–‡ä»¶çš„è·¯å¾„éœ€è¦è‡ªå·±æ‰‹åŠ¨æ·»åŠ ï¼Œåœ¨ç›®å½•ä¸Šæ–°å»ºmodelå’Œpointwiseæ–‡ä»¶å¤¹ï¼š 12345678910111213simnet |-tf |- date |- examples |- layers |- losses |- nets |- tools |- util # æ–°å»ºä¸‹é¢çš„æ–‡ä»¶å¤¹ |- model |- pointwise ä¸‹è½½æ•°æ®é›†è¯·ç‚¹å‡» Quora è¿›è¡Œæ•°æ®é›†ä¸‹è½½ã€‚æœ¬æ–‡åªç”¨åˆ°è®­ç»ƒæ•°æ®é›†ï¼Œæ‰€ä»¥ä¸‹è½½train.csvå³å¯ã€‚train.csvè¿˜ä¸èƒ½ç›´æŽ¥ä½œä¸ºSimNetçš„è¾“å…¥æ•°æ®ï¼Œéœ€è¦åšè¯åµŒå…¥ç­‰æ•°æ®é¢„å¤„ç†ã€‚ è¯åµŒå…¥å¤„ç†SimNetçš„è®­ç»ƒæ•°æ®æ˜¯æœ‰æ ¼å¼è¦æ±‚çš„ã€‚å…·ä½“è¯·æŸ¥çœ‹SimNetçš„README.mdã€‚è¿™ä¸€æ­¥æ•°æ®å¤„ç†ä¹Ÿå¯ä»¥åœ¨winçŽ¯å¢ƒä¸‹æ“ä½œã€‚ç”±äºŽQuoraè®­ç»ƒé›†æ˜¯ä¸¤ä¸ªé—®å¥åˆ—åŠ ä¸€ä¸ªlabelåˆ—ï¼Œé€‚åˆSimNetçš„pointwiseæ•°æ®æ ¼å¼ã€‚ pointwiseæ•°æ®æ ¼å¼ï¼šæ•°æ®åŒ…å«ä¸‰åˆ—ï¼Œä¾æ¬¡ä¸ºQuery1çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼ŒQuery2çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼ŒLabelï¼Œæ¯åˆ—é—´ä½¿ç”¨TABåˆ†å‰²ï¼Œä¾‹å¦‚ï¼› 1231 1 1 1 1 2 2 2 2 2 01 1 1 1 1 1 1 1 1 1 1... pointwiseéœ€è¦é—®å¥éƒ½ä»¥idçš„å½¢å¼ï¼Œæ‰€ä»¥word embeddingé€‰æ‹©è¯è¢‹æ³•(BOW)ã€‚ æ–°å»ºä¸€ä¸ªquora.pyæ–‡ä»¶æ¥åšè¯åµŒå…¥å¤„ç†ï¼Œå…ˆå¤„ç†äº†ç©º(null)é—®é¢˜ï¼Œå†é¢„è§ˆ10ä¸ªé—®é¢˜å¯¹ï¼ˆquestion pairsï¼‰ã€‚ è¾“å…¥ï¼š 123456789101112131415161718192021222324252627282930313233343536# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Wed Aug 29 16:50:59 2018@author: Yi&quot;&quot;&quot;import osos.chdir(&quot;C:/Users/Yi/Desktop/nlp/quora&quot;) # quora.pyçš„è·¯å¾„import pandas as pdimport numpy as npimport nltkfrom nltk.corpus import stopwordsfrom nltk.stem import SnowballStemmerimport refrom string import punctuationtrain = pd.read_csv(&quot;data/train.csv&quot;) # å…±404290ä¸ªé—®å¥å¯¹#test = pd.read_csv(&quot;data/test.csv&quot;)# Check for any null valuesprint(train.isnull().sum())#print(test.isnull().sum())# Add the string &apos;empty&apos; to empty stringstrain = train.fillna(&apos;empty&apos;)#test = test.fillna(&apos;empty&apos;)# Preview some of the pairs of questionsa = 0 for i in range(a,a+10): print(train.question1[i]) print(train.question2[i]) print() è¾“å‡ºï¼š 1234567891011121314151617181920212223242526272829What is the step by step guide to invest in share market in india?What is the step by step guide to invest in share market?What is the story of Kohinoor (Koh-i-Noor) Diamond?What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?How can I increase the speed of my internet connection while using a VPN?How can Internet speed be increased by hacking through DNS?Why am I mentally very lonely? How can I solve it?Find the remainder when [math]23^&#123;24&#125;[/math] is divided by 24,23?Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?Which fish would survive in salt water?Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?I&apos;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?Should I buy tiago?What keeps childern active and far from phone and video games?How can I be a good geologist?What should I do to be a great geologist?When do you use ã‚· instead of ã—?When do you use &quot;&amp;&quot; instead of &quot;and&quot;?Motorola (company): Can I hack my Charter Motorolla DCX3400?How do I hack Motorola DCX3400 for free internet? ç»§ç»­åšåŒä¹‰è¯æ›¿æ¢ã€åœç”¨è¯å¤„ç†ã€åˆ é™¤ä»‹è¯ç­‰æ•°æ®æ¸…æ´—ï¼Œè¾“å…¥ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111stop_words = [&apos;the&apos;,&apos;a&apos;,&apos;an&apos;,&apos;and&apos;,&apos;but&apos;,&apos;if&apos;,&apos;or&apos;,&apos;because&apos;,&apos;as&apos;,&apos;what&apos;,&apos;which&apos;,&apos;this&apos;,&apos;that&apos;,&apos;these&apos;,&apos;those&apos;,&apos;then&apos;, &apos;just&apos;,&apos;so&apos;,&apos;than&apos;,&apos;such&apos;,&apos;both&apos;,&apos;through&apos;,&apos;about&apos;,&apos;for&apos;,&apos;is&apos;,&apos;of&apos;,&apos;while&apos;,&apos;during&apos;,&apos;to&apos;,&apos;What&apos;,&apos;Which&apos;, &apos;Is&apos;,&apos;If&apos;,&apos;While&apos;,&apos;This&apos;]def text_to_wordlist(text, remove_stop_words=True, stem_words=False): # Clean the text, with the option to remove stop_words and to stem words. # Clean the text text = re.sub(r&quot;[^A-Za-z0-9]&quot;, &quot; &quot;, text) text = re.sub(r&quot;what&apos;s&quot;, &quot;&quot;, text) text = re.sub(r&quot;What&apos;s&quot;, &quot;&quot;, text) text = re.sub(r&quot;\&apos;s&quot;, &quot; &quot;, text) text = re.sub(r&quot;\&apos;ve&quot;, &quot; have &quot;, text) text = re.sub(r&quot;can&apos;t&quot;, &quot;cannot &quot;, text) text = re.sub(r&quot;n&apos;t&quot;, &quot; not &quot;, text) text = re.sub(r&quot;I&apos;m&quot;, &quot;I am&quot;, text) text = re.sub(r&quot; m &quot;, &quot; am &quot;, text) text = re.sub(r&quot;\&apos;re&quot;, &quot; are &quot;, text) text = re.sub(r&quot;\&apos;d&quot;, &quot; would &quot;, text) text = re.sub(r&quot;\&apos;ll&quot;, &quot; will &quot;, text) text = re.sub(r&quot;60k&quot;, &quot; 60000 &quot;, text) text = re.sub(r&quot; e g &quot;, &quot; eg &quot;, text) text = re.sub(r&quot; b g &quot;, &quot; bg &quot;, text) text = re.sub(r&quot;\0s&quot;, &quot;0&quot;, text) text = re.sub(r&quot; 9 11 &quot;, &quot;911&quot;, text) text = re.sub(r&quot;e-mail&quot;, &quot;email&quot;, text) text = re.sub(r&quot;\s&#123;2,&#125;&quot;, &quot; &quot;, text) text = re.sub(r&quot;quikly&quot;, &quot;quickly&quot;, text) text = re.sub(r&quot; usa &quot;, &quot; America &quot;, text) text = re.sub(r&quot; USA &quot;, &quot; America &quot;, text) text = re.sub(r&quot; u s &quot;, &quot; America &quot;, text) text = re.sub(r&quot; uk &quot;, &quot; England &quot;, text) text = re.sub(r&quot; UK &quot;, &quot; England &quot;, text) text = re.sub(r&quot;india&quot;, &quot;India&quot;, text) text = re.sub(r&quot;switzerland&quot;, &quot;Switzerland&quot;, text) text = re.sub(r&quot;china&quot;, &quot;China&quot;, text) text = re.sub(r&quot;chinese&quot;, &quot;Chinese&quot;, text) text = re.sub(r&quot;imrovement&quot;, &quot;improvement&quot;, text) text = re.sub(r&quot;intially&quot;, &quot;initially&quot;, text) text = re.sub(r&quot;quora&quot;, &quot;Quora&quot;, text) text = re.sub(r&quot; dms &quot;, &quot;direct messages &quot;, text) text = re.sub(r&quot;demonitization&quot;, &quot;demonetization&quot;, text) text = re.sub(r&quot;actived&quot;, &quot;active&quot;, text) text = re.sub(r&quot;kms&quot;, &quot; kilometers &quot;, text) text = re.sub(r&quot;KMs&quot;, &quot; kilometers &quot;, text) text = re.sub(r&quot; cs &quot;, &quot; computer science &quot;, text) text = re.sub(r&quot; upvotes &quot;, &quot; up votes &quot;, text) text = re.sub(r&quot; iPhone &quot;, &quot; phone &quot;, text) text = re.sub(r&quot;\0rs &quot;, &quot; rs &quot;, text) text = re.sub(r&quot;calender&quot;, &quot;calendar&quot;, text) text = re.sub(r&quot;ios&quot;, &quot;operating system&quot;, text) text = re.sub(r&quot;gps&quot;, &quot;GPS&quot;, text) text = re.sub(r&quot;gst&quot;, &quot;GST&quot;, text) text = re.sub(r&quot;programing&quot;, &quot;programming&quot;, text) text = re.sub(r&quot;bestfriend&quot;, &quot;best friend&quot;, text) text = re.sub(r&quot;dna&quot;, &quot;DNA&quot;, text) text = re.sub(r&quot;III&quot;, &quot;3&quot;, text) text = re.sub(r&quot;the US&quot;, &quot;America&quot;, text) text = re.sub(r&quot;Astrology&quot;, &quot;astrology&quot;, text) text = re.sub(r&quot;Method&quot;, &quot;method&quot;, text) text = re.sub(r&quot;Find&quot;, &quot;find&quot;, text) text = re.sub(r&quot;banglore&quot;, &quot;Banglore&quot;, text) text = re.sub(r&quot; J K &quot;, &quot; JK &quot;, text) # Remove punctuation from text text = &apos;&apos;.join([c for c in text if c not in punctuation]) # Optionally, remove stop words if remove_stop_words: text = text.split() text = [w for w in text if not w in stop_words] text = &quot; &quot;.join(text) # Optionally, shorten words to their stems if stem_words: text = text.split() stemmer = SnowballStemmer(&apos;english&apos;) stemmed_words = [stemmer.stem(word) for word in text] text = &quot; &quot;.join(stemmed_words) # Return a list of words return(text)def process_questions(question_list, questions, question_list_name, dataframe): &apos;&apos;&apos;transform questions and display progress&apos;&apos;&apos; for question in questions: question_list.append(text_to_wordlist(question)) if len(question_list) % 100000 == 0: progress = len(question_list)/len(dataframe) * 100 print(&quot;&#123;&#125; is &#123;&#125;% complete.&quot;.format(question_list_name, round(progress, 1))) train_question1 = []process_questions(train_question1, train.question1, &apos;train_question1&apos;, train)train_question2 = []process_questions(train_question2, train.question2, &apos;train_question2&apos;, train)#test_question1 = []#process_questions(test_question1, test.question1, &apos;test_question1&apos;, test)##test_question2 = []#process_questions(test_question2, test.question2, &apos;test_question2&apos;, test)# Preview some transformed pairs of questionsa = 0 for i in range(a,a+10): print(train_question1[i]) print(train_question2[i]) print() è¾“å‡ºå¤„ç†ä¹‹åŽçš„10ä¸ªé—®é¢˜å¯¹ï¼š 1234567891011121314151617181920212223242526272829step by step guide invest in share market in Indiastep by step guide invest in share marketstory Kohinoor Koh i Noor Diamondwould happen Indian government stole Kohinoor Koh i Noor diamond backHow can I increase speed my internet connection using VPNHow can Internet speed be increased by hacking DNSWhy am I mentally very lonely How can I solve itfind remainder when math 23 24 math divided by 24 23one dissolve in water quickly sugar salt methane carbon di oxidefish would survive in salt waterastrology I am Capricorn Sun Cap moon cap rising does say meI am triple Capricorn Sun Moon ascendant in Capricorn does say meShould I buy tiagokeeps childern active far from phone video gamesHow can I be good geologistshould I do be great geologistWhen do you use insteadWhen do you use insteadMotorola company Can I hack my Charter Motorolla DCX3400How do I hack Motorola DCX3400 free internet ç»§ç»­åˆ é™¤è‡ªå®šä¹‰åœç”¨è¯ï¼Œåˆ é™¤åªå‡ºçŽ°è¿‡ä¸€æ¬¡çš„è¯ï¼Œå°†é—®é¢˜1å’Œé—®é¢˜2åˆå¹¶æˆä¸€ä¸ªå¤§è¯­æ–™åº“ï¼Œè¾“å…¥ï¼š 123456789101112131415import itertoolsraw_corpus = list(itertools.chain.from_iterable([train_question1,train_question2]))#[train_question1,train_question2]stoplist = stop_wordstexts = [[word for word in document.lower().split() if word not in stoplist] for document in raw_corpus]from collections import defaultdictfrequency = defaultdict(int)for text in texts: for token in text: frequency[token] += 1 precessed_corpus = [[token for token in text if frequency[token] &gt; 1] for text in texts] å½¢æˆå­—å…¸ï¼Œè¾“å…¥ï¼š 12345from gensim import corporadictionary = corpora.Dictionary(precessed_corpus)print(dictionary)print(dictionary.token2id) è¾“å‡ºå­—å…¸ï¼š 1Dictionary(52069 unique tokens: [&apos;vicodin&apos;, &apos;mermaid&apos;, &apos;kgb&apos;, &apos;dusk&apos;, &apos;glonass&apos;]...) è¾“å…¥ä¸€ä¸ªæ–°æ–‡æœ¬ï¼Œè¯•ä¸€ä¸‹è¿™ä¸ªå­—å…¸ï¼Œè¾“å…¥ï¼š 12345new_doc = &quot;would happen Indian government stole Kohinoor Koh i Noor diamond back&quot;new_vec = dictionary.doc2bow(new_doc.lower().split())#dictionary.doc2idx(new_doc.lower().split())print(new_vec) #åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç»„ä¸­ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ è¡¨ç¤ºå­—å…¸ä¸­å•è¯çš„IDï¼Œç¬¬äºŒä¸ªè¡¨ç¤ºåœ¨è¿™ä¸ªå¥å­ä¸­è¿™ä¸ªå•è¯å‡ºçŽ°çš„æ¬¡æ•°ã€‚ è¾“å‡ºï¼š 1234567891011[(8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (107, 1), (186, 1), (226, 1), (416, 1), (828, 1), (4496, 1)] æ„Ÿè§‰è¿˜å¯ä»¥ï¼Œé‚£ä¹ˆå°†quoraçš„æ‰€æœ‰é—®é¢˜å¯¹çš„è¯­æ–™éƒ½ç”¨å­—å…¸é‡Œçš„idä»£æ›¿ï¼Œè¾“å…¥ï¼š 12345bow_corpus = [dictionary.doc2idx(text) for text in precessed_corpus]bow_corpus_plus_1 = [[i+1 for i in bow_corpu] for bow_corpu in bow_corpus]bow_corpus_str = [[str(i) for i in bow_corpu_plus] for bow_corpu_plus in bow_corpus_plus_1]bow_corpus_join = [&apos; &apos;.join(bow_corpus_) for bow_corpus_ in bow_corpus_str] ç”±äºŽè¯­æ–™åº“æ˜¯é—®é¢˜1å’Œé—®é¢˜2æŒ‰é¡ºåºç»„æˆï¼Œé‚£ä¹ˆç”¨idä»£æ›¿åŽçš„è¯­æ–™åº“å‰ä¸€åŠçš„æ˜¯è¯è¢‹å¤„ç†åŽé—®é¢˜1ï¼ŒåŽä¸€åŠæ˜¯è¯è¢‹å¤„ç†åŽé—®é¢˜2ï¼Œæœ€ç»ˆæ¢å¤åˆ°pointwiseæ ¼å¼çš„æ•°æ®ï¼Œè¾“å…¥ï¼š 123456789101112131415# ç”Ÿæˆæ–‡ä»¶pointwise_train = pd.DataFrame(bow_corpus_join[:404290], columns = [&apos;question1&apos;])pointwise_train[&apos;question2&apos;] = bow_corpus_join[404290:]pointwise_train[&apos;is_duplicate&apos;] = train[&apos;is_duplicate&apos;]# é˜²æ­¢ç©º(null)é—®é¢˜pointwise_train = pointwise_train[[len(i)&gt;0 for i in pointwise_train[&apos;question1&apos;]]]pointwise_train = pointwise_train[[len(i)&gt;0 for i in pointwise_train[&apos;question2&apos;]]]# æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†size = round(len(pointwise_train)*0.8) # æ¯”ä¾‹ä¸º8:2# tsvæ ¼å¼çš„æ•°æ®æ–‡ä»¶pointwise_train[:size].to_csv(&apos;data/train_0829.tsv&apos;,sep = &apos;\t&apos;, index=False, header=False)pointwise_train[size:].to_csv(&apos;data/test_0829.tsv&apos;,sep = &apos;\t&apos;, index=False, header=False) å¾—åˆ°tsvæ ¼å¼çš„train_0829.tsvï¼Œtest_0829.tsvï¼Œå¯ä»¥éšä¾¿å‘½åã€‚ æ•°æ®å‡†å¤‡åˆ‡æ¢åˆ°Linuxï¼Œå°†åµŒå…¥å®Œçš„ tsv æ•°æ®é›†æ”¾å…¥AnyQ/tools/simnet/train/tf/dataè·¯å¾„ä¸‹ï¼š 12345simnet |-tf |- date //ç¤ºä¾‹æ•°æ®ï¼Œtsvæ ¼å¼ï¼Œæ²¡æœ‰è¡¨å¤´ |- train_0829.tsv //è®­ç»ƒé›†æ•°æ® |- test_0829.tsv //æµ‹è¯•é›†æ•°æ® æŒ‰ç…§ä¸‹å›¾è·¯å¾„AnyQ/tools/simnet/train/tf/ï¼Œæ–°å»º run_convert_data.sh è„šæœ¬æ–‡ä»¶ï¼Œ å…¶å®žå°±æ˜¯æŠŠåŽŸæ¥çš„run_train.shé‡Œè½¬æ¢æ•°æ®çš„å‘½ä»¤æ‹¿å‡ºæ¥ã€‚å› ä¸ºä¹‹åŽéœ€è¦å¤šæ¨¡åž‹è·‘ä¸€æ ·çš„æ•°æ®ï¼Œæ•°æ®è½¬æ¢åšä¸€æ¬¡å°±å¤Ÿäº†ï¼Œå†…å®¹å¦‚ä¸‹ï¼š 123456789set -e # set -o errexitset -u # set -o nounsetset -o pipefail echo &quot;convert train data&quot;python ./tools/tf_record_writer.py pointwise ./data/å¾…è½¬æ¢çš„è®­ç»ƒæ•°æ®æ–‡ä»¶åtrain_0829.tsv ./data/å·²è½¬æ¢çš„è®­ç»ƒæ•°æ®æ–‡ä»¶åconvert_train_0829 0 32echo &quot;convert test data&quot;python ./tools/tf_record_writer.py pointwise ./data/å¾…è½¬æ¢çš„æµ‹è¯•æ•°æ®æ–‡ä»¶åtest_0829.tsv ./data/å·²è½¬æ¢çš„æµ‹è¯•æ•°æ®æ–‡ä»¶åconvert_test_0829 0 32echo &quot;convert data finish&quot; åœ¨Linuxé»‘å‘½ä»¤æ¡†é‡Œæ•²å…¥å‘½ä»¤./run_convert_data.shï¼Œå¦‚æžœæœ‰permission deniedæƒ…å†µï¼Œå…ˆä½¿ç”¨chmod 777 æ–‡ä»¶åï¼Œåœ¨è¿™é‡Œæ˜¯chmod 777 run_convert_data.shã€‚å¦‚æžœæˆåŠŸå°†æ‰“å°å‡ºï¼š 123convert train dataconvert test dataconvert data finish åœ¨dataæ–‡ä»¶å¤¹ç›®å½•ä¸‹ï¼Œæ–°å¢žä¸¤ä¸ªè½¬æ¢åŽçš„æ•°æ®æ–‡ä»¶ï¼Œconvert_train_0829 å’Œ convert_test_0829ã€‚ ä¿®æ”¹ä»£ç ä¿®æ”¹é…ç½®æ–‡ä»¶ç”¨ jupyter notebook æ‰“å¼€ examples æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰å½¢å¦‚ xxx-pointwise.jsonçš„é…ç½®æ–‡ä»¶ï¼Œä¿®æ”¹ä»¥ä¸‹å‡ ä¸ªå‚æ•°æ•°å€¼ï¼š data_size = 323273 , å› ä¸ºtrain_0829.tsvæœ‰ 323273 æ¡æ ·æœ¬ vocabulary_size = 1000000 batch_size = 800 num_epochs = 1 print_iter = 10 train_file = data/convert_train_0829 test_file = data/convert_test_0829 ä»¥ä¸Šï¼Œåªæ˜¯ä¿®æ”¹æ¨¡åž‹è®­ç»ƒçš„é…ç½®å‚æ•°ï¼Œè¿˜å¾—å¦å¤–ä¿®æ”¹æ¨¡åž‹æ£€éªŒçš„é…ç½®å‚æ•°ï¼Œæ‰“å¼€AnyQ/tools/simnet/train/tf/ç›®å½•ä¸‹çš„ tf_simnet.pyï¼Œæ‰¾åˆ°def predict(conf_dict)ï¼Œæ‰¾åˆ°å¦‚ä¸‹ä»£ç ï¼ˆåº”è¯¥åœ¨ç¬¬90è¡Œï¼‰ï¼š 12conf_dict.update(&#123;&quot;num_epochs&quot;: &quot;1&quot;, &quot;batch_size&quot;: &quot;1&quot;, &quot;shuffle&quot;: &quot;0&quot;, &quot;train_file&quot;: conf_dict[&quot;test_file&quot;]&#125;) å°†å…¶ä¿®æ”¹æˆï¼š 12conf_dict.update(&#123;&quot;num_epochs&quot;: &quot;1&quot;, &quot;batch_size&quot;: &quot;400&quot;, &quot;shuffle&quot;: &quot;0&quot;, &quot;train_file&quot;: conf_dict[&quot;test_file&quot;]&#125;) ä¿å­˜ï¼Œå…³é—­æ–‡ä»¶ã€‚ ä¿®æ”¹ä¿å­˜æ¨¡åž‹æ–‡ä»¶è§„åˆ™defaultçš„ä»£ç å°†åœ¨æ¯ä¸ªepochè¿­ä»£æ—¶ä¿å­˜ä¸€ä¸ªæ¨¡åž‹ï¼Œä¸”æœ€ç»ˆè·‘å®Œè¿˜ä¼šä¿å­˜ä¸€ä¸ªæ¨¡åž‹ã€‚ç”±äºŽæ¨¡åž‹æ–‡ä»¶è¿‡å¤§ï¼Œæ‰€ä»¥å°†æŠŠä»£ç ä¿®æ”¹æˆåªä¿å­˜æœ€åŽè·‘å®Œçš„æ¨¡åž‹ï¼Œå¦‚æžœä¸éœ€è¦å¯ä¸åšæ­¤ä¿®æ”¹ã€‚æ‰“å¼€utilsæ–‡ä»¶å¤¹ä¸‹çš„controler.pyï¼Œå°†100-104è¡ŒéšåŽ»ï¼š 12345# if step % epoch_iter == 0:# print(&quot;save model epoch%d&quot; % (epoch_num))# save_path = saver.save(sess, # &quot;%s/%s.epoch%d&quot; % (model_path, model_file, epoch_num))# epoch_num += 1 ä¿®æ”¹æ‰“å°å‘½ä»¤defaultçš„ä»£ç åœ¨æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€ä¸ªprint_iterä¼šæ‰“å°å‡ºä¸€ä¸ªlosså€¼ï¼Œåœ¨æ¨¡åž‹æ£€éªŒè¿‡ç¨‹ä¸­ï¼Œæœ€åŽä¼šæ‰“å°å‡ºä¸€ä¸ªaccuracyå€¼ã€‚ä½†æ˜¯ä¸ºäº†è§‚å¯Ÿè·‘è¿­ä»£çš„é€Ÿåº¦å’Œç²¾åº¦ï¼Œè¿˜éœ€è¦åœ¨æ¯æ¬¡æŠ¥lossçš„æ—¶å€™ï¼Œæ‰“å°å‡ºæ¯ä¸ªprint_iterèŠ±äº†å‡ ç§’é’Ÿï¼ˆå¦‚ä¸éœ€è¦æ­¤åŠŸèƒ½å¯ä¸åšä¿®æ”¹ï¼‰ã€‚æ‰“å¼€utilsæ–‡ä»¶å¤¹ä¸‹çš„controler.pyï¼Œå°†90-99è¡Œä»£ç ä¿®æ”¹æˆå¦‚ä¸‹ï¼š 12345678910111213epoch_num = 1last_timestamp = datetime.datetime.now() # å¢žåŠ çš„ä»£ç while not coord.should_stop(): try: step += 1 c, _= sess.run([loss, optimizer]) avg_cost += c if step % print_iter == 0: now_timestamp = datetime.datetime.now() # å¢žåŠ çš„ä»£ç  print(&quot;step: %d, loss: %4.4f (%4.2f sec/print_iter)&quot; % (step,(avg_cost / print_iter),(now_timestamp-last_timestamp).seconds)) # ä¿®æ”¹çš„ä»£ç  avg_cost = 0.0 last_timestamp = now_timestamp # å¢žåŠ çš„ä»£ç  ä¿å­˜ï¼Œå…³é—­æ–‡ä»¶ã€‚ æ¯”å¯¹æ¨¡åž‹æ•ˆæžœåœ¨AnyQ/tools/simnet/train/tf/è·¯å¾„å¢žåŠ  .sh æ–‡ä»¶ã€‚å› ä¸ºSimNetç›®å‰æœ‰7ä¸ªå¯é€‰æ‹©çš„ç½‘ç»œï¼Œåˆ†åˆ«æ˜¯bow, cnn, knrm, lstm, mmdnn, mvlstm, pyramidï¼Œåˆ†åˆ«ä¸Žnetsæ–‡ä»¶å¤¹é‡Œçš„æ–‡ä»¶ä¸€ä¸€å¯¹åº”ï¼Œæ‰€ä»¥æ¯ç§ä»»åŠ¡éƒ½æœ‰7ä¸ª .sh è„šæœ¬ã€‚ä»»åŠ¡ç±»åž‹åˆ†åˆ«æ˜¯ train/predict/freezeï¼Œå¯¹åº”æ¨¡åž‹è®­ç»ƒï¼Œæ¨¡åž‹æ£€éªŒï¼Œæ¨¡åž‹ç»“æžœç¤ºæ„ã€‚ å¢žåŠ æ¨¡åž‹è®­ç»ƒä»»åŠ¡çš„ .shæ–‡ä»¶ä»¥cnnä¸ºä¾‹ï¼Œæ–°å»ºrun_train_cnn.shï¼Œå†…å®¹å¦‚ä¸‹ï¼š 123456789set -e # set -o errexitset -u # set -o nounsetset -o pipefail in_task_type=&apos;train&apos;in_task_conf=&apos;./examples/cnn-pointwise.json&apos;python tf_simnet.py \ --task $in_task_type \ --task_conf $in_task_conf å¢žåŠ æ¨¡åž‹éªŒè¯ä»»åŠ¡çš„ .shæ–‡ä»¶ä»¥cnnä¸ºä¾‹ï¼Œæ–°å»ºrun_predict_cnn.shï¼Œå†…å®¹å¦‚ä¸‹ï¼š 123456789set -e # set -o errexitset -u # set -o nounsetset -o pipefail in_task_type=&apos;predict&apos;in_task_conf=&apos;./examples/cnn-pointwise.json&apos;python tf_simnet.py \ --task $in_task_type \ --task_conf $in_task_conf å¢žåŠ æ¨¡åž‹ç»“æžœç¤ºæ„ä»»åŠ¡çš„ .shæ–‡ä»¶ä»¥cnnä¸ºä¾‹ï¼Œæ–°å»ºrun_freeze_cnn.shï¼Œå†…å®¹å¦‚ä¸‹ï¼š 123456789set -e # set -o errexitset -u # set -o nounsetset -o pipefail in_task_type=&apos;freeze&apos;in_task_conf=&apos;./examples/cnn-pointwise.json&apos;python tf_simnet.py \ --task $in_task_type \ --task_conf $in_task_conf æœ€ç»ˆï¼Œç”Ÿæˆ7ä¸ª run_train_xxx.sh æ–‡ä»¶ï¼Œ7ä¸ª run_predict_xxx.shæ–‡ä»¶ï¼Œ7ä¸ª run_freeze_xxx.shæ–‡ä»¶ï¼Œæˆ–è€…é€‰æ‹©æ€§ç”Ÿæˆå‡ ä¸ªï¼Œç¤ºæ„å¦‚ä¸‹å›¾ï¼š åˆ‡æ¢å›žLinuxå‘½ä»¤ç•Œé¢ï¼Œå¼€å§‹è¿è¡Œå„ç§å‘½ä»¤ï¼Œç»“æžœå¦‚é…å›¾ã€‚ bow ./run_train_bow.sh ./run_predict_bow.sh cnn ./run_train_cnn.sh ./run_predict_cnn.sh knrm ./run_train_knrm.sh ./run_predict_knrm.sh lstm ./run_train_lstm.sh ./run_predict_lstm.sh mmdnn ./run_train_mmdnn.sh ./run_predict_mmdnn.sh mvlstm ./run_train_mvlstm.sh ./run_predict_mvlstm.sh pyramid ./run_train_pyramid.sh ./run_predict_pyramid.sh å¯è‡ªè¡Œå°è¯•./run_freeze_xxx.shå‘½ä»¤ç³»åˆ—ã€‚å½“è·‘å®Œä¸Šé¢æ‰€æœ‰å‘½ä»¤æ—¶ï¼ŒåŽŸæ–‡ä»¶å¤¹ä¸­å°±è‡ªåŠ¨å½¢æˆäº†é¢„æµ‹æ–‡ä»¶ï¼Œå¦‚ä¸‹ï¼š éšä¾¿æ‰“å¼€å…¶ä¸­ä¸€ä¸ªçœ‹ä¸€ä¸‹ï¼š å¦‚ä¸‹æ˜¯è‡ªåŠ¨ä¿å­˜çš„æ¨¡åž‹æ–‡ä»¶ï¼š ä¹Ÿä¼šè‡ªåŠ¨ç”Ÿæˆlogæ–‡ä»¶å¤¹ï¼Œè¿è¡Œfreezeä»»åŠ¡åŽä¼šè‡ªåŠ¨ç”Ÿæˆgraphæ–‡ä»¶å¤¹ï¼ŒæŠŠä»»åŠ¡ç»“æžœä¿å­˜åœ¨æ–‡ä»¶å¤¹é‡Œã€‚ ä¼˜ç¼ºç‚¹ç¼ºç‚¹ï¼š ç”±äºŽSimNetçš„æ•°æ®æ ¼å¼æœ‰è¦æ±‚ï¼Œå°†æ–‡æœ¬éƒ½ä»¥IDæ ¼å¼ä»£æ›¿ï¼Œå› æ­¤è¯åµŒå…¥ä½¿ç”¨çš„æ˜¯è¯è¢‹bowå¤„ç†ã€‚ åœ¨å‰æœŸæ–‡æœ¬æ¸…æ´—çš„è§„åˆ™å¯ä»¥å†å®Œå–„äº›ã€‚ å‘ƒã€‚ã€‚ã€‚ã€‚ç™¾åº¦ï¼ˆæ‘Šæ‰‹ðŸ¤·â€â™€ï¸ ä¼˜ç‚¹ï¼š SimNetæ˜¯ä¸€ä¸ªé›†æˆä½“ï¼Œæœ‰å¾ˆå¤šæ·±åº¦æ¨¡åž‹å¯ä»¥é€‰æ‹©ã€‚ å†™åœ¨æœ€åŽ è·‘äº†ä¸ªSimNetæµç¨‹ï¼Œä»…ä½œæ•ˆæžœæ¯”å¯¹ï¼Œæ²¡æœ‰è¿½æ±‚ç²¾åº¦çš„æå‡ã€‚åŽæ¥æˆ‘æœ‰å°è¯•æŠŠbatch_sizeè°ƒæˆ30ï¼Œé‚£ä¹ˆå°†æœ‰10000å¤šæ­¥ï¼Œç²¾åº¦æœ‰å‡ ä¸ªç™¾åˆ†ç‚¹çš„æå‡ï¼Œä½†è¿˜è¿œè¿œä¸å¤Ÿã€‚æ‰€ä»¥ä»»é‡é“è¿œå‘€ï¼Œè¿˜æœ‰å¾ˆå¤šéœ€è¦å­¦ä¹ çš„ã€‚ åœ¨æœ¬æ–‡æ²¡æœ‰ç”¨åˆ°æµ‹è¯•é›†ï¼Œå¦‚æžœè¦å‚åŠ æ¯”èµ›ï¼Œåœ¨è¯åµŒå…¥åšå­—å…¸æ—¶ï¼Œæ˜¯å¦åº”è¯¥æŠŠtestæµ‹è¯•é›†çš„å•è¯ä¹ŸåŠ å…¥åˆ°å­—å…¸é‡Œæ¥ï¼Ÿè¿™ä¸ªè¿˜æ²¡æœ‰çœŸçš„å°è¯•ä¸€ä¸‹ï¼Œæ¯•ç«Ÿæµ‹è¯•é›†test.cvså¤§çš„å“äººï¼ å¦‚æœ‰ç–‘é—®ï¼Œæ¬¢è¿Žç•™è¨€æˆ–è€…ç‚¹è¿™é‡Œæ‰¾åˆ°æˆ‘ã€‚]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>kaggle</tag>
        <tag>quora</tag>
        <tag>Semantic Matching</tag>
        <tag>NLP</tag>
        <tag>AnyQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç èµ°è¯» - ç™¾åº¦å¼€æºæ™ºèƒ½é—®ç­”æ¡†æž¶ AnyQ]]></title>
    <url>%2F2018%2F09%2F06%2F%E4%BB%A3%E7%A0%81%E8%B5%B0%E8%AF%BB-%E7%99%BE%E5%BA%A6%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6-AnyQ%2F</url>
    <content type="text"><![CDATA[é¢å‘å¯¹è±¡ï¼šæƒ³æ­å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€æ·±åº¦è¯­ä¹‰åŒ¹é…çš„nlpé€‰æ‰‹ã€‚åœ¨è‡ªå·±äº²æ‰‹æ­å»ºä¸€ä¸ªä¹‹å‰ï¼Œå­¦ä¹ å’Œèµ°è¯»ä¼˜ç§€çš„æ¡†æž¶ä»£ç æ˜¯ä¸ªä¸ä¼šé”™çš„é€‰æ‹©ã€‚ AnyQ(ANswer Your Questions) ï¼šç™¾åº¦QAå¼€æºé¡¹ç›®ï¼Œä¸»è¦åŒ…å«é¢å‘FAQé›†åˆçš„é—®ç­”ç³»ç»Ÿæ¡†æž¶ã€æ–‡æœ¬è¯­ä¹‰åŒ¹é…å·¥å…·SimNetã€‚ æ¡†æž¶ç›®å½•æœ¬æ–‡é‡ç‚¹èµ°è¯»SimNetæ¡†æž¶çš„ä»£ç ã€‚å¼€æºä»£ç åœ°å€ï¼Œç‚¹è¿™é‡Œã€‚TensorFlowç‰ˆSimNetçš„ç»“æž„å¦‚ä¸‹ï¼šï¼ˆè‡ªåŠ¨å±è”½åå­—å¸¦ â€˜pairwiseâ€™ çš„æ–‡ä»¶ï¼Œç¨åŽè§£é‡Šï¼‰ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647simnet |-tf |- date //ç¤ºä¾‹æ•°æ®ï¼Œtsvæ ¼å¼ï¼Œæ²¡æœ‰è¡¨å¤´ |- train_pointwise_data //è®­ç»ƒé›†æ•°æ® |- test_pointwise_data //æµ‹è¯•é›†æ•°æ® |- examples //ç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼Œä»¥æ¨¡åž‹ç§ç±»å‘½åï¼Œé‡Œé¢çš„å‚æ•°éœ€ç†ŸçŸ¥ |- bow-pointwise.json |- cnn-pointwise.json |- knrm-pointwise.json |- lstm-pointwise.json |- mmdnn-pointwise.json |- mvlstm-pointwise.json |- pyramid-pointwise.json |- layers //ç½‘ç»œä¸­ä½¿ç”¨æ“ä½œå±‚çš„å®žçŽ° |- tf_layers.py |- losses //æŸå¤±å‡½æ•°å®žçŽ°ï¼Œå¯æ”¾ç½®å„ç§æŸå¤±å‡½æ•°class |- simnet_loss.py |- nets //ç½‘ç»œç»“æž„å®žçŽ°ï¼Œç”±tf_layers.pyä¸åŒç»„åˆå®žçŽ° |- bow.py |- knrm.py |- lstm.py |- matchpyramid.py |- mlpcnn.py |- mm_dnn.py |- mvlstm.py |- tools //æ•°æ®è½¬åŒ–åŠè¯„ä»·å·¥å…· |- evaluate.py |- tf_record_reader.py |- tf_record_writer.py |- util //å·¥å…·ç±» |- controler.py |- converter.py # æ•°æ®è½¬æ¢ |- datafeeds.py # è¯»å–æ•°æ® |- utility.py |- README.md //è¯·ä»”ç»†åå¤è¯»ï¼Œ |- run_infer.sh //è¿è¡Œpredictä»»åŠ¡ |- run_train.sh //è¿è¡Œtrainä»»åŠ¡ |- tf_simnet.py //ä¸»è¿è¡Œæ–‡ä»¶ è¯»æ¡†æž¶ä»£ç çš„å·¥å…·ï¼Œç›¸è¾ƒäºŽjupyterï¼Œspyderï¼ŒæŽ¨èPycharmã€‚ è¿è¡ŒçŽ¯å¢ƒ linuxï¼Œå…¶å®ƒç³»ç»ŸæŽ¨èdocker python 2.7 tensorflow 1.7.0 æ•°æ®ç±»åž‹è§£é‡Šä¸ºä½•å±è”½åå­—å¸¦ â€˜pairwiseâ€™ çš„æ–‡ä»¶ï¼š è¯­ä¹‰åŒ¹é…ç½‘ç»œSimNetå¯ä»¥ä½¿ç”¨Pointwiseä¸ŽPairwiseä¸¤ç§ç±»åž‹çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ Pointwiseè®­ç»ƒåŠæµ‹è¯•æ•°æ®æ ¼å¼ è®­ç»ƒæ•°æ®æ ¼å¼ï¼šè®­ç»ƒæ•°æ®åŒ…å«ä¸‰åˆ—ï¼Œä¾æ¬¡ä¸ºQuery1çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼ŒQuery2çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼ŒLabelï¼Œæ¯åˆ—é—´ä½¿ç”¨TABåˆ†å‰²ï¼Œä¾‹å¦‚ï¼› 1231 1 1 1 1 2 2 2 2 2 01 1 1 1 1 1 1 1 1 1 1... æµ‹è¯•æ•°æ®æ ¼å¼ï¼šPointwiseæµ‹è¯•æ•°æ®æ ¼å¼ä¸Žè®­ç»ƒæ•°æ®æ ¼å¼ç›¸åŒã€‚ Pairwiseè®­ç»ƒåŠæµ‹è¯•æ•°æ®æ ¼å¼ è®­ç»ƒæ•°æ®æ ¼å¼ï¼šè®­ç»ƒæ•°æ®åŒ…å«ä¸‰åˆ—ï¼Œä¾æ¬¡ä¸ºQuery1çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼ŒPositive Query2çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼ŒNegative Query3çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼Œæ¯åˆ—é—´ä½¿ç”¨TABåˆ†å‰²ï¼Œä¾‹å¦‚ï¼› 1231 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3... æµ‹è¯•æ•°æ®æ ¼å¼ï¼šæµ‹è¯•æ•°æ®æ ¼å¼åŒ…å«ä¸‰åˆ—ï¼Œä¾æ¬¡ä¸ºQuery1çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼ŒQuery2çš„IDåºåˆ—ï¼ˆIDé—´ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼‰ï¼ŒLabelï¼Œæ¯åˆ—é—´ä½¿ç”¨TABåˆ†å‰²ï¼Œä¾‹å¦‚ï¼› 12341 1 1 1 1 1 1 1 1 1 11 1 1 1 1 2 2 2 2 2 03 3 3 3 3 3 3 3 3 3 1... ç”±äºŽä½¿ç”¨çš„æ•°æ®é›†æ˜¯Quoraæ•°æ®é›†ï¼Œä¸º [é—®å¥1ï¼Œé—®å¥2ï¼Œlabel] çš„Pointwiseæ ¼å¼æ•°æ®é›†ï¼Œå› æ­¤åå­—å¸¦ â€˜pairwiseâ€™ çš„æ–‡ä»¶æš‚æ—¶éƒ½ç”¨ä¸ä¸Šã€‚è‹¥æ˜¯æ•°æ®é›†ä¸ºpairwiseï¼Œå°±èƒ½ç”¨çš„ä¸Šäº†ã€‚ .json é…ç½®æ–‡ä»¶èµ°è¯»å‡†å¤‡å®Œæ•°æ®æ–‡ä»¶ä¹‹åŽï¼Œè§‚å¯Ÿé…ç½®æ–‡ä»¶ï¼Œä»¥cnn-pointwise.jsonä¸ºä¾‹ã€‚ é€šè¿‡é…ç½®æ–‡ä»¶å¯ä»¥çµæ´»çš„é€‰æ‹©ç½‘ç»œç±»åž‹ï¼Œæ•°æ®ç±»åž‹ï¼ŒæŸå¤±å‡½æ•°ä»¥åŠå…¶ä»–è¶…å‚æ•°ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; &quot;train_data&quot;:&#123; &quot;train_file&quot;: &quot;data/convert_train_pointwise_data&quot;, //è®­ç»ƒæ–‡ä»¶è·¯å¾„ &quot;data_size&quot;: 400, //è®­ç»ƒé›†å¤§å°ï¼Œæ ¹æ®ä¸åŒçš„è®­ç»ƒæ•°æ®æ–‡ä»¶éœ€åšæ”¹åŠ¨ &quot;left_slots&quot; : [[&quot;left&quot;,32]], //left slotçš„åå­—åŠæœ€å¤§é•¿åº¦ &quot;right_slots&quot; : [[&quot;right&quot;,32]] //right slotçš„åå­—åŠæœ€å¤§é•¿åº¦ &#125;, &quot;model&quot;:&#123; &quot;net_py&quot;: &quot;./nets/mlpcnn&quot;, //ç½‘ç»œå¯¹åº”æ¨¡å—è·¯å¾„ &quot;net_class&quot;: &quot;MLPCnn&quot;, //ç½‘ç»œå¯¹åº”ç±»å &quot;vocabulary_size&quot;: 3, //è¯å…¸å¤§å°ï¼Œæ ¹æ®è¯åµŒå…¥å¾—å‡ºçš„å­—å…¸å¤§å°éœ€åšæ”¹åŠ¨ # ä¸åŒçš„ç½‘ç»œnetæœ‰ä¸åŒçš„å‚æ•°ï¼Œè¿™äº›æ˜¯cnnçš„å‚æ•° &quot;embedding_dim&quot;: 128, // EmbeddingåµŒå…¥å±‚ç»´åº¦ &quot;num_filters&quot;: 256, //å·æœºæ ¸æ•°é‡ &quot;hidden_size&quot;: 128, //éšè—å±‚å¤§å° &quot;window_size&quot;: 3, //å·æœºæ ¸å¤§å° # æŸå¤±å‡½æ•°å‚æ•° &quot;loss_py&quot;: &quot;./losses/simnet_loss&quot;, //æŸå¤±å¯¹åº”æ¨¡å—è·¯å¾„ï¼Œå†…æœ‰å„ç§æŸå¤±å‡½æ•° &quot;loss_class&quot;: &quot;SoftmaxWithLoss&quot; //æŸå¤±å¯¹åº”ç±»åï¼Œå¯é€‰æ‹©å…¶å®ƒæŸå¤±å‡½æ•° &#125;, &quot;global&quot;:&#123; &quot;training_mode&quot;: &quot;pointwise&quot;, //è®­ç»ƒæ¨¡å¼ï¼Œä¹Ÿæ˜¯æ•°æ®æ ¼å¼ &quot;n_class&quot;: 2, //ç±»åˆ«æ•°ç›®ï¼Œ2ä¸ºäºŒåˆ†ç±» &quot;max_len_left&quot;: 32, //Left slotçš„æœ€å¤§é•¿åº¦ &quot;max_len_right&quot;: 32 //Right slotçš„æœ€å¤§é•¿åº¦ &#125;, &quot;setting&quot;:&#123; &quot;batch_size&quot;: 64, // Batch Sizeï¼Œæ¯ä¸€æ­¥è·‘çš„æ ·æœ¬æ•° &quot;num_epochs&quot;: 1, // Number of Epochsï¼Œé‡å¤å…¨ä½“æ ·æœ¬çš„å€æ•° &quot;thread_num&quot;: 6, //çº¿ç¨‹æ•° &quot;print_iter&quot;: 100, //æ˜¾ç¤ºé—´éš”ï¼Œæ¯100æ­¥æ˜¾ç¤ºä¸€ä¸ªloss &quot;model_path&quot;: &quot;model/pointwise&quot;, //æ¨¡åž‹ä¿å­˜è·¯å¾„ï¼Œåœ¨æ¡†æž¶ç›®å½•é‡Œæ²¡æœ‰ï¼Œéœ€è¦è‡ªå·±æ–°å»º &quot;model_prefix&quot;: &quot;cnn&quot;, //æ¨¡åž‹ä¿å­˜åå‰ç¼€ &quot;learning_rate&quot;: 0.001, //å­¦ä¹ çŽ‡ &quot;shuffle&quot;: 1 //æ˜¯å¦æ‰“ä¹±æ•°æ® &#125;, &quot;test_data&quot;:&#123; &quot;test_file&quot;: &quot;data/convert_test_pointwise_data&quot;, //æµ‹è¯•æ•°æ®è·¯å¾„ &quot;test_model_file&quot;: &quot;model/pointwise/cnn.epoch1&quot;, //æµ‹è¯•ä½¿ç”¨æ¨¡åž‹ï¼Œè¦å…ˆè·‘trainä»»åŠ¡åŽæ‰æœ‰æ¨¡åž‹æ–‡ä»¶ä¿å­˜ä¸‹æ¥ï¼Œæ‰èƒ½åšé¢„æµ‹ &quot;test_result&quot;: &quot;result_cnn_pointwise&quot; //æµ‹è¯•ç»“æžœæ–‡ä»¶ &#125;, &quot;graph&quot;:&#123; &quot;graph_path&quot;: &quot;graph&quot;, //freezeä»»åŠ¡æ–‡ä»¶ä¿å­˜è·¯å¾„ &quot;graph_name&quot;: &quot;model_cnn_pairwise.protxt&quot; //freezeä»»åŠ¡ç»“æžœæ–‡ä»¶ &#125;&#125; å‚æ•°è¯´æ˜Žï¼š å‡è®¾è®­ç»ƒé›†çš„ data_size = 1000æ—¶ï¼Œè¿è¡Œæ¨¡åž‹è®­ç»ƒtrainä»»åŠ¡æ—¶ï¼Œè®¾ç½®äº†num_epochs = 5ï¼Œbatch_size = 50ï¼Œshuffle = 1ï¼Œprint_iter = 10ï¼Œé‚£ä¹ˆè®­ç»ƒé›†æœ€ç»ˆçš„æ ·æœ¬é‡ = data_size num_epochs =5000ï¼Œé‡å¤äº†5å€åŽŸæœ‰æ ·æœ¬é‡ï¼Œshuffle = 1è¡¨ç¤ºæ‰“ä¹±æ ·æœ¬æ•°æ®ï¼Œè€Œæ¯æ­¥stepè·‘ batch_size æ¡æ ·æœ¬ï¼Œä¸€å…±èƒ½è·‘ data_size num_epochs / batch_size = 5000/50 = 100 æ­¥ï¼Œè€Œæ¯print_iteræ­¥æŠ¥ä¸€æ¬¡lossï¼Œé‚£ä¹ˆä¸€å…±æŠ¥ 100/print_iter = 10æ¬¡lossã€‚ åœ¨ä»£ç ä¸­è¿˜æœ‰ä¸ªå‚æ•°ä¸º epoch_iterï¼Œä¸ºä¿å­˜æ¨¡åž‹è€Œè®¾ç½®ï¼Œæ„ä¸ºæ¯epoch_iteræ­¥æ—¶ï¼Œä¿å­˜ä¸€æ¬¡æ¨¡åž‹æ–‡ä»¶ã€‚epoch_iter = data_size / batch_size = 1000/50 = 20ï¼Œå…±100æ­¥ï¼Œå³è·‘å®ŒtrainåŽå…±ä¿å­˜ 100/epoch_iter =100/20 =5ä¸ªæ¨¡åž‹æ–‡ä»¶ã€‚è‹¥shuffle = 0ï¼Œepoch_iter æ„ä¸ºè·‘å®Œä¸€æ¬¡åŽŸdata_size éœ€è¦çš„æ­¥æ•°ï¼Œè‹¥shuffle = 1ï¼Œepoch_iter æ„ä¸ºè·‘å®Œä¸€æ¬¡ä¸ŽåŽŸdata_sizeä¸€æ ·å¤§çš„æ•°æ®é›†éœ€è¦çš„æ­¥æ•°ï¼Œè¿™æ ·çš„è¯ï¼Œæœ€ç»ˆä¿å­˜çš„æ¨¡åž‹æ•°é‡ = num_epochs ã€‚ å‡è®¾æµ‹è¯•é›†çš„ data_size = 200æ—¶ï¼Œè¿è¡Œæ¨¡åž‹é¢„æµ‹predictä»»åŠ¡æ—¶ï¼Œç»å¸¸è®¾ç½®num_epochs = 1ï¼Œå› ä¸ºåœ¨æµ‹è¯•æ—¶æ²¡å¿…è¦é‡å¤æµ‹è¯•æ ·æœ¬ï¼Œæ˜¯å¦æ‰“ä¹±æ•°æ®å½±å“ä¹Ÿä¸å¤§ï¼Œå½“è®¾ç½®batch_size = 50ï¼Œæµ‹è¯•æ•°æ®æ€»æ ·æœ¬é‡ = data_size *num_epochs =200ä»ç„¶æ˜¯åŽŸæµ‹è¯•æ•°æ®é›†ï¼Œæ¯æ­¥è·‘batch_size = 50æ¡æ•°æ®ï¼Œå…±200/50=4stepsè·‘å®Œï¼Œæœ€ç»ˆä¼šæ‰“å°ä¸€ä¸ªaccuracyæ•°å€¼ã€‚ è¿™äº›å‚æ•°åªå¯¹æ¨¡åž‹è®­ç»ƒæœ‰æ•ˆï¼Œæ¨¡åž‹é¢„æµ‹çš„predictä»»åŠ¡é‡Œçš„è¿™äº›å‚æ•°éœ€è¦åœ¨tf_simnet.pyå†…çš„predictå‡½æ•°å†…ä¿®æ”¹ï¼ï¼ï¼ï¼ˆä¸ºè‡ªå·±ä¿®æ”¹ä»£ç æŠŠé…ç½®å‚æ•°æåˆ°é…ç½®æ–‡ä»¶é‡ŒåŸ‹ä¸‹ä¼ç¬”ðŸ˜‚ï¼‰ å…¶å®ƒè¯´æ˜Žï¼š ä¿å­˜æ¨¡åž‹æ–‡ä»¶çš„è·¯å¾„éœ€è¦è‡ªå·±æ‰‹åŠ¨æ·»åŠ ï¼Œåœ¨ç›®å½•ä¸Šæ–°å»ºmodelå’Œpointwiseæ–‡ä»¶å¤¹ï¼š 12345678910111213simnet |-tf |- date |- examples |- layers |- losses |- nets |- tools |- util # æ–°å»ºä¸‹é¢çš„æ–‡ä»¶å¤¹ |- model |- pointwise .sh ä»»åŠ¡æ–‡ä»¶èµ°è¯»æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œé…ç½®æ–‡ä»¶ä¿®æ”¹å®ŒæˆåŽï¼Œå¯åœ¨Linuxæ‰§è¡Œ.shè„šæœ¬æ–‡ä»¶æ¥å®žçŽ° train / predict / freeze / â€¦ç­‰ä»»åŠ¡ã€‚ run_train.shé€šè¿‡æ‰§è¡Œè„šæœ¬run_train.shå¯ä»¥å¯åŠ¨è®­ç»ƒä»»åŠ¡ï¼Œæ‰“å¼€run_train.shï¼š 123456789101112131415161718192021222324set -e # set -o errexitset -u # set -o nounsetset -o pipefail # ä»¥ä¸‹å‘½ä»¤ç”¨æ¥åšè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®è½¬æ¢ï¼Œè½¬æ¢ä¸€æ¬¡å½¢æˆconvertæ–‡ä»¶ä¾¿å¯ä»¥ï¼Œä¸éœ€è¦é‡å¤è½¬æ¢#-----------------------------------------------------------------------# å°†train_pointwise_dataè½¬æˆconvert_train_pointwise_dataecho &quot;convert train data&quot;python ./tools/tf_record_writer.py pointwise ./data/train_pointwise_data ./data/convert_train_pointwise_data 0 32#-----------------------------------------------------------------------# å°†test_pointwise_dataè½¬æˆconvert_test_pointwise_dataecho &quot;convert test data&quot;python ./tools/tf_record_writer.py pointwise ./data/test_pointwise_data ./data/convert_test_pointwise_data 0 32echo &quot;convert data finish&quot;#-----------------------------------------------------------------------in_task_type=&apos;train&apos; # è¾“å…¥ä»»åŠ¡ç±»åž‹ï¼Œå¯é€‰æ‹© train/predict/freeze/convertin_task_conf=&apos;./examples/cnn-pointwise.json&apos; # è¾“å…¥é…ç½®æ–‡ä»¶åœ°å€#-----------------------------------------------------------------------# ä»¥ä¸‹å‘½ä»¤è¿è¡Œtf_simnet.pyæ–‡ä»¶ï¼Œæ‰§è¡Œtrainä»»åŠ¡ï¼Œæ·±åº¦è¯­ä¹‰åŒ¹é…ä½¿ç”¨ä¸ºcnnç½‘ç»œpython tf_simnet.py \ --task $in_task_type \ --task_conf $in_task_conf ä¹Ÿå¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼å¯åŠ¨è‡ªå®šä¹‰è®­ç»ƒï¼Œæ•ˆæžœä¸Žä¸Šé¢çš„.shæ–‡ä»¶æ˜¯ä¸€æ ·çš„ï¼š 123python tf_simnet.py --task train --task_conf examples/cnn_pointwise.json æ‰§è¡Œå®Œrun_train.shåŽï¼Œåœ¨modelæ–‡ä»¶å¤¹å†…ä¼šè‡ªåŠ¨ä¿å­˜å„ä¸ªepoch_iterå’Œfinalçš„æ¨¡åž‹æ–‡ä»¶ã€‚ run_infer.shé€šè¿‡æ‰§è¡Œè„šæœ¬run_infer.shå¯ä»¥å¯åŠ¨é¢„æµ‹ä»»åŠ¡ï¼Œå¯ä»¥å¾—åˆ°æ¨¡åž‹é¢„æµ‹ç»“æžœæˆ–å¾—åˆ†ï¼Œæ‰“å¼€.shæ–‡ä»¶ï¼š 123456789set -e # set -o errexitset -u # set -o nounsetset -o pipefail in_task_type=&apos;predict&apos; # é€‰æ‹©äº†predictä»»åŠ¡in_task_conf=&apos;./examples/cnn-pointwise.json&apos; # ä»ç„¶æ˜¯cnné…ç½®æ–‡ä»¶è·¯å¾„python tf_simnet.py \ --task $in_task_type \ --task_conf $in_task_conf ä¹Ÿå¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼å¯åŠ¨è‡ªå®šä¹‰è®­ç»ƒï¼š 123python tf_simnet.py --task predict --task_conf examples/cnn_pointwise.json æ‰§è¡Œå®Œrun_infer.shä¹‹åŽï¼Œä¼šè‡ªåŠ¨åœ¨è·¯å¾„ä¸Šç”Ÿæˆresultæ–‡ä»¶ï¼Œå¯æ‰“å¼€æŸ¥çœ‹ã€‚ è‡ªå®šä¹‰.shä»»åŠ¡æ–‡ä»¶æ®è§‚å¯Ÿï¼Œ.shæ–‡ä»¶ä¸»è¦è¿è¡Œtf_simnet.pyæ–‡ä»¶ï¼Œæœ‰ä¸¤ä¸ªå‚æ•°å¯ä»¥è‡ªå®šä¹‰ã€‚ å‚æ•°è¯´æ˜Žï¼š task: ä»»åŠ¡ç±»åž‹ ï¼Œå¯é€‰æ‹© train/predict/freeze/convert ã€‚ task_conf: ä½¿ç”¨é…ç½®æ–‡ä»¶åœ°å€ æŽ¥ä¸‹æ¥å°è¯•ç”Ÿæˆè‡ªå®šä¹‰çš„.shä»»åŠ¡æ–‡ä»¶ï¼š å¯ä»¥æŠŠè½¬æ¢æ•°æ®çš„å‘½ä»¤æŠ½å–å‡ºæ¥ï¼Œå½¢æˆ run_convert_data.shæ–‡ä»¶ï¼š æ‰§è¡Œå®Œrun_convert_data.shåŽï¼Œåœ¨dataæ–‡ä»¶å¤¹é‡Œä¼šè‡ªåŠ¨ç”Ÿæˆconvertå‰ç¼€çš„æ•°æ®æ–‡ä»¶ã€‚ 12345678910set -e # set -o errexitset -u # set -o nounsetset -o pipefail # å°†å…·ä½“çš„æ–‡ä»¶åæŠŠä¸‹é¢å‘½ä»¤é‡Œçš„ä¸­æ–‡å­—æ›¿æ¢æŽ‰å³å¯echo &quot;convert train data&quot;python ./tools/tf_record_writer.py pointwise ./data/å¾…è½¬åŒ–çš„è®­ç»ƒæ•°æ®æ–‡ä»¶å ./data/å·²è½¬åŒ–çš„è®­ç»ƒæ•°æ®æ–‡ä»¶å 0 32echo &quot;convert test data&quot;python ./tools/tf_record_writer.py pointwise ./data/å¾…è½¬åŒ–çš„æµ‹è¯•æ•°æ®æ–‡ä»¶å ./data/å·²è½¬åŒ–çš„æµ‹è¯•æ•°æ®æ–‡ä»¶å 0 32echo &quot;convert data finish&quot; å®šä¹‰ä¸€ä¸ªcnné…ç½®æ–‡ä»¶çš„æ‰§è¡Œfreezeä»»åŠ¡çš„run_freeze_cnn.shï¼š æ‰§è¡Œå®Œrun_freeze_cnn.shåŽï¼Œæ ¹æ®é…ç½®æ–‡ä»¶é‡Œâ€œgraphâ€çš„å‚æ•°è®¾ç½®ï¼Œåœ¨è·¯å¾„ä¸Šä¼šè‡ªåŠ¨ç”Ÿæˆgraphæ–‡ä»¶å¤¹ï¼Œé‡Œé¢æœ‰model_cnn_pairwise.protxtæ–‡ä»¶ã€‚ 123456789set -e # set -o errexitset -u # set -o nounsetset -o pipefail in_task_type=&apos;freeze&apos; # è¾“å…¥ä»»åŠ¡ç±»åž‹ï¼Œå¯é€‰æ‹© train/predict/freeze/convertin_task_conf=&apos;./examples/cnn-pointwise.json&apos;python tf_simnet.py \ --task $in_task_type \ --task_conf $in_task_conf å®šä¹‰ä¸€ä¸ªä½¿ç”¨lstmç½‘ç»œçš„è®­ç»ƒä»»åŠ¡ run_train_lstm.shï¼ŒåŒæ—¶ä¸€å®šè¦è®°å¾—ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ï¼ï¼š 123456789set -e # set -o errexitset -u # set -o nounsetset -o pipefail in_task_type=&apos;train&apos;in_task_conf=&apos;./examples/lstm-pointwise.json&apos; # ä¿®æ”¹äº†é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé…ç½®æ–‡ä»¶å†…å‚æ•°ä¹Ÿå¾—ä¿®æ”¹å¥½python tf_simnet.py \ --task $in_task_type \ --task_conf $in_task_conf å½“lstmçš„trainä»»åŠ¡è·‘å®ŒåŽï¼Œåˆ©ç”¨å…¶ä¿å­˜çš„æ¨¡åž‹æ–‡ä»¶è¿›è¡Œpredictä»»åŠ¡ï¼Œæ–°å»ºrun_predict_lstm.shï¼š 123456789set -e # set -o errexitset -u # set -o nounsetset -o pipefail in_task_type=&apos;predict&apos; # ä¿®æ”¹äº†ä»»åŠ¡ç±»åž‹in_task_conf=&apos;./examples/lstm-pointwise.json&apos; # ç¡®è®¤lstmé…ç½®æ–‡ä»¶è·¯å¾„python tf_simnet.py \ --task $in_task_type \ --task_conf $in_task_conf .py æ–‡ä»¶èµ°è¯»tf_simnet.pytf_simnet.pyæ˜¯æ•´ä¸ªæ·±åº¦è¯­ä¹‰åŒ¹é…æ¡†æž¶çš„ä¸»è¿è¡Œpyæ–‡ä»¶ï¼Œæ‰“å¼€å¦‚ä¸‹ï¼Œå·²å¯¹ä¸»è¦ä»£ç è¿›è¡Œä¸€è¡Œè¡Œçš„æ³¨é‡Šï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187#coding=utf-8# Copyright (c) 2018 Baidu, Inc. All Rights Reserved.# # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at# # http://www.apache.org/licenses/LICENSE-2.0# # Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.import argparseimport loggingimport jsonimport sysimport osimport tensorflow as tffrom utils import datafeedsfrom utils import controlerfrom utils import utilityfrom utils import converter_WORK_DIR = os.path.split(os.path.realpath(__file__))[0]sys.path.append(os.path.join(_WORK_DIR, &apos;../../../common&apos;))import log# åŠ è½½é…ç½®æ–‡ä»¶ä¿¡æ¯ï¼Œå½¢æˆconfé…ç½®å­—å…¸def load_config(config_file): &quot;&quot;&quot; load config &quot;&quot;&quot; with open(config_file, &quot;r&quot;) as f: try: conf = json.load(f) except Exception: logging.error(&quot;load json file %s error&quot; % config_file) conf_dict = &#123;&#125; unused = [conf_dict.update(conf[k]) for k in conf] logging.debug(&quot;\n&quot;.join( [&quot;%s=%s&quot; % (u, conf_dict[u]) for u in conf_dict])) return conf_dictdef train(conf_dict): &quot;&quot;&quot; train &quot;&quot;&quot; # æ ¹æ®é…ç½®æ–‡ä»¶å…ˆåˆ¤æ–­dataæ–‡ä»¶æ˜¯ pairwise è¿˜æ˜¯ pointwise training_mode = conf_dict[&quot;training_mode&quot;] # æ ¹æ®é…ç½®æ–‡ä»¶è¾“å…¥netç½‘ç»œæ–‡ä»¶è·¯å¾„ + ç½‘ç»œç±»åž‹class net = utility.import_object( conf_dict[&quot;net_py&quot;], conf_dict[&quot;net_class&quot;])(conf_dict) if training_mode == &quot;pointwise&quot;: # å–‚æ•°æ®ï¼Œä»Žtrain_file, batch_size, num_epochs, shuffleç­‰é…ç½®ä¿¡æ¯ç¡®å®šæ•°æ®é˜Ÿåˆ—é•¿åº¦å’Œç§©åº datafeed = datafeeds.TFPointwisePaddingData(conf_dict) # ä»Žè½¬æ¢åŽçš„æ•°æ®ä¸­æ‹¿å‡ºä¸€ç»„ç»„ input_l, input_r, label_y, ä¸€æ­¥æ­¥çš„æ‹¿è¿›æ¥è®­ç»ƒ input_l, input_r, label_y = datafeed.ops() # åšè¯­ä¹‰åŒ¹é…é¢„æµ‹ pred = net.predict(input_l, input_r) # æ ¹æ®é…ç½®æ–‡ä»¶è®¾ç½®losså‡½æ•°è·¯å¾„ + lossç§ç±» loss_layer = utility.import_object( conf_dict[&quot;loss_py&quot;], conf_dict[&quot;loss_class&quot;])() loss = loss_layer.ops(pred, label_y) # pairwise å…ˆå¿½ç•¥ elif training_mode == &quot;pairwise&quot;: datafeed = datafeeds.TFPairwisePaddingData(conf_dict) input_l, input_r, neg_input = datafeed.ops() pos_score = net.predict(input_l, input_r) neg_score = net.predict(input_l, neg_input) loss_layer = utility.import_object( conf_dict[&quot;loss_py&quot;], conf_dict[&quot;loss_class&quot;])(conf_dict) loss = loss_layer.ops(pos_score, neg_score) else: print &gt;&gt; sys.stderr, &quot;training mode not supported&quot; sys.exit(1) # -------------------- # define optimizer # -------------------- # è¶…å‚æ•° å­¦ä¹ é€ŸçŽ‡çš„è®¾ç½® lr = float(conf_dict[&quot;learning_rate&quot;]) optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss) # è¿è¡Œ controler çš„ run_trainer å‡½æ•° controler.run_trainer(loss, optimizer, conf_dict)def predict(conf_dict): &quot;&quot;&quot; predict &quot;&quot;&quot; # æ ¹æ®é…ç½®æ–‡ä»¶è¾“å…¥netç½‘ç»œæ–‡ä»¶è·¯å¾„ + ç½‘ç»œç±»åž‹class net = utility.import_object( conf_dict[&quot;net_py&quot;], conf_dict[&quot;net_class&quot;])(conf_dict) # æ›´æ–°/è¦†ç›– conf_dicté…ç½®æ–‡ä»¶ çš„é…ç½®å‚æ•° ï¼ˆè¿™é‡Œéœ€è¦æ‰‹åŠ¨è°ƒæ•´ï¼‰ conf_dict.update(&#123;&quot;num_epochs&quot;: &quot;1&quot;, &quot;batch_size&quot;: &quot;1&quot;, &quot;shuffle&quot;: &quot;0&quot;, &quot;train_file&quot;: conf_dict[&quot;test_file&quot;]&#125;) # num_epochs = 1ï¼Œæ•°æ®é›†ä¸ºæ ·æœ¬å…¨é‡çš„1å€ï¼Œä»ä¸ºåŽŸæµ‹è¯•æ ·æœ¬ # batch_size = 1ï¼Œä¸€æ¬¡åªè¯»ä¸€æ¡æ ·æœ¬ï¼Œè¦†ç›–æŽ‰é…ç½®æ–‡ä»¶batch_sizeçš„å€¼ # shuffle = 0 /1ï¼Œæ ·æœ¬æ•°æ®æ˜¯å¦éšæœºè¯»å–ï¼Œè¦†ç›–æŽ‰é…ç½®æ–‡ä»¶shuffleçš„å€¼ # train_file ä¸º æµ‹è¯•é›†æ ·æœ¬è·¯å¾„ï¼Œè¦†ç›–æŽ‰é…ç½®æ–‡ä»¶çš„è®­ç»ƒé›†dataè·¯å¾„ # å–‚æ•°æ®ï¼Œä»Žtrain_file, batch_size, num_epochs, shuffleç­‰é…ç½®ä¿¡æ¯ç¡®å®šæ•°æ®é˜Ÿåˆ—é•¿åº¦å’Œç§©åº test_datafeed = datafeeds.TFPointwisePaddingData(conf_dict) # ä»Žè½¬æ¢åŽçš„æ•°æ®ä¸­æ‹¿å‡º test_l, test_r, test_y test_l, test_r, test_y = test_datafeed.ops() # test network # åšè¯­ä¹‰åŒ¹é…é¢„æµ‹ pred = net.predict(test_l, test_r) # è¿è¡Œ controler çš„ run_predict å‡½æ•° controler.run_predict(pred, test_y, conf_dict) # run_predict(pred, label, config)def freeze(conf_dict): &quot;&quot;&quot; freeze net for c api predict &quot;&quot;&quot; # ç½‘ç»œæ–‡ä»¶è·¯å¾„ + ç½‘ç»œç±»åž‹class net = utility.import_object( conf_dict[&quot;net_py&quot;], conf_dict[&quot;net_class&quot;])(conf_dict) test_l = dict([(u, tf.placeholder(tf.int32, [None, v], name=u)) for (u, v) in dict(conf_dict[&quot;left_slots&quot;]).iteritems()]) test_r = dict([(u, tf.placeholder(tf.int32, [None, v], name=u)) for (u, v) in dict(conf_dict[&quot;right_slots&quot;]).iteritems()]) pred = net.predict(test_l, test_r) controler.graph_save(pred, conf_dict)def convert(conf_dict): &quot;&quot;&quot; convert &quot;&quot;&quot; converter.run_convert(conf_dict)if __name__ == &quot;__main__&quot;: # åœ¨tfç›®å½•ä¸‹è‡ªåŠ¨æ–°å¢žlogæ–‡ä»¶å¤¹ log.init_log(&quot;./log/tensorflow&quot;) # å‘½ä»¤è§£æž parser = argparse.ArgumentParser() # å¢žåŠ taskå‘½ä»¤ï¼š å‘½ä»¤é€‰é¡¹ä¸º train // predict // freeze // convert parser.add_argument(&apos;--task&apos;, default=&apos;train&apos;, help=&apos;task: train/predict/freeze/convert, the default value is train.&apos;) # å¢žåŠ task_confå‘½ä»¤ï¼šå‘½ä»¤é€‰é¡¹ä¸ºexamplesç›®å½•ä¸‹çš„jsonæ–‡ä»¶ parser.add_argument(&apos;--task_conf&apos;, default=&apos;./examples/cnn-pointwise.json&apos;, help=&apos;task_conf: config file for this task&apos;) args = parser.parse_args() task_conf = args.task_conf # åŠ è½½é…ç½®æ–‡ä»¶ï¼Œconfig config = load_config(task_conf) task = args.task # åˆ¤æ–­ä»»åŠ¡ç±»åž‹ if args.task == &apos;train&apos;: train(config) elif args.task == &apos;predict&apos;: predict(config) elif args.task == &apos;freeze&apos;: freeze(config) elif args.task == &apos;convert&apos;: convert(config) else: print &gt;&gt; sys.stderr, &apos;task type error.&apos; å¯ç›´æŽ¥å…ˆçœ‹æœ€ä¸‹é¢çš„if __name__ == &quot;__main__&quot;:ï¼Œä¸¤ä¸ªparser.add_argumentåˆ†åˆ«å¯¹åº”.shæ–‡ä»¶çš„ä¸¤ä¸ªtaskå’Œtask_confå‚æ•°ã€‚é€‰æ‹©å¥½ä»»åŠ¡å’Œé…ç½®æ–‡ä»¶åŽå°±è¿è¡Œä»»åŠ¡ï¼Œå‡å¦‚ args.task == â€˜trainâ€™ï¼Œé‚£ä¹ˆè¿è¡Œtrainå‡½æ•°ã€‚ å†è·³åˆ°def train(conf_dict):è¿™è¡Œï¼Œçœ‹trainå‡½æ•°å¦‚ä½•è¿è¡Œã€‚å‰é¢å‡ è¡Œè¿˜æ˜¯æ¯”è¾ƒå¥½ç†è§£ï¼Œå¦‚æœ‰ç–‘é—®å¯å‘ç§ä¿¡é—®æˆ‘ï¼Œå¾—åˆ°lossï¼Œoptimizerå’Œé…ç½®conf_dictä¹‹åŽï¼Œæœ€åŽä¸€è¡Œåˆè¿è¡Œäº†ä¸€ä¸ªå¤§å‡½æ•°ï¼š 12# è¿è¡Œ controler çš„ run_trainer å‡½æ•°controler.run_trainer(loss, optimizer, conf_dict) é‚£ä¹ˆå°±å†è·³åˆ°controlerè¿™ä¸ªæ–‡ä»¶ï¼Œç»§ç»­å¾€ä¸‹çœ‹ã€‚ éœ€è¦ç‰¹æ®Šè¯´æ˜Žçš„æ˜¯ï¼Œå½“æ¨¡åž‹é¢„æµ‹æ‰§è¡Œpredictä»»åŠ¡è¿è¡Œpredictå‡½æ•°æ—¶ï¼Œnum_epochs/batch_size/shuffle/â€¦ç­‰å‚æ•°éœ€è¦åœ¨ä»£ç é‡Œé¢è°ƒæ•´ï¼Œè€Œé…ç½®æ–‡ä»¶é‡Œçš„å‚æ•°è®¾ç½®å¯¹æ¨¡åž‹é¢„æµ‹ä¸èµ·ä½œç”¨ï¼Œåœ¨tf_simnet.pyçš„predictå‡½æ•°æ‰¾åˆ°å¦‚ä¸‹ä»£ç è¿›è¡Œè®¾ç½®ï¼š 1234567# æ›´æ–°æµ‹è¯•é›†çš„ conf_dicté…ç½®æ–‡ä»¶ çš„é…ç½®å‚æ•° ï¼ˆè¿™é‡Œéœ€è¦æ‰‹åŠ¨è°ƒæ•´ï¼‰ conf_dict.update(&#123;&quot;num_epochs&quot;: &quot;1&quot;, &quot;batch_size&quot;: &quot;1&quot;, &quot;shuffle&quot;: &quot;0&quot;, &quot;train_file&quot;: conf_dict[&quot;test_file&quot;]&#125;) # num_epochs = 1ï¼Œæ•°æ®é›†ä¸ºæ ·æœ¬å…¨é‡çš„1å€ï¼Œä»ä¸ºåŽŸæµ‹è¯•æ ·æœ¬ # batch_size = 1ï¼Œä¸€æ¬¡åªè¯»ä¸€æ¡æ ·æœ¬ï¼Œè¦†ç›–æŽ‰åŽŸæœ‰batch_sizeçš„å€¼ # shuffle = 0 /1ï¼Œæ ·æœ¬æ•°æ®æ˜¯å¦éšæœºè¯»å–ï¼Œè¦†ç›–æŽ‰åŽŸæœ‰shuffleçš„å€¼ # train_file ä¸º æµ‹è¯•é›†æ ·æœ¬è·¯å¾„ï¼Œè¦†ç›–æŽ‰åŽŸå…ˆçš„è®­ç»ƒé›†dataè·¯å¾„ controler.pyæ‰“å¼€controler.pyå¯ä»¥çœ‹è§ï¼Œé‡Œé¢æœ‰ä¸Žtf_simnet.pyé‡Œçš„train/predict/freezeå‡½æ•°ä¸€ä¸€å¯¹åº”çš„run_train/run_predict/graph_saveå‡½æ•°ï¼Œå¯¹é‡è¦ä»£ç å·²åšæ³¨é‡Šï¼š ç»§ç»­ä¸Šé¢çš„æ€è·¯ï¼Œç›´æŽ¥çœ‹run_trainer(loss, optimizer, conf_dict)å‡½æ•°ã€‚åŒæ ·ï¼Œå¦‚æœ‰ç–‘é—®å¯ç•™è¨€æˆ–ç§ä¿¡æˆ‘ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162#coding=utf-8# Copyright (c) 2018 Baidu, Inc. All Rights Reserved.# # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at# # http://www.apache.org/licenses/LICENSE-2.0# # Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.import sysimport tensorflow as tfdef run_predict(pred, label, config): &quot;&quot;&quot; run classification predict function handle &quot;&quot;&quot; mean_acc = 0.0 # æ‰§è¡Œæ¨¡åž‹ä¿å­˜ä»»åŠ¡ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªSaverå¯¹è±¡ saver = tf.train.Saver() mode = config[&quot;training_mode&quot;] # labelè¾“å…¥å€¼ä¸ºäºŒç»´çš„0/1å‘é‡ï¼Œç±»ä¼¼å°†åŽŸå…ˆçš„labelå€¼onehotåŒ–ï¼ŒpredåŒç†ä¸ºäºŒç»´å‘é‡ # label = pred = å½¢å¦‚([0,1],[1,0],[1,0],[0,1],...) # æ‰¾å‡ºæ•°å€¼ä¸º1/æˆ–è€…è¯´æ•°å€¼æœ€å¤§çš„indexå€¼ï¼Œå› ä¸ºæ˜¯2ç»´ï¼Œindexå–å€¼åœ¨[0,1]ä¹‹é—´ï¼Œ(,1)ä»£è¡¨è¡Œ(,0)ä»£è¡¨åˆ— label_index = tf.argmax(label, 1) # label_indexä¸ºå‘é‡ï¼Œå½¢å¦‚([1],[0],[0],[1],...) if mode == &quot;pointwise&quot;: pred_prob = tf.nn.softmax(pred, -1) # å‘é‡ï¼Œ å°†predå˜æˆäºŒåˆ†ç±»çš„æ¦‚çŽ‡å°æ•°å‘é‡ï¼Œå½¢å¦‚([0.73,0.27],[0.1,0.9],[1.0,0],[0.87,0.13],...) score = tf.reduce_max(pred_prob, -1) # å‘é‡ï¼Œ å–å‘é‡é‡Œçš„æœ€å¤§æ¦‚çŽ‡å€¼ï¼Œè€Œpredç»“æžœä¸ºå¤§æ¦‚çŽ‡å€¼çš„è¾“å‡ºï¼Œ(,-1)ä¸ºåŽ»æŽ‰æœ€åŽä¸€ä¸ªå‘é‡ï¼Œå½¢å¦‚([0.73],[0.9],[1.0],[0.87],...) pred_index = tf.argmax(pred_prob, 1) # å‘é‡ï¼Œ åŒç†label_indexï¼Œå½¢å¦‚([0],[1],[0],[0],...) correct_pred = tf.equal(pred_index, label_index) # å‘é‡ï¼Œ åˆ¤æ–­label_indexä¸Žpred_indexæ˜¯å¦ä¸€è‡´ï¼Œä¸€è‡´ä¸º1ï¼Œä¸ä¸€è‡´ä¸º0 acc = tf.reduce_mean(tf.cast(correct_pred, &quot;float&quot;)) # æ•°å­—ï¼Œarray([0,0,0,1,1])ä¸Žarray([1,0,1,0,1]), æ±‚å¾—correct_pred = 2ï¼Œacc=2/5= 0.4=æ±‚çŒœå¯¹çš„æœŸæœ›å€¼ # &apos;pairwise&apos;å…ˆå¿½ç•¥ elif mode == &quot;pairwise&quot;: score = pred pred_index = tf.argmax(pred, 1) acc = tf.constant([0.0]) # æ‰¾åˆ°æ¨¡åž‹æ–‡ä»¶è·¯å¾„ modelfile = config[&quot;test_model_file&quot;] #æ–°å»ºresultæ–‡ä»¶ï¼Œä¿å­˜é¢„æµ‹ç»“æžœ result_file = file(config[&quot;test_result&quot;], &quot;w&quot;) step = 0 init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1)) \ as sess: sess.run(init) #åˆå§‹åŒ–è®¾ç½® saver.restore(sess, modelfile) # åŠ è½½æ¨¡åž‹æ–‡ä»¶ coord = tf.train.Coordinator() read_thread = tf.train.start_queue_runners(sess=sess, coord=coord) while not coord.should_stop(): step += 1 try: ground, pi, a, prob = sess.run([label_index, pred_index, acc, score]) # ç”¨äºŒç»´å‘é‡æ¥è¡¨ç¤ºlabelå’Œpredï¼Œç”¨æ•°å€¼1åœ¨äºŒç»´å‘é‡ä¸­çš„ä½ç½®æ˜¯å¦ä¸€è‡´åˆ¤æ–­labelä¸Žpredç»“æžœæ˜¯å¦ä¸€è‡´ mean_acc += a for i in range(len(prob)): result_file.write(&quot;%d\t%d\t%f\n&quot; % (ground[i], pi[i], prob[i])) # ä¸€æ­¥æ­¥å†™resultæ–‡ä»¶ï¼Œå…±3åˆ—ï¼Œåˆ†åˆ«æ˜¯ground[i], pi[i], prob[i] # ground[i]æ˜¯æµ‹è¯•é›†labelçš„å€¼ï¼Œ0æˆ–1 # pi[i]æ˜¯é¢„æµ‹çš„å€¼ï¼Œ0æˆ–1 # prob[i]æ˜¯é¢„æµ‹ç»“æžœçš„æ¦‚çŽ‡ except tf.errors.OutOfRangeError: coord.request_stop() coord.join(read_thread) sess.close() result_file.close() if mode == &quot;pointwise&quot;: mean_acc = mean_acc / step print &gt;&gt; sys.stderr, &quot;accuracy: %4.2f&quot; % (mean_acc * 100) # è¾“å‡ºaccuracydef run_trainer(loss, optimizer, config): &quot;&quot;&quot; run classification training function handle &quot;&quot;&quot; thread_num = int(config[&quot;thread_num&quot;]) model_path = config[&quot;model_path&quot;] # æ¨¡åž‹æ–‡ä»¶çš„å­˜å‚¨è·¯å¾„ model_file = config[&quot;model_prefix&quot;] # æ¨¡åž‹æ–‡ä»¶åçš„å‰ç¼€ print_iter = int(config[&quot;print_iter&quot;]) data_size = int(config[&quot;data_size&quot;]) batch_size = int(config[&quot;batch_size&quot;]) epoch_iter = int(data_size / batch_size) avg_cost = 0.0 # æ‰§è¡Œæ¨¡åž‹ä¿å­˜ä»»åŠ¡ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªSaverå¯¹è±¡ saver = tf.train.Saver(max_to_keep=None) # åˆå§‹åŒ– global variables init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=thread_num, inter_op_parallelism_threads=thread_num)) \ as sess: sess.run(init) # åˆ›å»ºä¸€ä¸ªçº¿ç¨‹ç®¡ç†å™¨ï¼ˆåè°ƒå™¨ï¼‰å¯¹è±¡ coord = tf.train.Coordinator() # æŠŠtensoræŽ¨å…¥å†…å­˜åºåˆ—ä¸­ï¼Œä¾›è®¡ç®—å•å…ƒè°ƒç”¨ read_thread = tf.train.start_queue_runners(sess=sess, coord=coord) # å¯åŠ¨å…¥é˜Ÿçº¿ç¨‹ï¼Œç”±å¤šä¸ªæˆ–å•ä¸ªçº¿ç¨‹ step = 0 epoch_num = 1 while not coord.should_stop(): # coord.should_stop() æŸ¥è¯¢æ˜¯å¦åº”è¯¥ç»ˆæ­¢æ‰€æœ‰çº¿ç¨‹ try: step += 1 c, _= sess.run([loss, optimizer]) avg_cost += c #----------------------------------------------------------- # å½“stepæ­¥æ•°æ˜¯print_iterçš„å€æ•°æ—¶ï¼Œå³æ¯print_iteræ­¥æ—¶ï¼Œæ‰“å°ä¸€ä¸ªloss if step % print_iter == 0: print(&quot;loss: %f&quot; % ((avg_cost / print_iter))) avg_cost = 0.0 #----------------------------------------------------------- # å½“stepæ­¥æ•°æ˜¯epoch_iterçš„å€æ•°æ—¶ï¼Œå³æ¯epoch_iteræ­¥æ—¶ï¼Œä¿å­˜ä¸€ä¸ªmodelæ–‡ä»¶ if step % epoch_iter == 0: print(&quot;save model epoch%d&quot; % (epoch_num)) save_path = saver.save(sess, &quot;%s/%s.epoch%d&quot; % (model_path, model_file, epoch_num)) epoch_num += 1 except tf.errors.OutOfRangeError: # å…¨éƒ¨æ­¥æ•°èµ°å®ŒåŽï¼Œä¿å­˜ä¸€ä¸ªfinalçš„æ¨¡åž‹æ–‡ä»¶ save_path = saver.save(sess, &quot;%s/%s.final&quot; % (model_path, model_file)) coord.request_stop() # å‘å‡ºç»ˆæ­¢æ‰€æœ‰çº¿ç¨‹çš„å‘½ä»¤ coord.join(read_thread) # æŠŠçº¿ç¨‹åŠ å…¥ä¸»çº¿ç¨‹ï¼Œç­‰å¾…threadsç»“æŸ sess.close()def graph_save(pred, config): &quot;&quot;&quot; run classify predict &quot;&quot;&quot; graph_path=config[&quot;graph_path&quot;] graph_name=config[&quot;graph_name&quot;] mode = config[&quot;training_mode&quot;] if mode == &quot;pointwise&quot;: pred_prob = tf.nn.softmax(pred, -1, name=&quot;output_prob&quot;) elif mode == &quot;pairwise&quot;: pred_prob = tf.identity(pred, name=&quot;output_prob&quot;) saver = tf.train.Saver() step = 0 init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1)) \ as sess: sess.run(init) tf.train.write_graph(sess.graph_def, graph_path, graph_name, as_text=True) sess.close() è·‘å®Œrun_trainer(loss, optimizer, config)ï¼Œå°±è‡ªåŠ¨ä¿å­˜äº†æ¨¡åž‹æ–‡ä»¶ï¼Œä¾›æ¨¡åž‹é¢„æµ‹ä½¿ç”¨ã€‚ åŒç†æŒ‰ç…§è¿™ä¸ªæ€è·¯ï¼Œå›žåˆ°tf_simnet.pyå¯ä»¥çœ‹predictä»»åŠ¡æˆ–è€…freeze(graph)ä»»åŠ¡çš„ä»£ç æµç¨‹ã€‚åŒæ ·ä¹Ÿä¼šå†çœ‹åˆ°controler.pyé‡Œé¢çš„å‡½æ•°æ˜¯å¦‚ä½•æ‰§è¡Œä»»åŠ¡çš„ã€‚ å†™åœ¨æœ€åŽ ä¸åŒçš„.shæ–‡ä»¶æœ‰ä¸åŒçš„taskå’Œtask_confå‚æ•°å€¼ï¼Œå¯¹åº”çš„æ˜¯è¿è¡Œtf_simnet.pyå’Œcontroler.pyé‡Œä¸åŒçš„å‡½æ•°ã€‚ å…¶å®ƒçš„.pyæ–‡ä»¶ç‚¹å¼€æ¥åº”è¯¥ä¸å¤ªéš¾ç†è§£ï¼Œå¦‚æœ‰ç–‘é—®å¯ç•™è¨€æˆ–ç§ä¿¡æˆ‘ã€‚ æ¯ä¸ªæ¨¡åž‹æ–‡ä»¶æ¯”è¾ƒå¤§ï¼Œæˆ‘è·‘å‡ºæ¥çš„æ¯ä¸ªæœ‰1.5Gå·¦å³å¤§å°ã€‚ è‹¥æƒ³è§‚å¯Ÿæ¯ä¸€æ­¥çš„æ•°æ®å½¢çŠ¶æˆ–è€…å˜æ¢æƒ…å†µï¼Œå¯è‡ªè¡ŒåŠ å…¥printå‘½ä»¤æ‰“å°å‡ºæ¥çœ‹çœ‹ã€‚ æ¯ä¸ªä»»åŠ¡å®ŒæˆåŽæœ‰è‡ªåŠ¨ç”Ÿæˆçš„æ–‡ä»¶ï¼Œå¯ä»¥ç ”ç©¶çœ‹çœ‹ï¼Œæ¯”å¦‚modelæ–‡ä»¶ï¼Œresultæ–‡ä»¶ï¼Œgraphæ–‡ä»¶ï¼Œlogæ–‡ä»¶ã€‚ ä»£ç èµ°è¯»åªæ˜¯ç†è§£çš„ç¬¬ä¸€æ­¥ï¼Œè‹¥è¦æ‰Žå®žæŽŒæ¡è¿˜éœ€è‡ªå·±æ”¹åŠ¨ä»£ç ï¼Œå†è°ƒè¯•è°ƒè¯•ã€‚ æ¬¢è¿Žæ‰“èµðŸ˜˜]]></content>
      <tags>
        <tag>Semantic Matching</tag>
        <tag>NLP</tag>
        <tag>code review</tag>
        <tag>QA</tag>
        <tag>è¯­ä¹‰åŒ¹é…</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLPç¬”è®° - Word Tokenization // wordcloud è¯äº‘å›¾æ•™ç¨‹]]></title>
    <url>%2F2018%2F09%2F03%2FNLP%E7%AC%94%E8%AE%B0-Word-Tokenization-wordcloud%2F</url>
    <content type="text"><![CDATA[é¢å‘å¯¹è±¡ï¼šæƒ³åšç®€å•çš„æ–‡æœ¬å¯è§†åŒ–åˆ†æž é€‰æ‰‹ã€‚ æ¦‚å¿µè§£é‡Š è¯äº‘å›¾ï¼ˆwordcloudï¼‰ï¼šæ˜¯è¿™ä¸¤å¹´æ¯”è¾ƒç«çš„æ–‡æœ¬å¯è§†åŒ–åˆ†æžçš„ä¸€ç§ï¼Œä¸Šå›¾å°±çŸ¥é“è¯´çš„å•¥äº†ï¼š jiebaï¼špythonåº“ï¼Œç”¨äºŽä¸­æ–‡åˆ†è¯ã€‚ wordcloudï¼špythonåº“ï¼Œç”¨äºŽè¯äº‘å›¾åˆ¶ä½œã€‚ åœç”¨è¯è¡¨ï¼ˆstopwordsï¼‰ï¼šåœ¨è‹±æ–‡ä¸­åƒâ€œthe / of / a / for /â€¦â€ï¼Œåœ¨ä¸­æ–‡ä¸­åƒâ€œçš„ / æ˜¯ / ä¹Ÿ / ä¹‹ /â€¦â€è¿™æ ·çš„æ²¡æœ‰å®žé™…æ„ä¹‰å´å‡ºçŽ°é¢‘çŽ‡è¾ƒé«˜çš„è¯ã€‚ä¸ºäº†é˜²æ­¢è¿™äº›è¯æŠ¢äº†æ¯”å¦‚æ•…äº‹ä¸»è§’åçš„ä½ç½®ï¼Œå°±äº‹å…ˆä½œä¸ºåœç”¨è¯ï¼Œä¸è¿›å…¥æ–‡æœ¬åˆ†æžã€‚ æŒ‰è§„çŸ©ï¼Œå…ˆä¸Šæ–‡æ¡£ç»“æž„å›¾ï¼æ–‡æ¡£ä¸­æ‰€éœ€æ–‡ä»¶çš„ä¸‹è½½åœ°å€ï¼Œç‚¹è¿™é‡Œ 123456789101112|-wordcloud //æ–°å»ºæ–‡ä»¶å¤¹ |- data //æ–°å»ºæ–‡ä»¶å¤¹ |- alice.txt //ä¸‹è½½æ–‡ä»¶ |- yxgltext.txt //ä¸‹è½½æ–‡ä»¶ |- stopwords.txt //ä¸‹è½½æ–‡ä»¶ |- SourceHanSerifK-Light.otf //ä¸‹è½½æ–‡ä»¶ |- plot //æ–°å»ºæ–‡ä»¶å¤¹ |- alice_color.png //ä¸‹è½½å›¾ç‰‡ |- queen.jpg //ä¸‹è½½å›¾ç‰‡ |- alice1.py //æ–°å»ºpythonæ–‡ä»¶ |- alice2.py //æ–°å»ºpythonæ–‡ä»¶ |- queen.py //æ–°å»ºpythonæ–‡ä»¶ è‹±æ–‡è¯äº‘å›¾ä¾‹å­1ï¼š10è¡Œä»£ç æžå®šçš„è¯äº‘å›¾ // alice1.pyè¾“å…¥ï¼š 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Mon Sep 3 17:53:03 2018@author: Yi&quot;&quot;&quot;import osos.chdir(&quot;C:/Users/Yi/Desktop/nlp/wordcloud&quot;) # æ¢æˆä½ çš„wordcloudæ–‡ä»¶å¤¹æ‰€åœ¨è·¯å¾„from wordcloud import WordCloudf = open(&apos;data/alice.txt&apos;).read()wordcloud = WordCloud(background_color=&quot;white&quot;,width=1000, height=860, margin=2).generate(f) # width,height,marginå¯ä»¥è®¾ç½®å›¾ç‰‡å±žæ€§# generate å¯ä»¥å¯¹å…¨éƒ¨æ–‡æœ¬è¿›è¡Œè‡ªåŠ¨åˆ†è¯,ä½†æ˜¯å¯¹ä¸­æ–‡æ”¯æŒä¸å¥½# wordcloud = WordCloud(font_path = r&apos;D:\Fonts\simkai.ttf&apos;).generate(f)# ä½ å¯ä»¥é€šè¿‡font_pathå‚æ•°æ¥è®¾ç½®å­—ä½“é›†# background_colorå‚æ•°ä¸ºè®¾ç½®èƒŒæ™¯é¢œè‰²,é»˜è®¤é¢œè‰²ä¸ºé»‘è‰²import matplotlib.pyplot as pltax = plt.imshow(wordcloud)fig = ax.figurefig.set_size_inches(25,20) # å¯è°ƒèŠ‚å›¾ç‰‡ç´§å¯† å°ºå¯¸ç¨‹åº¦plt.axis(&quot;off&quot;)plt.show()wordcloud.to_file(&apos;plot/test.png&apos;) è¾“å‡ºï¼š ä¾‹å­2ï¼šæœ‰å½¢çŠ¶çš„è¯äº‘å›¾ // alice2.pyè¾“å…¥ï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Tue Sep 4 10:05:29 2018@author: Yi&quot;&quot;&quot;import osos.chdir(&quot;C:/Users/Yi/Desktop/nlp/wordcloud&quot;)from PIL import Imageimport numpy as npimport matplotlib.pyplot as pltfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator# Read the whole text.text = open(&apos;data/alice.txt&apos;).read()alice_coloring = np.array(Image.open(&quot;plot/alice_color.png&quot;)) # å¯éšæ„æ›´æ¢å›¾ç‰‡stopwords = set(STOPWORDS)stopwords.add(&quot;said&quot;)# ä½ å¯ä»¥é€šè¿‡ mask å‚æ•° æ¥è®¾ç½®è¯äº‘å½¢çŠ¶wc = WordCloud(background_color=&quot;white&quot;, max_words=2000, mask=alice_coloring, stopwords=stopwords, max_font_size=40, random_state=42)# generate word cloudwc.generate(text)# create coloring from imageimage_colors = ImageColorGenerator(alice_coloring)# æ–¹å¼ 1 -------------------------------------------------------------------# show#fig, axes = plt.subplots(1, 3)#axes[0].imshow(wc, interpolation=&quot;bilinear&quot;)#axes[1].imshow(wc.recolor(color_func=image_colors), interpolation=&quot;bilinear&quot;)#axes[2].imshow(alice_coloring, cmap=plt.cm.gray, interpolation=&quot;bilinear&quot;)##for ax in axes:# ax.set_axis_off()# fig = ax.figure# fig.set_size_inches(25,20) # å¯è°ƒèŠ‚å›¾ç‰‡ç´§å¯† å°ºå¯¸ç¨‹åº¦#plt.show()# æ–¹å¼ 2 -------------------------------------------------------------------# show# åœ¨åªè®¾ç½®maskçš„æƒ…å†µä¸‹,ä½ å°†ä¼šå¾—åˆ°ä¸€ä¸ªæ‹¥æœ‰å›¾ç‰‡å½¢çŠ¶çš„è¯äº‘plt.axis(&quot;off&quot;)ax = plt.imshow(wc, interpolation=&quot;bilinear&quot;)fig = ax.figurefig.set_size_inches(25,20) # å¯è°ƒèŠ‚å›¾ç‰‡ç´§å¯† å°ºå¯¸ç¨‹åº¦plt.figure()# recolor wordcloud and show# we could also give color_func=image_colors directly in the constructor# æˆ‘ä»¬è¿˜å¯ä»¥ç›´æŽ¥åœ¨æž„é€ å‡½æ•°ä¸­ç›´æŽ¥ç»™é¢œè‰²# é€šè¿‡è¿™ç§æ–¹å¼è¯äº‘å°†ä¼šæŒ‰ç…§ç»™å®šçš„å›¾ç‰‡é¢œè‰²å¸ƒå±€ç”Ÿæˆå­—ä½“é¢œè‰²ç­–ç•¥plt.axis(&quot;off&quot;)ax = plt.imshow(wc.recolor(color_func=image_colors), interpolation=&quot;bilinear&quot;)fig = ax.figurefig.set_size_inches(25,20) # å¯è°ƒèŠ‚å›¾ç‰‡ç´§å¯† å°ºå¯¸ç¨‹åº¦plt.figure()plt.axis(&quot;off&quot;)ax = plt.imshow(alice_coloring, cmap=plt.cm.gray, interpolation=&quot;bilinear&quot;)fig = ax.figurefig.set_size_inches(25,20) # å¯è°ƒèŠ‚å›¾ç‰‡ç´§å¯† å°ºå¯¸ç¨‹åº¦plt.show() è¾“å‡ºï¼š ä¸­æ–‡è¯äº‘å›¾ä¾‹å­ï¼šå»¶ç¦§æ”»ç•¥çš„ç™½æœˆå…‰ // queen.py ä¸­æ–‡ä¸Žè‹±æ–‡è¿˜æ˜¯æœ‰ç‚¹ä¸ä¸€æ ·çš„ï¼Œåœ¨åœç”¨è¯è¡¨å°±éœ€è¦è‡ªå·±å¼„ä¸€å¥—ç­‰ç­‰ã€‚è®°å¾—è·‘ä¹‹å‰è¦æŠŠè¯¥ä¸‹è½½çš„æ–‡ä»¶ä¸‹è½½åˆ°æ–‡ä»¶å¤¹é‡Œã€‚ è¾“å…¥ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Mon Sep 3 18:27:38 2018@author: Yi&quot;&quot;&quot;import osos.chdir(&quot;C:/Users/Yi/Desktop/nlp/wordcloud&quot;) import jieba.analyse # å¯¼å…¥ç»“å·´åˆ†è¯import numpy as np # numpyfrom wordcloud import WordCloud, STOPWORDS # è¯äº‘å·¥å…·å’Œè‡ªå¸¦çš„çš„åœç”¨è¯ï¼Œè‹±æ–‡from PIL import Image # å›¾ç‰‡å¤„ç†import matplotlib.pyplot as pltdef handle(textfile, stopword): with open(textfile, &apos;r&apos;) as f: data = f.read() wordlist = jieba.analyse.extract_tags(data, topK=100) # åˆ†è¯ï¼Œå–å‰100 wordStr = &quot; &quot;.join(wordlist) print (wordStr) hand = np.array(Image.open(&apos;plot/queen.jpg&apos;)) # æ‰“å¼€ä¸€å¼ å›¾ç‰‡ï¼Œè¯è¯­ä»¥å›¾ç‰‡å½¢çŠ¶ä¸ºèƒŒæ™¯åˆ†å¸ƒ my_cloudword = WordCloud( # wordcloudå‚æ•°é…ç½® width=1024, height=768, background_color = &apos;white&apos;, # èƒŒæ™¯é¢œè‰²è®¾ç½®ç™½è‰² mask = hand, # èƒŒæ™¯å›¾ç‰‡ max_words = 100, # æœ€å¤§æ˜¾ç¤ºçš„å­—æ•° stopwords = stopword, # åœç”¨è¯ max_font_size = 100, # å­—ä½“æœ€å¤§å€¼ font_path=&apos;data/SourceHanSerifK-Light.otf&apos;, # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè‹¥æ˜¯æœ‰ä¸­æ–‡çš„è¯ï¼Œè¿™å¥ä»£ç å¿…é¡»æ·»åŠ ï¼Œä¸ç„¶ä¼šå‡ºçŽ°æ–¹æ¡†ï¼Œä¸å‡ºçŽ°æ±‰å­— random_state=3, # è®¾ç½®æœ‰å¤šå°‘ç§éšæœºç”ŸæˆçŠ¶æ€ï¼Œå³æœ‰å¤šå°‘ç§é…è‰²æ–¹æ¡ˆ ) my_cloudword.generate(wordStr) # ç”Ÿæˆå›¾ç‰‡ my_cloudword.to_file(&apos;plot/queen.png&apos;) # ä¿å­˜ plt.axis(&apos;off&apos;) # æ˜¯å¦æ˜¾ç¤ºxè½´ã€yè½´ä¸‹æ ‡ ax = plt.imshow(my_cloudword) # æ˜¾ç¤ºè¯äº‘å›¾ fig = ax.figure fig.set_size_inches(25,20) # å¯è°ƒèŠ‚å›¾ç‰‡ç´§å¯† å°ºå¯¸ç¨‹åº¦ plt.show() # æ˜¾ç¤ºstopwords = open(&apos;data/stopwords.txt&apos;).read()stopwords = set(stopwords.split(&apos;\n&apos;))if __name__ == &apos;__main__&apos;: handle(&apos;data/yxgl.txt&apos;, stopwords) è¾“å‡ºï¼š å†æ¥ä¸€æ¬¡ï¼š ç™½æœˆå…‰çš‡åŽçš„äººå¤´å½¢çŠ¶å’Œæ‰‡å­å½¢çŠ¶éƒ½è¿˜åœ¨çš„ã€‚ å†™åœ¨æœ€åŽè¿™é‡Œç”¨åˆ°çš„æ–‡æœ¬åˆ†æžçš„æŠ€æœ¯åªåœç•™åœ¨åˆ†è¯é˜¶æ®µï¼Œè¿˜æ˜¯æ¯”è¾ƒç®€å•çš„ã€‚å¯è§†åŒ–åˆ†æžæ°¸è¿œæ˜¯æœ€å¸å¼•äººçš„ã€‚å¿«åŽ»è¯•ä¸€ä¸‹å§~ðŸ˜‹]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>jieba</tag>
        <tag>tokenization</tag>
        <tag>wordcloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLPç¬”è®° - Word Embedding // doc2vec ä¹‹ å»¶ç¦§æ”»ç•¥]]></title>
    <url>%2F2018%2F08%2F28%2FNLP%E7%AC%94%E8%AE%B0-Word-Embedding-doc2vec%2F</url>
    <content type="text"><![CDATA[é¢å‘è¯»è€…ï¼šnlpå…¥é—¨ï¼Œpythoné€‰æ‰‹ï¼Œå¯¹word embeddingï¼ˆè¯åµŒå…¥ï¼‰æœ‰å¤§æ¦‚äº†è§£ã€‚æœ¬æ–‡æ˜¯åŸºäºŽdoc2vecçš„ä¸€ä¸ªå…³äºŽå»¶ç¦§æ”»ç•¥å‰§æƒ…æ–‡æœ¬çš„å°demoã€‚doc2vevåŸºäºŽword2vecï¼Œå®ƒä¿©å¾ˆåƒï¼Œä½¿ç”¨æ–¹æ³•ä¹Ÿå¾ˆåƒã€‚æœ‰ç©ºå†æŠŠåŽŸç†è¡¥ä¸Šã€‚ è¯­æ–™æ–‡æœ¬yxgltext.txtç‚¹è¿™é‡Œä¸‹è½½ï¼Œå…¶å®žå°±æ˜¯ä»Žç™¾åº¦ä¸Šå¤åˆ¶ç²˜è´´çš„å‰20é›†å·¦å³çš„å‰§æƒ…æ–‡å­—ï¼Œå¤§å®¶å¯ä»¥éšæ„æ›´æ”¹è¯­æ–™æ–‡å­—ã€‚æ–‡ä»¶ç»“æž„å¦‚ä¸‹ï¼Œè®°å¾—ä¸‹è½½yxgltext.txtã€‚ 12345|-nlp //æ–°å»ºæ–‡ä»¶å¤¹ |- doc2vec.py //æ–°å»ºpythonæ–‡ä»¶ |- data //æ–°å»ºæ–‡ä»¶å¤¹ |- yxgltext.txt //ä¸‹è½½è¯­æ–™æ•°æ®æ”¾åœ¨dataæ–‡ä»¶å¤¹ç›®å½•ä¸‹ |- model //æ–°å»ºæ–‡ä»¶å¤¹ Talk is cheap, show me the code! ä¸Šä»£ç ~~ è¾“å…¥ï¼š 1234567891011121314import osos.chdir(&quot;C:/Users/Yi/Desktop/nlp&quot;) # nlpæ–‡ä»¶å¤¹çš„è·¯å¾„import jieba # ä¸­æ–‡åˆ†è¯å·¥å…·import sysimport gensimimport sklearnimport numpy as npfrom gensim.models.doc2vec import Doc2Vec, LabeledSentence #ä»Žgensimå¯¼å…¥doc2vecTaggededDocument = gensim.models.doc2vec.TaggedDocument# è™šè¯ï¼Œå¯ä»¥éšæ„æ·»åŠ åˆ é™¤stoplist = [&apos;çš„&apos;,&apos;äº†&apos;,&apos;è¢«&apos;,&apos;ã€‚&apos;,&apos;ï¼Œ&apos;,&apos;ã€&apos;,&apos;å¥¹&apos;,&apos;è‡ªå·±&apos;,&apos;ä»–&apos;,&apos;å¹¶&apos;,&apos;å’Œ&apos;,&apos;éƒ½&apos;,&apos;åŽ»&apos;,&apos;\n&apos;] è¾“å…¥ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#è¿›è¡Œä¸­æ–‡åˆ†è¯def cut_files(): filePath = &apos;data/yxgltext.txt&apos; fr = open(filePath, &apos;rb&apos;) fvideo = open(&apos;data/yxglCut.txt&apos;, &quot;w&quot;) for line in fr.readlines(): curLine =&apos; &apos;.join(list(jieba.cut(line))) fvideo.writelines(curLine) #è¯»å–åˆ†è¯åŽçš„æ•°æ®å¹¶æ‰“æ ‡è®°ï¼Œæ”¾åˆ°x_trainä¾›åŽç»­ç´¢å¼•ï¼Œå ç”¨å¾ˆå¤§å†…å­˜ï¼ˆä¾›å°æ•°æ®é‡ä½¿ç”¨ï¼‰def get_datasest(): with open(&quot;data/yxglCut.txt&quot;, &apos;r&apos;) as cf: docs = cf.readlines() # åˆ é™¤å¸¸ç”¨è¯ for idx in list(range(0,len(docs))): docs[idx] = &apos; &apos;.join([word for word in docs[idx].split( ) if word not in stoplist]) docs = [doc for doc in docs if len(doc)&gt;0] print(len(docs)) x_train = [] for i, text in enumerate(docs): word_list = text.split(&apos; &apos;) l = len(word_list) word_list[l - 1] = word_list[l - 1].strip() document = TaggededDocument(word_list, tags=[i]) x_train.append(document) return x_train#æ¨¡åž‹è®­ç»ƒdef train(x_train, size=200, epoch_num=1): # size=200 æ„å‘³ç€ æ¯ä¸ªè¯å‘é‡æ˜¯200ç»´çš„ # ä½¿ç”¨ Doc2Vec å»ºæ¨¡ model_dm = Doc2Vec(x_train, min_count=1, window=3, size=size, sample=1e-3, negative=5, workers=4) #model_dm.train(x_train, total_examples=model_dm.corpus_count, epochs=70) model_dm.save(&apos;model/model_dm_doc2vec&apos;) return model_dm#å®žä¾‹def test():# model_dm = Doc2Vec.load(&quot;model/model_dm_doc2vec&quot;) test_text = [&apos;æˆ‘&apos;, &apos;å–œæ¬¢&apos;, &apos;å‚…æ’&apos;] inferred_vector_dm = model_dm.infer_vector(test_text) # é€‰å–ç›¸å…³åº¦æœ€é«˜çš„10ä¸ªè¯ sims = model_dm.docvecs.most_similar([inferred_vector_dm], topn=10) return sims è¾“å…¥ï¼š 12345678910cut_files()x_train=get_datasest()model_dm = train(x_train)sims = test()for count, sim in sims: sentence = x_train[count] words = &apos;&apos; for word in sentence[0]: words = words + word + &apos; &apos; print (words, sim, len(sentence[0])) è¾“å‡ºï¼š 123å¨´å¦ƒ æè®® è®© ä»Ž æ±Ÿå— è¯·æ¥ ååŒ» å¶å¤©å£« ä¸º äº” é˜¿å“¥ åŒ»æ²» çš‡ä¸Š åº”å… å¶å¤©å£« è‚¯å®š äº” é˜¿å“¥ æ‚£ é»„ç–¸ ä¿è¯ ç”¨é€€ é»„æ–¹ å°± å¯æ²»å¥½ å¼˜åŽ† æ¾ å£æ°” é«˜ è´µå¦ƒ è§é£Žä½¿èˆµ å‘ çš‡ä¸Š å‘Šç½ª çš‡ä¸Š è¡¨ç¤º è°…è§£ è¿™æ—¶ çº¯å¦ƒ å¸¦ äºº æŠ¬ æ­¤å‰ ç…§æ–™ æ„‰ è´µäºº é¥®é£Ÿ ä¸€å è’™å¤ åŽ¨å¸ˆ å°¸ä½“ ä¸Šæ¥ äº†è§£ è¿‡æ„‰ è´µäºº å­•æœŸ é¥®é£Ÿä¹ æƒ¯ åŽ å¶å¤©å£« ç¦€æŠ¥ å¼˜åŽ† å©´å„¿ çž³å­” é‡‘é»„ æ€ªç—… å¤šå›  æ¯ä½“ æ¹¿çƒ­ èƒ†æ± æ·¤ç§¯ è€Œç”Ÿ å­•å¦‡ åº”å½“ æ³¨æ„ é¥®é£Ÿ ä¸è¿‡ åˆ† é£Ÿç”¨ ç”œé£Ÿ çƒ«é£Ÿ è…¥è†» ä¹‹ç‰© ç’Žçž æ„æœ‰ æ‰€æŒ‡ æ˜¯ é«˜ è´µå¦ƒ æƒ³ å¯¹ä»˜ æ„‰ è´µäºº äº” é˜¿å“¥ é«˜ è´µå¦ƒ è¾©é©³ çº¯å¦ƒ å´ å‘ˆ ä¸Š è¯æ® æ˜¯ åŽ¨å¸ˆ æ­»å‰ ç•™ä¸‹ ä¸€å° æŒ‡è®¤ é«˜ è´µå¦ƒ è¡€ä¹¦ å¼˜åŽ† å¤§ä¸º æ¼ç« è½¯ç¦ é«˜ è´µå¦ƒ çš‡ä¸Š å‡†å¤‡ ç¦»å¼€ æ—¶ æ˜ŽçŽ‰ æ‹¦ä½ çš‡ä¸Š å‘Šå‘ ç’Žçž ç›—ç”¨ çš‡åŽ é‡‘å° ç’Žçž æ‰“å¼€ åŒ£å­ é‡Œé¢ åªæ˜¯ ä¸€å— ç šå° æ˜ŽçŽ‰å›  è¯¬å‘Š è€Œ å—ç½š éšåŽ ç’Žçž æ‹¦ä½ çº¯å¦ƒ ä¸Ž è¯´è¯ æŒ‡å‡º åŽ¨å¸ˆ è‡ªå°½ ç•™ä¸‹ è¡€ä¹¦ ä¸€äº‹ æ˜¯ ç­–åˆ’ çº¯å¦ƒ æé†’ ç’Žçž åˆ«ç«™ é”™ é˜Ÿ 0.17824654281139374 160é«˜ è´µå¦ƒ å·§å¦™ åˆ©ç”¨ è¿™æ¬¡ æœºä¼š å¯¹ çš‡ä¸Š è¯‰è¯´ è¡·è‚  èµ¢å¾— å¼˜åŽ† è°…è§£ ä¸Žæ­¤åŒæ—¶ çš‡åŽ åœ¨ é•¿æ˜¥ å®«é—¨ å£è‹¦ ç­‰ å¼˜åŽ† ä¸è‡³ æ·±æ„Ÿ å¤±æœ› æ¬¡æ—¥ ç’Žçž åœ¨ å¾¡èŠ±å›­ å‘æ³„ å¿ƒä¸­ å¯¹ çš‡ä¸Š è¾œè´Ÿ çš‡åŽ ä¸æ»¡ é‡åˆ° å‚…æ’ ç’Žçž å‘ æ›¿ çš‡åŽ é¸£ä¸å¹³ å‡ºè¨€ä¸é€Š å‚…æ’ åŠé˜» éšå³ ç’Žçž å›  ç™¾èˆ¬ æŸ¥æŽ¢ å§å§ æ­»å›  å´ ä¸€æ— æ‰€èŽ· è¶Šå‘ ç„¦èº å¼˜åŽ† æ·±å¤œ æ‰¹é˜… å¥ç«  èº«ä½“ ä¸é€‚ å¬ å¤ªåŒ» å‰æ¥ è¯Šæ²» å‘çŽ° æ‚£ ç–¥ç–® çš‡åŽ ä¸é¡¾ ä¼ æŸ“ å±é™© æ‰§æ„ è¦ æ¬å…¥ å…»å¿ƒæ®¿ äº²è‡ª ç…§æ–™ å¼˜åŽ† åŽŸæœ¬ è®© æ˜ŽçŽ‰ éšè¡Œ å¯æ˜ŽçŽ‰ å´ å°† æ­¤ å·® æŽ¨ ç»™ ç’Žçž ç’Žçž ä¸ºäº† è°ƒæŸ¥ çš‡ä¸Š èº«è¾¹ äº²ä¿¡ æŸ¥æŽ¢ å§å§ çœŸæ­£ æ­»å›  äºŽæ˜¯ åŒæ„ è·Ÿ æ˜ŽçŽ‰ è°ƒæ¢ å·®äº‹ è·Ÿéš çš‡åŽ ä¸€é“ æ¬ å…»å¿ƒæ®¿ ç’Žçž æ›¿ çš‡ä¸Š ä¸Šè¯ çš‡ä¸Š ååˆ† åæ„Ÿ æ‹’ç» ç’Žçž å¯ æŽçŽ‰ ç²—æ‰‹ç¬¨è„š å¼„ ç—› çš‡ä¸Š çš‡ä¸Š æ¼æ€’ ç’Žçž å‘Šè¯‰ çš‡ä¸Š å…»å¿ƒæ®¿ ä¼ºå€™ å¤šåŠ æ˜¯ å¤ªç›‘ å¦‚æžœ åšæŒ é‚£ä¹ˆ åªå¥½ è¯· çš‡åŽ æ¥ æ›¿ ä¸Šè¯ çš‡ä¸Š æ— å¥ˆ åªå¥½ è®© ç’Žçž ç»§ç»­ çš‡ä¸Š æ¶‚ è¯ ä¹‹åŽ ä¾ç„¶ ç‡¥çƒ­ ç˜™ç—’ éš¾è€ çš‡åŽ è¡£ä¸è§£å¸¦ åœ° ç…§é¡¾ æ•´æ•´ ä¸€ æ™šä¸Š ç’Žçž è§çŠ¶ ååˆ† åŠ¨å®¹ ç’Žçž è¯•æŽ¢ æŽçŽ‰ è¯¢é—® ä¹¾ æ¸…å®« å¤œå®´ å½“æ™š æ›¾ç» ç¦»å¸­ å®—å®¤ å¯æƒœ ä¸€æ— æ‰€èŽ· 0.1443883627653122 185... è¾“å…¥ï¼š 1print(model_dm.wv[&apos;ç’Žçž&apos;]) è¾“å‡ºâ€™ç’Žçžâ€™çš„200ç»´è¯å‘é‡ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[ 1.32339250e-03 -4.36101720e-04 8.61682580e-04 -5.60876098e-04 -1.10074517e-03 -4.01582598e-04 1.39182212e-05 1.03741838e-03 1.33155310e-03 4.53286630e-04 -1.02781062e-03 -8.92800104e-04 1.19402306e-03 -3.00986052e-04 -1.55415002e-03 -2.69316044e-03 1.58681255e-03 -8.10362690e-04 5.34354069e-04 -1.31634891e-03 -3.59648140e-03 2.49065284e-04 8.13953171e-04 -8.55766921e-05 2.76492530e-04 -1.29517284e-03 1.02521526e-03 8.73336976e-04 1.62727723e-03 -6.10298535e-04 -1.21042994e-03 -1.87295862e-03 -2.03051459e-04 -3.54788470e-04 1.25130301e-03 8.69541487e-04 -2.45160703e-03 -9.03088134e-04 5.02681173e-03 -1.03742653e-03 3.97383585e-04 1.10275706e-03 3.76813230e-04 -2.43625650e-03 3.11101991e-04 1.97053305e-03 2.52972008e-03 -1.45180838e-03 -1.74685894e-03 1.52873830e-03 4.81644034e-04 1.05112646e-04 2.67350441e-03 8.58452288e-04 6.63276296e-05 -1.97039312e-03 5.31882746e-04 -4.36584116e-04 1.26765005e-03 -3.08679766e-03 1.69386994e-03 -2.96112709e-03 2.48387340e-03 -3.73846688e-03 -3.07446043e-03 -4.49631305e-04 1.78120867e-03 -1.19638827e-03 -2.00018892e-03 -6.16657664e-04 1.24890637e-03 1.04953512e-03 6.38565107e-04 -8.65224341e-04 1.56678446e-03 2.29814858e-03 -4.69850667e-04 6.30659808e-04 2.44404143e-03 1.34824484e-03 -3.52538045e-04 2.64616770e-04 9.84614133e-04 -5.64393296e-04 -1.46174955e-03 2.11890996e-03 2.74263322e-04 -1.95100205e-03 2.42348132e-03 -4.13818937e-03 1.28919329e-03 -7.49823987e-04 3.59561713e-03 2.89021351e-04 1.64465397e-04 3.35634919e-04 6.11493131e-04 2.10861443e-03 6.76521973e-04 -1.72132370e-03 -9.39077465e-04 1.75529323e-03 -1.22920389e-03 2.14341236e-03 -2.19211495e-03 1.65924046e-03 2.23257625e-03 -2.71887379e-03 -3.23694688e-03 -2.48166034e-03 -3.01317009e-03 1.18382962e-03 3.18966959e-05 3.01953492e-04 2.36387877e-03 5.23283597e-05 1.89765415e-03 8.61766574e-04 -2.39132158e-03 -1.02647720e-03 -1.90407838e-04 5.11635910e-04 1.44841790e-03 2.69743241e-03 1.57171465e-03 -7.98581314e-05 -3.73520626e-04 2.92094832e-04 -7.90165941e-05 -1.03529333e-03 3.86003614e-03 2.65925983e-03 -9.42493731e-04 -2.91984412e-03 -8.32679973e-04 -6.22316380e-04 1.62830914e-03 1.41070038e-03 -1.05310581e-03 -4.29132691e-04 -3.38748004e-03 -2.14482704e-03 2.66522495e-03 -1.70672731e-03 2.21871235e-03 7.67852471e-04 -4.05522675e-04 3.69134732e-03 -2.68788106e-04 8.00681883e-04 1.98179367e-03 -1.21154217e-03 -7.56838883e-04 -9.01334104e-04 -2.56626052e-03 4.35368915e-04 7.19753269e-04 -2.40792311e-03 7.30484782e-04 -1.04375300e-04 1.82642520e-03 -1.83782264e-04 -2.16018991e-03 -1.67128816e-03 -3.14951874e-03 -1.74462073e-03 -3.66404653e-04 1.16418314e-03 2.36262940e-03 -7.21087854e-04 2.59639206e-03 -1.85696199e-03 7.52747059e-04 -1.90908764e-03 -2.16792268e-03 -2.83251936e-03 -1.03030400e-03 3.27490713e-03 4.00006247e-04 3.08081927e-03 -1.79204450e-03 1.68617186e-03 9.10512696e-04 1.23125815e-03 -1.02122920e-03 4.01859492e-04 -3.32432962e-03 9.13784548e-04 -2.05583894e-03 -2.35229125e-03 -8.21198220e-04 -6.70439913e-04 -1.70158059e-03 3.93540040e-03 1.72487774e-03 1.93191075e-03 2.05451762e-03 3.47349187e-03 -2.65299017e-03 -3.04736476e-03] è¾“å…¥ï¼š 12#å¯ä»¥ç”¨å¥å‘é‡æ¨¡åž‹ç›´æŽ¥æ ¹æ®è¯å‘é‡æŸ¥è¯¢ç›¸ä¼¼åº¦print (model_dm.wv.most_similar(&apos;ç’Žçž&apos;)) è¾“å‡ºè·Ÿâ€œç’Žçžâ€æœ€ç›¸å…³çš„å‰10ä¸ªè¯ï¼Œä»¥åŠç›¸å…³ç³»æ•°ï¼š 12345678910[(&apos;åº†é”¡&apos;, 0.36982783675193787), (&apos;ç­”åº”&apos;, 0.30098065733909607), (&apos;å¼˜åŽ†&apos;, 0.29272472858428955), (&apos;è´µå¦ƒ&apos;, 0.28584328293800354), (&apos;å‘çŽ°&apos;, 0.2655611038208008), (&apos;è®¤å®š&apos;, 0.25713881850242615), (&apos;ä¸æ˜¯&apos;, 0.2567000389099121), (&apos;å¤šå¤š&apos;, 0.24867823719978333), (&apos;åˆ&apos;, 0.2475070059299469), (&apos;åˆ©ç”¨&apos;, 0.2474854439496994)] çœ‹å®Œä¸Žâ€œç’Žçžâ€å¼ºç›¸å…³çš„è¯åŽï¼Œä¹Ÿå¯ä»¥å°è¯•çœ‹çœ‹â€œå‚…æ’â€ï¼Œâ€œçš‡ä¸Šâ€çš„ç›¸å…³è¯ã€‚ï¼ˆè¿˜æ˜¯å¿ƒç–¼å‚…æ’ðŸ˜­ï¼‰ â€œå‚…æ’â€çš„è¾“å‡ºç»“æžœæ˜¯ï¼š 12345678910[(&apos;è°£è¨€&apos;, 0.25737541913986206), (&apos;ç¦»å¼€&apos;, 0.24646247923374176), (&apos;æ¶‚&apos;, 0.23385187983512878), (&apos;åªšæƒ‘&apos;, 0.2333744615316391), (&apos;æ˜¯&apos;, 0.2153894603252411), (&apos;å€Ÿ&apos;, 0.20425426959991455), (&apos;å´&apos;, 0.20283949375152588), (&apos;ç’Žçž&apos;, 0.20118796825408936), (&apos;è´µäºº&apos;, 0.19429181516170502), (&apos;å…¬é“&apos;, 0.1942289024591446)] è¯­æ–™åº“æ˜¯å‰20é›†çš„å†…å®¹ï¼Œå¯ä»¥çœ‹ä¸‹åœ¨å‰20é›†â€œå‚…æ’â€ä¸Žâ€œç’Žçžâ€çš„ç›¸å…³ç¨‹åº¦ï¼Œè¾“å…¥ï¼š 1print(model_dm.similarity(&apos;ç’Žçž&apos;, &apos;å‚…æ’&apos;)) è¾“å‡ºï¼š 10.23082738 åŒç†è¾“å…¥çœ‹çœ‹ä¸Žå¯Œå¯Ÿçš‡åŽå’Œå¤§çŒªè¹„å­çš„ç¼˜åˆ†ï¼š 12print(model_dm.similarity(&apos;ç’Žçž&apos;, &apos;çš‡åŽ&apos;))print(model_dm.similarity(&apos;ç’Žçž&apos;, &apos;çš‡ä¸Š&apos;)) æŸ¥è¯¢å­—å…¸çš„æ ·å­ï¼Œè¾“å…¥ï¼š 1print(model_dm.wv.vocab) è¾“å‡ºï¼š 1234567891011121314151617181920212223242526272829303132&#123;...&apos;å‚…æ’æ‰€è¨€&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f309e8&gt;, &apos;ç¥¥ç‘ž&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f5b438&gt;, &apos;æ’ç­‰&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30a58&gt;, &apos;çæƒœ&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30a90&gt;, &apos;å‡ºæ˜¯&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f6eb70&gt;, &apos;ç½®ä¹‹ä¸ç†&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f5ba90&gt;, &apos;ç«‹åˆ»&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30b00&gt;, &apos;è‡ªä»Ž&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30b38&gt;, &apos;é‡Œé¢&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30ba8&gt;, &apos;å‘æŠ–&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30be0&gt;, &apos;ä¹‹å·…&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30c18&gt;, &apos;æ¬&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30c50&gt;, &apos;å¯¹å°”æ™´&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f5e470&gt;, &apos;å¿ƒæƒŠ&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f5e5f8&gt;, &apos;æˆ‘è¡Œæˆ‘ç´ &apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f6afd0&gt;, &apos;é€”ä¸­&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30cf8&gt;, &apos;ä¸‰ä¸ª&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30d30&gt;, &apos;åŽŸæ¥&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30d68&gt;, &apos;ä¿æŠ¤&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30da0&gt;, &apos;é‚£å°”å¸ƒ&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x1838cf9b5c0&gt;, &apos;è¿‘èº«&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30e48&gt;, &apos;ååŒ»&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x1838a970908&gt;, &apos;å«ç»™&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30eb8&gt;, &apos;ç››ä¸–&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30ef0&gt;, &apos;å è½&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30f28&gt;, &apos;æ·¤ç§¯&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f74eb8&gt;, &apos;éšæ‚£&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f4b470&gt;, &apos;è¿›&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f30f98&gt;, &apos;ä¾å¥‰&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f54048&gt;, &apos;è¿¥å¼‚&apos;: &lt;gensim.models.keyedvectors.Vocab at 0x18391f33080&gt;, ...&#125; æŸ¥è¯¢å­—å…¸å¤§å°ï¼š 1print(len(model_dm.wv.vocab)) åŒæ ·ï¼Œä¹Ÿå¯ä»¥æŠŠåŽé¢çš„å‰§æƒ…åŠ è¿›åŽ»ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ðŸ˜ å…¶ä½™æ“ä½œå‚è€ƒé“¾æŽ¥ï¼š models.doc2vec models.word2vec Doc2Vec Tutorial on the Lee Dataset]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>word2vec</tag>
        <tag>word embedding</tag>
        <tag>doc2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLPç¬”è®° - Word Embedding // bag of words]]></title>
    <url>%2F2018%2F08%2F24%2FNLP%E7%AC%94%E8%AE%B0-Word-Embedding%2F</url>
    <content type="text"><![CDATA[é¢å‘è¯»è€…ï¼šnlpå…¥é—¨å­¦è€…ï¼Œpythoné€‰æ‰‹ å¯èƒ½è¿˜æ²¡åšè¿‡nlpçš„é¡¹ç›®ï¼Œå°±å¯¹ word embeddingï¼ˆè¯åµŒå…¥ï¼‰æœ‰æ‰€è€³é—»ã€‚æ·±åº¦å­¦ä¹ ä¸ºä»€ä¹ˆé‚£ä¹ˆç«ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯ä¸ç”¨æ€Žä¹ˆæ“å¿ƒå‰æœŸæ•°æ®æ¸…æ´—ã€‚åœ¨ï¼ˆæ·±åº¦ï¼‰è¯­ä¹‰åŒ¹é…é‡Œï¼Œè¿›è¡Œembeddingï¼ˆåµŒå…¥ï¼‰æ˜¯è¿›è¡Œæ·±åº¦å­¦ä¹ çš„å‰ä¸€æ­¥ã€‚ æ¦‚å¿µè§£é‡Š è¯­ä¹‰åŒ¹é…ï¼ˆsemantic matchingï¼‰ï¼šæ ¹æ®è¯­ä¹‰æ¥åŒ¹é…ï¼Œçœ‹ä¸¤å¥è¯ï¼ˆæˆ–è€…å¤šå¥è¯ï¼‰è¯´çš„æ˜¯ä¸æ˜¯ä¸€ä¸ªæ„æ€ã€‚æ¯”å¦‚â€œæˆ‘æƒ³å…¥é—¨nlpã€‚â€å’Œâ€œå¦‚ä½•å­¦nlpæŠ€æœ¯ï¼Ÿâ€å¯ä»¥è®¤ä¸ºæ˜¯åŒä¸€ä¸ªæ„æ€ï¼Œé‚£ä¹ˆè¿™ä¸¤å¥è¯å°±åŒ¹é…æˆåŠŸã€‚ä¼ ç»Ÿçš„æ–¹æ³•åªæ˜¯å­—å­—åŒ¹é…ï¼ˆterm matchingï¼‰ï¼Œä¸ä¼šå°†â€œå…¥é—¨â€å’Œâ€œå­¦ä¹ â€è¿™ä¸¤ä¸ªåŒ¹é…èµ·æ¥ã€‚å†åŠ ä¸€å¥â€œnlpçš„æ·±åº¦æ¨¡åž‹æœ‰å“ªäº›ï¼Ÿâ€ï¼Œæ˜Žæ˜¾å’Œå‰ä¸¤å¥ä¸æ˜¯ä¸€ä¸ªæ„æ€ï¼Œé‚£ä¹ˆå°±åŒ¹é…å¤±è´¥ã€‚è¯­ä¹‰åŒ¹é…ç»å¸¸ç”¨åœ¨æœç´¢å¼•æ“Žæˆ–åƒçŸ¥ä¹Žé—®ç­”ä¸Šï¼Œä½ æé—®â€œå¦‚ä½•å­¦nlpæŠ€æœ¯ï¼Ÿâ€ï¼Œè€Œâ€œæˆ‘æƒ³å…¥é—¨nlpã€‚â€è¿™ä¸ªå·²ç»æœ‰äººå›žç­”è¿‡äº†ï¼Œå­˜åœ¨çŸ¥è¯†åº“é‡Œï¼Œæœºå™¨éœ€è¦åšçš„å°±æ˜¯æŠŠä½ çš„é—®é¢˜ä¸Žå·²æœ‰ç­”æ¡ˆçš„é—®é¢˜åŒ¹é…èµ·æ¥ï¼ŒæŠŠå¯¹åº”çš„ç­”æ¡ˆä¼ é€ç»™ä½ ã€‚ å­—å…¸ï¼ˆdictionaryï¼‰ï¼šåƒæ–°åŽå­—å…¸ä¸€æ ·çš„å­˜åœ¨ï¼Œæœºå™¨ä¹Ÿéœ€è¦æœ‰ä¸€ä¸ªå­—å…¸æ¥ç†è§£æ–‡å­—ã€‚ä¸€ä¸ªå•è¯å¯¹åº”ä¸€ä¸ªç´¢å¼•ï¼Œè¿™ä¸ªç´¢å¼•indexå¾€å¾€æ˜¯ä¸€ä¸ªåºåˆ—æ•´æ•°ã€‚ è¯­æ–™åº“ï¼ˆcorporaï¼‰ï¼šå­—å…¸æ˜¯å¦‚ä½•æ¥çš„ï¼Œè‡ªç„¶æ˜¯å› ä¸ºæœ‰å¾ˆå¤šå¾ˆå¤šçš„æ–‡å­—ææ–™ã€‚è¯­æ–™å¯ä»¥æ˜¯æ‰€æœ‰èŽŽå£«æ¯”äºšå†™çš„æ–‡ç« ï¼Œæˆ–è€…æ‰€æœ‰ç»´åŸºç™¾ç§‘çš„æ–‡ç« ï¼Œæˆ–è€…ä¸€ä¸ªç‰¹å®šçš„äººå‘çš„æŽ¨æ–‡ã€‚ è¯/å¥/æ–‡æœ¬ åµŒå…¥ï¼ˆembeddingï¼‰ï¼šä¸è¦è¢«ä¸­æ–‡çš„â€œåµŒå…¥â€æ„æ€å¸¦åã€‚embeddingæ˜¯ä¸€ä¸ªæ•°å­¦æœ¯è¯­ï¼Œä»£è¡¨çš„æ˜¯ä¸€ä¸ªæ˜ å°„å…³ç³»ã€‚æ¯”å¦‚æ±‰è‹±å­—å…¸é‡Œçš„ä¸­æ–‡â€œé’žç¥¨â€æ˜ å°„åˆ°è‹±æ–‡å°±æ˜¯å•è¯â€œmoneyâ€ã€‚è¿™é¡¹æŠ€æœ¯æŠŠè¯æ±‡è¡¨ä¸­çš„å•è¯æˆ–çŸ­è¯­æ˜ å°„æˆç”±å®žæ•°æž„æˆçš„å‘é‡ã€‚åœ¨è®¡ç®—æœºä¸­ï¼Œä¸€ä¸ªå•è¯æ˜ å°„åˆ°çš„å¾€å¾€å°±æ˜¯å®ƒçš„ç´¢å¼•æ•°å­—ã€‚æ¯•ç«Ÿç›®å‰è®¡ç®—æœºä¹Ÿåªèƒ½ç†è§£æ•°å­—ã€‚ TF-IDFï¼ˆterm frequencyâ€“inverse document frequencyï¼‰ï¼šTFæ„æ€æ˜¯è¯é¢‘(Term Frequency)ï¼ŒIDFæ„æ€æ˜¯é€†æ–‡æœ¬é¢‘çŽ‡æŒ‡æ•°(Inverse Document Frequency)ã€‚TF-IDFæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨ä»¥è¯„ä¼°ä¸€å­—è¯å¯¹äºŽä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚å­—è¯çš„é‡è¦æ€§éšç€å®ƒåœ¨æ–‡ä»¶ä¸­å‡ºçŽ°çš„æ¬¡æ•°æˆæ­£æ¯”å¢žåŠ ï¼Œä½†åŒæ—¶ä¼šéšç€å®ƒåœ¨è¯­æ–™åº“ä¸­å‡ºçŽ°çš„é¢‘çŽ‡æˆåæ¯”ä¸‹é™ã€‚ è·‘ä¸ªå°ä¾‹å­åœ¨getting startedï¼Œæèµ·è¿‡gensimè¿™ä¸ªpythonåŒ…ã€‚æœ¬æ–‡å°±å…·ä½“è®²ä¸€ä¸‹è¿™ä¸ªåŒ…çš„ä½¿ç”¨æ–¹æ³•ã€‚é¦–å…ˆpip install gensimï¼Œç„¶åŽæ‰“å¼€python3ï¼Œå…¶å®ƒæ²¡ä¸‹è½½çš„åŒ…è¯·è‡ªå·±æ‰‹åŠ¨ä¸‹è½½ã€‚ï¼ˆjupyterç‰ˆæœ¬é“¾æŽ¥ï¼‰ è¾“å…¥ï¼š 12import logginglogging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO) 1234import osimport tempfileTEMP_FOLDER = tempfile.gettempdir()print(&apos;Folder &quot;&#123;&#125;&quot; will be used to save temporary dictionary and corpus.&apos;.format(TEMP_FOLDER)) ä¸‹é¢æ˜¯ä¸€ä¸ªè¿·ä½ çš„è¯­æ–™åº“ï¼Œç”±9ä¸ªå­—ç¬¦ä¸²æ–‡æœ¬ç»„æˆï¼Œæ¯ä¸ªå­—ç¬¦ä¸²åŒ…å«ä¸€ä¸ªå¥å­ã€‚è¯­æ–™æ˜¯æŒ‡ä¸€ç»„æ–‡æ¡£çš„é›†åˆã€‚è¿™ä¸ªé›†åˆæ˜¯gensimçš„è¾“å…¥ï¼Œgensimä¼šä»Žè¿™ä¸ªè¯­æ–™ä¸­æŽ¨æ–­å‡ºå®ƒçš„ç»“æž„ï¼Œä¸»é¢˜ç­‰ã€‚ä»Žè¯­æ–™ä¸­æŽ¨æ–­å‡ºçš„éšå«ç»“æž„ï¼Œå¯ä»¥ç”¨æ¥å¯¹ä¸€ä¸ªæ–°çš„æ–‡æ¡£æŒ‡å®šä¸€ä¸ªä¸»é¢˜ã€‚ è¯­æ–™åº“è¾“å…¥ï¼š 1234567891011from gensim import corporadocuments = [&quot;Human machine interface for lab abc computer applications&quot;, &quot;A survey of user opinion of computer system response time&quot;, &quot;The EPS user interface management system&quot;, &quot;System and human system engineering testing of EPS&quot;, &quot;Relation of user perceived response time to error measurement&quot;, &quot;The generation of random binary unordered trees&quot;, &quot;The intersection graph of paths in trees&quot;, &quot;Graph minors IV Widths of trees and well quasi ordering&quot;, &quot;Graph minors A survey&quot;] é¦–å…ˆï¼Œåšäº›é¢„å¤„ç†ã€‚ æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼ˆtokenizationï¼‰ åˆ åŽ»ä¸€äº›å¸¸ç”¨è¯/åœç”¨è¯ï¼ˆåƒfor/ a/ of/ the/â€¦è¿™äº›è¯ï¼‰ åˆ åŽ»åªå‡ºçŽ°ä¸€æ¬¡çš„è¯ï¼ˆé˜²æ­¢å¤ªç¨€ç–ï¼‰ è¾“å…¥ï¼š 12345678910111213141516# remove common words and tokenizestoplist = set(&apos;for a of the and to in&apos;.split())texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]# remove words that appear only oncefrom collections import defaultdictfrequency = defaultdict(int)for text in texts: for token in text: frequency[token] += 1texts = [[token for token in text if frequency[token] &gt; 1] for text in texts]from pprint import pprint # pretty-printerpprint(texts) è¾“å‡ºï¼š 123456789[[&apos;human&apos;, &apos;interface&apos;, &apos;computer&apos;], [&apos;survey&apos;, &apos;user&apos;, &apos;computer&apos;, &apos;system&apos;, &apos;response&apos;, &apos;time&apos;], [&apos;eps&apos;, &apos;user&apos;, &apos;interface&apos;, &apos;system&apos;], [&apos;system&apos;, &apos;human&apos;, &apos;system&apos;, &apos;eps&apos;], [&apos;user&apos;, &apos;response&apos;, &apos;time&apos;], [&apos;trees&apos;], [&apos;graph&apos;, &apos;trees&apos;], [&apos;graph&apos;, &apos;minors&apos;, &apos;trees&apos;], [&apos;graph&apos;, &apos;minors&apos;, &apos;survey&apos;]] é¢„å¤„ç†çš„æ–¹å¼å¯ä»¥åƒå˜ä¸‡åŒ–ï¼Œä¸Šé¢åªæ˜¯ä¸¾ä¸ªä¾‹å­ã€‚æŽ¥ä¸‹æ¥æ ¹æ®ä¸Šé¢å‰©ä¸‹çš„å•è¯ç”Ÿæˆå­—å…¸ï¼Œè¾“å…¥ï¼š 123dictionary = corpora.Dictionary(texts)dictionary.save(&apos;deerwester.dict&apos;) # store the dictionary, for future referenceprint(dictionary) è¾“å‡ºï¼š 1Dictionary(12 unique tokens: [&apos;human&apos;, &apos;interface&apos;, &apos;computer&apos;, &apos;survey&apos;, &apos;user&apos;]...) å¯ä»¥çœ‹å‡ºè¯­æ–™åº“ç”Ÿæˆçš„å­—å…¸é‡Œæœ‰12ä¸ªä¸åŒçš„å•è¯ã€‚æ„å‘³ç€è¯­æ–™åº“çš„æ¯ä¸€ä¸ªæ–‡æœ¬ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸€å¥è¯ï¼Œéƒ½å¯ä»¥è¢«12ç»´çš„ç¨€ç–å‘é‡è¡¨ç¤ºã€‚ è¾“å…¥ï¼š 1print(dictionary.token2id) è¾“å‡ºå­—å…¸mappingï¼Œè¯­æ–™ä¸­çš„æ¯ä¸€ä¸ªå•è¯å…³è”ä¸€ä¸ªå”¯ä¸€çš„idã€‚å­—å…¸å•è¯ä¸Židèƒ½ä¸€ä¸€å¯¹åº”å°±è¡Œï¼Œä¸åŒçš„äººè·‘çš„idæ•°å­—å¯èƒ½å˜åŒ–ï¼š 1&#123;&apos;human&apos;: 0, &apos;interface&apos;: 1, &apos;computer&apos;: 2, &apos;survey&apos;: 3, &apos;user&apos;: 4, &apos;system&apos;: 5, &apos;response&apos;: 6, &apos;time&apos;: 7, &apos;eps&apos;: 8, &apos;trees&apos;: 9, &apos;graph&apos;: 10, &apos;minors&apos;: 11&#125; å¦‚æžœè¦å¯¹æ–‡æ¡£çš„éšå«ç»“æž„è¿›è¡ŒæŽ¨æ–­ï¼Œå°±éœ€è¦ä¸€ç§æ•°å­¦ä¸Šèƒ½å¤„ç†çš„æ–‡æ¡£è¡¨ç¤ºæ–¹æ³•ã€‚ä¸€ç§æ–¹æ³•æ˜¯æŠŠæ¯ä¸ªæ–‡æ¡£è¡¨è¾¾ä¸ºä¸€ä¸ªå‘é‡ã€‚æœ‰å¾ˆå¤šç§è¡¨ç¤ºæ–¹æ³•ï¼Œä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯bag-of-wordsæ¨¡åž‹ï¼Œä¹Ÿå«åšâ€œè¯è¢‹â€ã€‚åœ¨è¯è¢‹æ¨¡åž‹ä¸­ï¼Œæ¯ç¯‡æ–‡æ¡£ï¼ˆåœ¨è¿™é‡Œæ˜¯æ¯ä¸ªå­—ç¬¦ä¸²å¥å­ï¼‰è¢«è¡¨ç¤ºæˆä¸€ä¸ªå‘é‡ï¼Œä»£è¡¨å­—å…¸ä¸­æ¯ä¸ªè¯å‡ºçŽ°çš„æ¬¡æ•°ã€‚è¯è¢‹æ¨¡åž‹çš„ä¸€ä¸ªé‡è¦ç‰¹ç‚¹æ˜¯ï¼Œå®ƒå®Œå…¨å¿½ç•¥çš„å•è¯åœ¨å¥å­ä¸­å‡ºçŽ°çš„é¡ºåºï¼Œè¿™ä¹Ÿå°±æ˜¯â€œè¯è¢‹â€è¿™ä¸ªåå­—çš„ç”±æ¥ã€‚ è¯è¢‹ç¤ºä¾‹ï¼Œè¾“å…¥ï¼š 123new_doc = &quot;Human computer interaction&quot;new_vec = dictionary.doc2bow(new_doc.lower().split())print(new_vec) # the word &quot;interaction&quot; does not appear in the dictionary and is ignored è¾“å‡ºï¼š 1[(0, 1), (2, 1)] æ–°æ ·æœ¬æ˜¯ä¸€ä¸ªæ–°å¥å­ï¼ˆæ³¨æ„åˆ°è¿™å¥è¯å¹¶æ²¡æœ‰å‡ºçŽ°åœ¨åŽŸå§‹çš„é¢„æ–™ä¸­ï¼‰ï¼šâ€Human computer interactionâ€ doc2bow()å‡½æ•°ç”Ÿæˆçš„å…ƒç»„ä¸­ï¼Œæ‹¬å·å·¦è¾¹ä»£è¡¨å•è¯idï¼Œæ‹¬å·å³è¾¹ä»£è¡¨å•è¯åœ¨æ ·ä¾‹ä¸­çš„å‡ºçŽ°æ¬¡æ•°ã€‚ç”Ÿæˆçš„æ˜¯ä¸€ä¸ªåƒ[(word_id, word_count), â€¦]çš„ç¨€ç–å‘é‡ï¼Œä¹Ÿå°±æ˜¯è¯è¢‹ã€‚ â€œHumanâ€å’Œâ€œcomputerâ€æ˜¯å‡ºçŽ°åœ¨è¯­æ–™åº“çš„ï¼Œå› æ­¤ä¹Ÿå­˜åœ¨åœ¨å­—å…¸é‡Œï¼Œå…¶idåˆ†åˆ«æ˜¯0å’Œ2ï¼Œå„è‡ªåœ¨æ–°æ ·æœ¬é‡Œå‡ºçŽ°è¿‡ä¸€æ¬¡ï¼Œå› æ­¤å‡ºçŽ°é¢‘æ¬¡éƒ½æ˜¯1ã€‚å› æ­¤(0, 1), (2, 1)åˆ†åˆ«ä»£è¡¨â€œHumanâ€å’Œâ€œcomputerâ€ã€‚â€œinteractionâ€ä¸å­˜åœ¨å­—å…¸é‡Œï¼Œä¸åœ¨ç¨€ç–å‘é‡é‡Œå‡ºçŽ°ã€‚è€Œå…¶ä»–å­˜åœ¨åœ¨å­—å…¸é‡Œï¼Œå´åœ¨æ–°å¥å­ä¸­å‡ºçŽ°0æ¬¡çš„å•è¯ï¼Œä¹Ÿä¸æ˜¾ç¤ºåœ¨ç¨€ç–å‘é‡é‡Œã€‚ä¹Ÿå°±è¯´æ˜Žæ¯ä¸ªå°æ‹¬å·å³è¾¹çš„æ•°å­—ä¸ä¼šå°äºŽ1ã€‚ å› æ­¤è¿™ä¸ªæ–°å¥å­çš„12ç»´å‘é‡æœ€ç»ˆç»“æžœæ˜¯[(0, 1), (2, 1)]ã€‚å¦‚æžœä¸æƒ³å‡ºçŽ°é¢‘æ¬¡è¿™ä¸ªç‰¹å¾ï¼Œå¯ä»¥å°è¯•ä¸‹doc2idxè¿™ä¸ªå‡½æ•°ï¼ŒåŒæ—¶æŒ‰ç…§å•è¯åœ¨å¥å­ä¸­å‡ºçŽ°çš„é¡ºåºè¿›è¡Œidçš„æ˜¾ç¤ºã€‚ æŠŠè¯­æ–™åº“çš„å¥å­éƒ½è½¬æ¢æˆç¨€ç–å‘é‡ï¼Œè¾“å…¥ï¼š 123corpus = [dictionary.doc2bow(text) for text in texts]corpora.MmCorpus.serialize(&apos;deerwester.mm&apos;, corpus) # store to disk, for later useprint(corpus) è¾“å‡ºï¼š 123456789[(0, 1), (1, 1), (2, 1)][(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)][(1, 1), (4, 1), (5, 1), (8, 1)][(0, 1), (5, 2), (8, 1)][(4, 1), (6, 1), (7, 1)][(9, 1)][(9, 1), (10, 1)][(9, 1), (10, 1), (11, 1)][(3, 1), (10, 1), (11, 1)] è·‘ä¸ªå¤§ä¾‹å­ä¸Šä¸ªä¾‹å­çš„è¯­æ–™åº“æ˜¯éžå¸¸å°çš„æ–‡æœ¬ï¼Œä½†å®žé™…æƒ…å†µæ˜¯ï¼Œè¯­æ–™åº“é‡Œä¼šæœ‰ç™¾ä¸‡ä¸Šäº¿æ¡æ–‡æœ¬ï¼Œæƒ³æƒ³æ–°åŽå­—å…¸éƒ½é‚£ä¹ˆåŽšã€‚æŠŠè¯­æ–™å…¨éƒ¨å­˜åœ¨RAM ä¸å®žé™…ã€‚å‡è®¾æ–‡æœ¬æ”¾åœ¨ä¸€ä¸ªæ–‡ä»¶å¤¹é‡Œï¼Œä¸€è¡Œè¯ä¸€è¡Œè¯çš„å½¢å¼å­˜å‚¨ï¼Œgensimå°±å¯ä»¥å®žçŽ°ä¸€æ¬¡è¿”å›žä¸€ä¸ªå¥å­çš„ç¨€ç–å‘é‡ã€‚ æ‰€ä»¥å¤§ä¾‹å­çš„ç²¾åŽæ— éžæ˜¯ï¼Œä¸€æ¬¡è·‘ä¸€æ¡æ–‡æœ¬ã€‚ç‚¹å‡»è¿™é‡Œä¸‹è½½æ ·æœ¬â€™mycorpus.txtâ€™ è¾“å…¥ï¼š 123456789class MyCorpus(object): def __iter__(self): for line in open(&apos;mycorpus.txt&apos;): # assume there&apos;s one document per line, tokens separated by whitespace yield dictionary.doc2bow(line.lower().split()) corpus_memory_friendly = MyCorpus() # doesn&apos;t load the corpus into memory!print(corpus_memory_friendly) # &lt;__main__.MyCorpus object at 0x10d5690&gt; è¾“å…¥ï¼š 12for vector in corpus_memory_friendly: # load one vector into memory at a time print(vector) è¾“å‡ºï¼š 123456789[(0, 1), (1, 1), (2, 1)][(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)][(1, 1), (4, 1), (5, 1), (8, 1)][(0, 1), (5, 2), (8, 1)][(4, 1), (6, 1), (7, 1)][(9, 1)][(9, 1), (10, 1)][(9, 1), (10, 1), (11, 1)][(3, 1), (10, 1), (11, 1)] è™½ç„¶çœ‹èµ·æ¥ç»“æžœè·Ÿè·‘ä¸ªå°ä¾‹å­ä¸€æ ·ï¼Œä½†æ˜¯è¿™ä¸ªè·‘çš„è¿‡ç¨‹å¯¹å†…å­˜æ›´å‹å¥½ã€‚çŽ°åœ¨ä½ å¯ä»¥éšæ„æ‰©å……è¯­æ–™åº“ã€‚ æŽ¥ä¸‹æ¥ï¼Œç”Ÿæˆå­—å…¸ï¼Œä½†æ— éœ€ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰çš„æ–‡æœ¬åˆ°å†…å­˜é‡Œï¼Œè¾“å…¥ï¼š 1234567891011&gt;&gt;&gt; from six import iteritems&gt;&gt;&gt; # collect statistics about all tokens&gt;&gt;&gt; dictionary = corpora.Dictionary(line.lower().split() for line in open(&apos;mycorpus.txt&apos;))&gt;&gt;&gt; # remove stop words and words that appear only once&gt;&gt;&gt; stop_ids = [dictionary.token2id[stopword] for stopword in stoplist&gt;&gt;&gt; if stopword in dictionary.token2id]&gt;&gt;&gt; once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]&gt;&gt;&gt; dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once&gt;&gt;&gt; dictionary.compactify() # remove gaps in id sequence after words that were removed&gt;&gt;&gt; print(dictionary)Dictionary(12 unique tokens) TransformationçŽ°åœ¨å·²ç»å‘é‡åŒ–äº†è¯­æ–™ï¼ŒæŽ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨å„ç§å‘é‡è½¬æ¢transformationäº†ï¼ŒæŒ‡çš„æ˜¯æŠŠæ–‡æ¡£è½¬åŒ–æˆå¦ä¸€ä¸ªã€‚åœ¨gensimä¸­ï¼Œæ–‡æ¡£ç”¨å‘é‡æ¥è¡¨ç¤ºï¼Œæ‰€ä»¥æ¨¡åž‹å¯ä»¥è®¤ä¸ºæ˜¯åœ¨ä¸¤ä¸ªå‘é‡ç©ºé—´è¿›è¡Œè½¬æ¢ã€‚è¿™ä¸ªè½¬æ¢æ˜¯ä»Žè¯­æ–™è®­ç»ƒé›†ä¸­å­¦ä¹ å‡ºæ¥çš„ã€‚ æ¯”è¾ƒç®€å•çš„ä¸€ä¸ªå«TF-IDFã€‚TF-IDFæŠŠè¯è¢‹è¡¨è¾¾çš„å‘é‡è½¬æ¢åˆ°å¦ä¸€ä¸ªå‘é‡ç©ºé—´ï¼Œè¿™ä¸ªå‘é‡ç©ºé—´ä¸­ï¼Œè¯é¢‘æ˜¯æ ¹æ®è¯­æ–™ä¸­æ¯ä¸ªè¯çš„ç›¸å¯¹ç¨€æœ‰ç¨‹åº¦ï¼ˆrelative rarityï¼‰è¿›è¡ŒåŠ æƒå¤„ç†çš„ã€‚ çœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚é¦–å…ˆåˆå§‹åŒ–ä¸€ä¸ªtf-idfï¼Œåœ¨æˆ‘ä»¬çš„è¯­æ–™ä¸­è¿›è¡Œè®­ç»ƒï¼Œç„¶åŽå¯¹â€œsystem minorsâ€è¿›è¡Œå¤„ç†ã€‚ï¼ˆå‚è€ƒï¼‰ è¾“å…¥ï¼š 1234567from gensim import modelstfidf = models.TfidfModel(bow_corpus)string = &quot;system minors&quot;string_bow = dictionary.doc2bow(string.lower().split())string_tfidf = tfidf[string_bow]print(string_bow)print(string_tfidf) è¾“å‡ºï¼š 12[(5, 1), (11, 1)][(5, 0.5898341626740045), (11, 0.8075244024440723)] TF-IDFè¿”å›žäº†ä¸€ç»„å…ƒç»„ã€‚å…ƒç»„ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ è¡¨ç¤ºidï¼Œç¬¬äºŒä¸ªè¡¨ç¤ºtf-idfæƒé‡ã€‚æ³¨æ„åˆ°ï¼Œâ€œsystemâ€åœ¨åŽŸè¯­æ–™ä¸­å‡ºçŽ°4æ¬¡ï¼Œâ€œminorsâ€å‡ºçŽ°2æ¬¡ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªæƒé‡æ¯”ç¬¬äºŒä¸ªå°ã€‚ å…¶å®ƒçš„è¿˜æœ‰ä¸‹é¢å‡ ä¸ªè½¬æ¢ï¼Œå…·ä½“è½¬æ¢ä»£ç ç‚¹è¿™é‡Œï¼š Latent Semantic Indexing, LSI (or sometimes LSA) Random Projections, RP Latent Dirichlet Allocation, LDA Hierarchical Dirichlet Process, HDP å†™åœ¨æœ€åŽWord Embeddingç›¸å…³çš„æœ‰å¾ˆå¤šæŠ€æœ¯ï¼Œpensimé‡Œä¹Ÿæœ‰æ›´å¤šå¥½ç”¨çš„åŠŸèƒ½ï¼Œæ¯”å¦‚word2vecï¼Œdoc2vecç­‰ï¼Œè¿™é‡Œåªæ˜¯æŠ›ç –å¼•çŽ‰ï¼Œä¸¾ä¸ªå°ä¾‹å­ã€‚è·‘ä¸€éåŽï¼Œå¯¹è¿™ä¸ªè¯åµŒå…¥æŠ€æœ¯æœ‰ä¸ªå¤§æ¦‚çš„æ„Ÿå—å°±ç®—ç›®çš„è¾¾æˆäº†~ðŸ˜Ž]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>embedding</tag>
        <tag>gensim</tag>
        <tag>tf-idf</tag>
        <tag>corpora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLPç¬”è®° - Getting Started]]></title>
    <url>%2F2018%2F08%2F24%2FNLP%E7%AC%94%E8%AE%B0-Getting-Started%2F</url>
    <content type="text"><![CDATA[èƒŒæ™¯ï¼š Getting startedï¼Œå…¥é—¨æŒ‡å—ã€‚ NLPï¼Œnatural language processingï¼Œæ— éžæ˜¯å¯¹æ–‡æœ¬æ•°æ®åšå¤„ç†ï¼Œå¯åº”ç”¨äºŽæ™ºèƒ½å¯¹è¯ï¼ˆèŠå¤©æœºå™¨äººï¼Œä¾‹å¦‚ Siri/å°å†°ï¼‰ï¼Œæ™ºèƒ½é—®ç­”ï¼ˆæ™ºèƒ½å®¢æœï¼‰ï¼Œæœºå™¨ç¿»è¯‘ï¼Œæœç´¢å¼•æ“Žï¼ˆgoogleï¼‰ï¼Œç­‰ç­‰ã€‚æœ¬ç¯‡ä¸»è¦ä»‹ç»å…¥é—¨èµ„æ–™åŽ»å“ªé‡Œæ‰¾ï¼Œä»¥åŠå­¦ä¹ å†…å®¹çš„ä¼˜å…ˆçº§æŽ’åºã€‚ é¢å‘è¯»è€…ï¼š å¯¹nlpæ–¹å‘æ„Ÿå…´è¶£ï¼Œä»¥åšé¡¹ç›®ä¸ºå¯¼å‘çš„å­¦ä¹ è€… nlpé›¶åŸºç¡€ï¼Œå¸Œæœ›å¿«é€Ÿå…¥é—¨ pythoné€‰æ‰‹ æ¦‚å¿µè§£é‡Šå›žé¡¾ä¸€ä¸‹äººç±»æ˜¯å¦‚ä½•ç†è§£ä¸€æ®µæ–‡å­—çš„ï¼Œä¸­è‹±æ–‡çš„å¤„ç†æ–¹å¼ä¸åŒï¼Œä»¥è‹±æ–‡ä¸ºä¾‹ã€‚ä¸€æ®µè¯ä¼šè¢«æ‹†æˆä¸€ä¸ªä¸ªå¥å­ï¼Œä¸€ä¸ªå¥å­åˆä¼šè¢«æ‹†æˆä¸€ä¸ªä¸ªå•è¯ï¼Œæ ¹æ®å•è¯åœ¨å¥å­ä¸­çš„ä¸åŒä½ç½®ã€å•è¯çš„å•å¤æ•°ã€å•è¯çš„æ—¶æ€ç­‰æ¥ç†è§£ã€‚æ‰€ä»¥å¯¹æ–‡å­—è¿›è¡Œåˆ†æžçš„æ“ä½œå°±å¾ˆç®€å•æ˜Žäº†äº†ã€‚ï¼ˆå‚è€ƒé“¾æŽ¥ï¼‰ sentence segmentationï¼ˆæ–­å¥ï¼‰ ä¸€èˆ¬æ ¹æ®æ ‡ç‚¹ç¬¦å·å³å¯è¿›è¡Œæ–­å¥æ“ä½œã€‚ä»¥ä¸Šé¢çš„åŠ¨å›¾ä¸ºä¾‹ï¼Œå¯ä»¥åˆ†æˆå››ä¸ªå¥å­ã€‚ word tokenizationï¼ˆåˆ†è¯ï¼‰ ä½ å¯ä»¥å¾ˆå¿«çŸ¥é“â€œæˆ‘çˆ±é’žç¥¨ã€‚â€é‡Œâ€œæˆ‘â€æ˜¯ä¸€ä¸ªè¯ï¼Œâ€œçˆ±â€æ˜¯å¦å¤–ä¸€ä¸ªï¼Œâ€œé’žç¥¨â€æ˜¯å¦å¤–å¦å¤–ä¸€ä¸ªè¯ã€‚ä½†æ˜¯æœºå™¨ä¸çŸ¥é“ï¼Œæ‰€ä»¥è¦åšåˆ†è¯ã€‚ç›¸è¾ƒäºŽä¸­æ–‡ï¼Œè‹±æ–‡æ¯”è¾ƒå®¹æ˜“è¾¨è¯†è¯çš„å±žæ€§ã€‚è‹±æ–‡çš„å¥å­ç”±ä¸€ä¸ªä¸ªå•è¯ç»„æˆï¼Œå•è¯ä¹‹é—´ä»¥ç©ºæ ¼éš”å¼€ï¼Œå› æ­¤ç©ºæ ¼ä¹‹é—´å°±æ˜¯ä¸€ä¸ªå•è¯ã€‚ â€œLondon is the capital and most populous city of England and the United Kingdom.â€ ä¸Šé¢è¿™å¥è¯çš„åˆ†è¯ç»“æžœå¦‚ä¸‹ï¼ŒåŒ…å«æ ‡ç‚¹ç¬¦å·ï¼š â€œLondonâ€, â€œisâ€, â€œ theâ€, â€œcapitalâ€, â€œandâ€, â€œmostâ€, â€œpopulousâ€, â€œcityâ€, â€œofâ€, â€œEnglandâ€, â€œandâ€, â€œtheâ€, â€œUnitedâ€, â€œKingdomâ€, â€œ.â€ parts-of-speechï¼ˆè¯æ€§æ ‡æ³¨ï¼‰ åŒºåˆ†ä¸€ä¸ªå•è¯æ˜¯åŠ¨è¯/åè¯/å½¢å®¹è¯/å‰¯è¯ç­‰ã€‚ï¼ˆæƒ³èµ·æ›¾ç»è¢«è¯­æ³•æ”¯é…çš„ææƒ§ðŸ˜­ï¼‰è¿™ä¸ªè¯æ€§æ ‡æ³¨çš„å·¥ä½œå¯ä»¥æ ¹æ®ä¸€ä¸ªè¯æ€§åˆ†ç±»æ¨¡åž‹å¾—å‡ºã€‚ å¾—å‡ºè¿™å¥è¯ä¸­æœ‰åè¯ã€åŠ¨è¯ã€é™å®šè¯ã€è¿žè¯ã€å‰¯è¯ã€å½¢å®¹è¯ç­‰ã€‚ text lemmatizationï¼ˆæ–‡æœ¬è¯æ€§è¿˜åŽŸï¼‰ è™½è¯´è‹±è¯­æ˜¯æœ€ç®€å•çš„è¯­ä¹‰ï¼Œä½†æ˜¯ä¸åŒè¯æ€§çš„å•è¯çš„å˜è¡Œè¿˜æ˜¯å¾ˆå¤šçš„ï¼Œæ¯”å¦‚å•å¤æ•°ã€beåŠ¨è¯å˜å½¢ã€åŠ¨è¯æ˜¯çŽ°åœ¨è¿›è¡Œæ—¶è¿˜æ˜¯è¿‡åŽ»æ—¶ç­‰ï¼Œéƒ½è¿˜åŽŸæˆæœ€åˆçš„æ ·å­ã€‚ identifying stop-wordsï¼ˆè¯†åˆ«åœç”¨è¯ï¼‰ï¼š åƒ â€œandâ€, â€œtheâ€, â€œaâ€, â€œofâ€, â€œforâ€ è¿™ç§å“ªé‡Œéƒ½é«˜é¢‘å‡ºçŽ°ä¼šé€ æˆç»Ÿè®¡å™ªéŸ³çš„è¯ï¼Œè¢«ç§°ä¸ºstop wordsã€‚ä¸‹é¢ç°è‰²çš„â€œtheâ€, â€œandâ€, â€œmostâ€å‡ä¸ºåœç”¨è¯ï¼Œä¸€èˆ¬ä¼šè¢«ç›´æŽ¥è¿‡æ»¤æŽ‰ã€‚æ­£å¦‚ç»´åŸºæ‰€è¯´ï¼ŒçŽ°åœ¨è™½ç„¶åœç”¨è¯åˆ—è¡¨å¾ˆå¤šï¼Œä½†ä¸€å®šè¦æ ¹æ®å®žé™…æƒ…å†µè¿›è¡Œé…ç½®ã€‚æ¯”å¦‚è‹±è¯­çš„theï¼Œé€šå¸¸æƒ…å†µæ˜¯åœç”¨è¯ï¼Œä½†å¾ˆå¤šä¹é˜Ÿåå­—é‡Œæœ‰theè¿™ä¸ªè¯ï¼ŒThe Doors, The Whoï¼Œç”šè‡³æœ‰ä¸ªä¹é˜Ÿç›´æŽ¥å°±å«The Theï¼è¿™ä¸ªæ—¶å€™å°±ä¸èƒ½çœ‹åšæ˜¯åœç”¨è¯äº†ã€‚ dependency-parsingï¼ˆè§£æžä¾èµ–å…³ç³»ï¼‰ è§£æžå¥å­ä¸­æ¯ä¸ªè¯ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæœ€ç»ˆå»ºç«‹èµ·ä¸€ä¸ªå…³ç³»ä¾èµ–æ ‘ã€‚è¿™ä¸ªæ•°çš„rootæ˜¯å…³é”®åŠ¨è¯ï¼Œä»Žè¿™ä¸ªå…³é”®åŠ¨è¯å¼€å§‹ï¼ŒæŠŠæ•´ä¸ªå¥å­ä¸­çš„è¯éƒ½è”ç³»èµ·æ¥ã€‚ ä»Žè¿™ä¸ªå…³ç³»æ ‘æ¥çœ‹ï¼Œä¸»è¯­æ˜¯Londonï¼Œå®ƒå’Œcapitalè¢«beè”ç³»èµ·æ¥ã€‚ç„¶åŽè®¡ç®—æœºå°±çŸ¥é“ï¼ŒLondon is a capitalã€‚å¦‚æ­¤ç±»æŽ¨ï¼Œæˆ‘ä»¬çš„è®¡ç®—æœºå°±è¢«è®­ç»ƒçš„æŽŒæ¡è¶Šæ¥è¶Šå¤šçš„ä¿¡æ¯ã€‚ å¯ä»¥ç‚¹å‡»è¿™ä¸ªðŸ”—é“¾æŽ¥è‡ªå·±å°è¯•è¿™ä¸ªåŠŸèƒ½ named entity recognitionï¼ˆå‘½åå®žä½“è¯†åˆ«ï¼‰ æ¥ç»™åè¯æ‰“æ ‡ç­¾ã€‚æ¯”å¦‚æˆ‘ä»¬å¯ä»¥æŠŠç¬¬ä¸€å¥è¯å½“ä¸­çš„åœ°ç†åç§°è¯†åˆ«å‡ºæ¥ã€‚ å¯ä»¥é€šè¿‡è¿™ä¸ªçš„é“¾æŽ¥ï¼Œåœ¨çº¿ä½“éªŒä¸€ä¸‹ã€‚éšä¾¿å¤åˆ¶ç²˜è´´ä¸€æ®µè‹±æ–‡ï¼Œä»–ä¼šè‡ªåŠ¨è¯†åˆ«å‡ºé‡Œé¢åŒ…å«å“ªäº›ç±»åˆ«çš„åè¯ã€‚ conference resolutionï¼ˆå…±æŒ‡æ¶ˆè§£ï¼‰ æŒ‡ä»£è¯ï¼Œæ¯”å¦‚ä»–ï¼Œå®ƒï¼Œè¿™ä¸ªï¼Œé‚£ä¸ªï¼Œå‰è€…ï¼ŒåŽè€…ç­‰ã€‚å†æ¯”å¦‚ç¼©å†™ç®€ç§°ï¼ŒåŒ—äº¬å¤§å­¦é€šå¸¸ç§°ä¸ºåŒ—å¤§ï¼Œä¸­åŽäººæ°‘å…±å’Œå›½é€šå¸¸å°±å«ä¸­å›½ã€‚è¿™ç§çŽ°è±¡ï¼Œè¢«ç§°ä¸ºå…±æŒ‡çŽ°è±¡ã€‚ word embeddingï¼ˆè¯åµŒå…¥ï¼‰ï¼šé€šå¸¸æ˜¯æ·±åº¦å­¦ä¹ ç¬¬ä¸€æ­¥ï¼Œå°†æ–‡æœ¬è½¬æ¢æˆæ•°å­—å½¢å¼ï¼Œè¿™æ ·æ‰èƒ½ä¸¢è¿›åŽ»è®­ç»ƒã€‚å°†ä¸€å¥è¯å˜æˆä¸€ä¸ªå‘é‡ï¼Œæ¯ä¸ªå•è¯ä¸Žæ•°å­—ä¸€ä¸€å¯¹åº”ã€‚ word2vec GloVe sentiment analysisï¼ˆæƒ…æ„Ÿåˆ†æžï¼‰ï¼šåˆ¤æ–­ä¸€æ®µæ–‡å­—çš„æƒ…ç»ªã€‚æ¯”å¦‚æ·˜å®è¯„ä»·æ–‡å­—æ˜¯å–œæ¬¢è¿˜æ˜¯ä¸å–œæ¬¢è¿™ä¸ªå•†å“ï¼Œå½±è¯„æ–‡å­—æ˜¯çœ‹å¥½è¿˜æ˜¯ä¸çœ‹å¥½è¿™ä¸ªç”µå½±ã€‚ semantic retrievalï¼ˆè¯­ä¹‰å¬å›žï¼‰ï¼šæŠŠæ„æ€ç›¸åŒçš„ä¿¡æ¯ä»Žè¯­æ–™åº“/çŸ¥è¯†åº“ä¸­ç»Ÿç»Ÿæ‰¾å‡ºæ¥ã€‚ matchingï¼ˆåŒ¹é…ï¼‰ semantic matchingï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰ï¼šåˆ¤æ–­ä¸¤å¥è¯è¯´çš„æ˜¯ä¸æ˜¯ä¸€ä¸ªæ„æ€ã€‚æ¯”å¦‚åœ¨çŸ¥ä¹Žæé—®åŽï¼Œç³»ç»Ÿéœ€è¦æœç´¢å‡ºç›¸å…³é—®é¢˜çš„ç­”æ¡ˆæ¥æ˜¾ç¤ºã€‚ term matchingï¼šæ‰€è°“çš„ Ctrl+Fï¼ŒåªåŒ¹é…æ˜¯å¦æœ‰è¿™ä¸ªè¯ã€‚æ¯”å¦‚æœç´¢è¯æ˜¯taxiï¼Œé‚£ä¹ˆå°±ç®—æœ‰â€˜çš„å£«â€™çš„ä¿¡æ¯ä¹Ÿæœä¸å‡ºæ¥ã€‚ æ™ºèƒ½é—®ç­”æ¡†æž¶ä¸€è§ˆä»¥ç™¾åº¦çš„å¼€æºAnyQä¸ºä¾‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé—®ç­”ç³»ç»Ÿæ¡†æž¶ï¼š Question Analysisï¼šæ¥äº†ä¸€ä¸ªé—®é¢˜å…ˆè¿›è¡Œæ–‡å­—é¢„å¤„ç†ï¼Œçº æ­£é”™åˆ«å­—/å‘½åå®žä½“è¯†åˆ«/è¯æ€§æ ‡æ³¨/è¯åµŒå…¥ç­‰ã€‚ Retrievalï¼šå¯ç”¨æ·±åº¦å­¦ä¹ ç¥žç»ç½‘ç»œè¿›è¡Œè¯­ä¹‰å¬å›žï¼ŒæŠŠç›¸å…³çš„ä¿¡æ¯éƒ½æ‰¾å‡ºæ¥ã€‚ Matchingï¼šç›¸å…³ä¿¡æ¯ä¸ä¸€å®šæ˜¯æ­£ç¡®ç­”æ¡ˆï¼Œå¯ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œè¯­ä¹‰åŒ¹é…ï¼Œæ‰¾å‡ºæœ€åŒ¹é…çš„ç­”æ¡ˆã€‚ ä¼˜ç§€çš„å…¬å¼€è¯¾ Dan Jurafsky &amp; Chris Manning: Natural Language Processing [å…¥é—¨è§†é¢‘ç³»åˆ—] Stanford CS224d: Deep Learning for Natural Language Processing [æ–¯å¦ç¦ç³»åˆ—ï¼Œå¿…çœ‹] Stanford CS224n: Natural Language Processing with Deep Learning Stanford CS224n åœ¨bç«™ä¸Šçš„è§†é¢‘ Stanford CS224d åœ¨bç«™ä¸Šçš„è§†é¢‘ Coursera: Introduction to Natural Language Processing [å‡ºè‡ª University of Michigan] Awesome ç³»åˆ—awesome-nlp(website)[åŒ…å«ä¼˜ç§€çš„nlpæ•™ç¨‹/åº“/æŠ€æœ¯/å¼€æºæ•°æ®/æ¨¡åž‹ç­‰ï¼Œå¿…çœ‹!] é‡Œé¢çš„æ¯ä¸€ä¸ªé“¾æŽ¥éƒ½å€¼å¾—å¥½å¥½ç¿»çœ‹ç¿»çœ‹ã€‚é‡ç‚¹ä»‹ç»ä¸‹é¢çš„å‡ ä¸ªpythonåº“ï¼š spaCy (website, blog) [Python; emerging open-source library with fantastic usage examples, API documentation, and demo applications] è¿™ä¸ªåº“çš„é“¾æŽ¥åšå®¢å€¼å¾—çœ‹çœ‹ï¼Œå¯ä»¥åœ¨ä¸Šé¢çš„demo applicationä¸Šå†™è‡ªå·±çš„å¥å­æ„Ÿå—ä¸‹è¯­è¨€æ˜¯å¦‚ä½•å¤„ç†çš„ï¼Œä¹Ÿå¯ä»¥å°è¯•å…¶ä»–çš„demoå’Œexampleï¼Œç½‘ç«™è¿˜æ˜¯åšçš„å¾ˆç”¨å¿ƒçš„ã€‚ Natural Language Toolkit (nltk) (website, book) [Python; practical intro to programming for NLP, mainly used for teaching] gensim - Python library to conduct unsupervised semantic modelling from plain text ðŸ‘ è¿™ä¸ªåº“ç”¨æ¥åšè¯åµŒå…¥word embeddingï¼Œå°†æ–‡å­—è½¬æ¢ä¸ºæ•°å­—ï¼Œç”Ÿæˆå­—å…¸ã€‚ jieba - é€‚ç”¨äºŽä¸­æ–‡çš„åˆ†è¯å·¥å…· ä¼˜ç§€çš„åšå®¢å’Œèµ„æº Deep Learning for NLP resources Quora: How do I learn Natural Language Processing? Google Research blog Explosion AI blog 52nlp Twitter: #nlproc, list of NLPers (by Jason Baldrige) twitterä¹Ÿæ˜¯æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ çš„å‹å¥½å¤©åœ°ï¼Œå¾ˆå¤šposté…å›¾é…æ–‡éƒ½å¾ˆæœ‰æ„æ€ï¼Œå°¤å…¶æ˜¯åæ§½æ–‡ðŸ˜œ Reddit: /r/LanguageTechnology Medium: Nlp ä¼˜ç§€çš„ä¹¦ç±ä¸ªäººæ¯”è¾ƒåå‘äºŽå…ˆçœ‹è¯¾ä»¶ï¼Œæœ‰ç»†èŠ‚é—®é¢˜å†å›žåˆ°ä¹¦é‡ŒåŽ»æ‰¾ç­”æ¡ˆã€‚ Speech and Language Processing (Daniel Jurafsky and James H. Martin) [classic NLP textbook that covers all the basics, 3rd edition coming soon] Foundations of Statistical Natural Language Processing (Chris Manning and Hinrich SchÃ¼tze) [more advanced, statistical NLP methods] Introduction to Information Retrieval (Chris Manning, Prabhakar Raghavan and Hinrich SchÃ¼tze) [excellent reference on ranking/search] Neural Network Methods in Natural Language Processing (Yoav Goldberg) [deep intro to NN approaches to NLP, primer here] å¼€æºçš„æ•°æ®é›† A thorough list of publicly available NLP data sets[å¼€æºæ•°æ®å¤§å…¨ï¼Œåšé¡¹ç›®ä¸ç”¨æ„æ•°æ®äº†~] Quoraé—®é¢˜åŒ¹é…æ•°æ®é›†ä¸‹è½½é“¾æŽ¥ æ·±åº¦å­¦ä¹ ç›¸å…³æ¨¡åž‹è¯­ä¹‰åŒ¹é…çš„ç¥žç»ç½‘ç»œæ¨¡åž‹é›†åˆ è¯­ä¹‰åŒ¹é…çš„ç¥žç»ç½‘ç»œç›¸å…³æ¨¡åž‹ï¼š DSSM Siamese Network RNN RNNå˜ç§ï¼šLSTMã€Match-LSTMã€Seq-to-Seqã€Attentionæœºåˆ¶ GitHub Awesome-Chinese-NLP ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³èµ„æ–™ NLP-progress å„ç§è¯­è¨€çš„NLPé¡¹ç›® ç»ƒæ‰‹é¡¹ç›® kaggle - ç”µå½±è¯„è®ºçš„æƒ…æ„Ÿåˆ†æž kaggle - Quoraé—®é¢˜åŒ¹é… åŸºäºŽ spaCy çš„æ–­å¥/åˆ†è¯/åœç”¨è¯è¯†åˆ«ç­‰åŸºæœ¬æ“ä½œ å†™åœ¨æœ€åŽNLPæŠ€æœ¯çš„åº”ç”¨èŒƒå›´å¾ˆå¹¿æ³›ï¼Œå¯ä»¥æŠ“ä½å…¶ä¸­ä¸€ä¸ªç‚¹æ¥æ·±å…¥ã€‚æ ¹æ®è·‘ä¸Šé¢å‡ ä¸ªä¾‹å­ï¼Œè§‚å¯Ÿè®­ç»ƒæ•°æ®æ¥å¯¹è¿™ä¸ªå¤„ç†è¿‡ç¨‹æœ‰ä¸ªå¤§æ¦‚çš„ç†è§£ã€‚ç”±äºŽæŽ¥è§¦æ™ºèƒ½é—®ç­”é¡¹ç›®çš„ç¼˜æ•…ï¼ŒæŽ¥ä¸‹æ¥çš„ç¬”è®°æ–¹å‘ä¹Ÿæ˜¯è·Ÿæ™ºèƒ½é—®ç­”å¼ºç›¸å…³ã€‚]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>getting started</tag>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Auto Machine Learningç¬”è®° - Pipelines åˆ¶ä½œæ•™ç¨‹]]></title>
    <url>%2F2018%2F08%2F07%2FMachine%20Learning%E7%AC%94%E8%AE%B0%20-%20Pipelines%20%E5%88%B6%E4%BD%9C%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[é¢å‘è¯»è€…ï¼š æœºå™¨å­¦ä¹ èƒŒæ™¯å¯¹ AutoML æ„Ÿå…´è¶£ï¼Œç†Ÿæ‚‰å¹¶å–œæ¬¢ sklearn å‘çŽ°è‡ªå·±åœ¨ç›¸ä¼¼åˆ†æžä¸­åšç€é‡å¤çš„æ­¥éª¤ kaggle è¿›é˜¶è€… èƒŒæ™¯ï¼š æœ¬æ–‡æ˜¯ä»¥ä¸€ä¸ªæ–‡æœ¬æ•°æ®å¤„ç†çš„ä¾‹å­æ¥å±•ç¤ºpipelineå¦‚ä½•æŠŠå°åŠŸèƒ½ä¸²åœ¨ä¸€èµ·ï¼Œå®žçŽ°æµæ°´çº¿æ“ä½œã€‚ Once youâ€™ve gotten your feet wet in basic sklearn modeling, you might find yourself doing the same few steps over and over again in the same analysis. To get to the next level, pipelines are your friend! æœ‰äº›ä¸œè¥¿ä½ ä¸çŸ¥é“ï¼Œä»¥ä¸ºå®ƒä¸å­˜åœ¨ï¼›ä¸€æ—¦ä½ çŸ¥é“åŽï¼Œå‘çŽ°æ»¡ä¸–ç•Œéƒ½æ˜¯å®ƒã€‚pipelineå°±æ˜¯è¿™æ ·çš„ã€‚ æ¦‚å¿µè§£é‡Špipeline(ç®¡é“) é¡¾åæ€ä¹‰å°±æ˜¯æŠŠæ ‡å‡†çš„/å›ºæœ‰çš„å»ºæ¨¡è¿‡ç¨‹æµæ°´çº¿åŒ–ã€‚ å‡å¦‚ä½ æœ‰ä¸€å¥—é€šç”¨çš„æ•°æ®æ¸…æ´—æµç¨‹ï¼Œå°±å¯ä»¥å†™æˆä¸€ä¸ªpipelineï¼Œè¿™æ ·å°±ä¸ç”¨æ ¹æ®ä¸åŒçš„æ•°æ®ä¸€ééçš„é‡å¤å†™è¿™ä¸ªæ¸…æ´—æµç¨‹äº†ã€‚ pipelineæ˜¯ä¸€å—å—çš„å°é€»è¾‘çš„é›†æˆå‡½æ•°ï¼Œå°¤å…¶å½“æ¨¡åž‹ååˆ†å¤æ‚æ—¶ï¼Œä¾¿äºŽå›žå¤´æ£€æŸ¥æ¨¡åž‹é€»è¾‘ã€‚ pipelineæ˜¯ä¸€ä¸ªç±»ï¼Œä¸€èˆ¬ç»§æ‰¿sklearnçš„ BaseEstimatorï¼ŒTransformerMixinã€‚ æ‹¥æœ‰ fit/transform/predict ç­‰åŠŸèƒ½å’Œå±žæ€§ã€‚ ä¸‹è½½æ•°æ®é›†âœ”æ•°æ®é›†ä¸‹è½½é“¾æŽ¥ ç‚¹å‡»å›¾ç‰‡å³ä¸Šè§’çš„ â€˜Download All â€™ï¼Œå¹¶è§£åŽ‹æ•°æ®é›†ã€‚ æž„å»ºæœ¬åœ°æ–‡ä»¶ç»“æž„ï¼š 123456|-pipelines //æ–‡ä»¶å |- pipeline.py //æ–°å»ºpythonæ–‡ä»¶ |- data //åˆšæ‰ä¸‹è½½ä¸”è§£åŽ‹çš„æ•°æ®é›† |- train.csv //è®­ç»ƒé›† |- test.csv //æµ‹è¯•é›† |- sample_submission.csv //æ¯”èµ›ç»“æžœæäº¤æ ·æœ¬ï¼Œæœ¬æ–‡ä¸­ç”¨ä¸åˆ° æ‰“å¼€pipeline.pyï¼Œè¾“å…¥ï¼š 123456789import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)df = pd.read_csv('data/train.csv')df.dropna(axis=0)df.set_index('id', inplace = True)df.head() è¾“å‡ºï¼š id text author id26305 This process, however, afforded me no means ofâ€¦ EAP id17569 It never once occurred to me that the fumblingâ€¦ HPL id11008 In his left hand was a gold snuff box, from whâ€¦ EAP id27763 How lovely is spring As we looked from Windsorâ€¦ MWS id12958 Finding nothing else, not even gold, the Superâ€¦ HPL å¯ä»¥çœ‹åˆ°æ•°æ®é›†æ˜¯æ–‡æœ¬ä¿¡æ¯ï¼Œ3åˆ—ï¼ŒåŒ…å«idï¼Œtextæ–‡æœ¬ï¼Œå’Œä½œè€…ã€‚è¿™ä¸ªæ¯”èµ›çš„åŽŸæ„æ˜¯ç»™å‡ºä¸€æ®µæ–‡å­—ï¼Œé¢„æµ‹æ˜¯å‡ºè‡ªå“ªä¸ªä½œå®¶ä¹‹æ‰‹ï¼Œæ¨¡åž‹ç”¨æ¥å­¦ä¹ ä½œå®¶çš„æ–‡é£Žã€‚ æ–‡æœ¬ç‰¹å¾é¢„å¤„ç†ä»¥ä¸‹ä¸ºé€‚ç”¨äºŽæ‰€æœ‰æ–‡æœ¬çš„æ•°æ®æ¸…æ´—æ“ä½œï¼š å°†æ–‡æœ¬ä¿¡æ¯åŽ»æ ‡ç‚¹ç¬¦å·ï¼Œä¸”å…¨éƒ¨ç”¨å°å†™å­—æ¯ è®¡ç®—æ–‡æœ¬é•¿åº¦ è®¡ç®—æ–‡æœ¬å­—æ•° è®¡ç®— éžåœç”¨è¯ å­—æ•° è®¡ç®— éžåœç”¨è¯å•è¯çš„ å¹³å‡é•¿åº¦ è®¡ç®—é€—å·æ•° å…ˆç”¨ä¼ ç»Ÿçš„ç»Ÿè®¡æ–¹å¼æ¥è¿›è¡Œæ•°æ®æ¸…æ´—ï¼Œè¾“å…¥ï¼š 1234567891011121314151617181920212223242526import refrom nltk.corpus import stopwordsstopWords = set(stopwords.words('english')) # å¯èƒ½éœ€è¦æ‰‹åŠ¨ä¸‹è½½ stopwords#creating a function to encapsulate preprocessing, to mkae it easy to replicate on submission datadef processing(df): #lowering and removing punctuation df['processed'] = df['text'].apply(lambda x: re.sub(r'[^\w\s]','', x.lower())) #numerical feature engineering #total length of sentence df['length'] = df['processed'].apply(lambda x: len(x)) #get number of words df['words'] = df['processed'].apply(lambda x: len(x.split(' '))) df['words_not_stopword'] = df['processed'].apply(lambda x: len([t for t in x.split(' ') if t not in stopWords])) #get the average word length df['avg_word_length'] = df['processed'].apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) if len([len(t) for t in x.split(' ') if t not in stopWords]) &gt; 0 else 0) #get the average word length df['commas'] = df['text'].apply(lambda x: x.count(',')) return(df)df = processing(df)df.head() è¾“å‡ºï¼š åˆ›å»º Pipelineæ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè¾“å…¥ï¼š 12345678from sklearn.model_selection import train_test_splitfeatures= [c for c in df.columns.values if c not in ['id','text','author']]numeric_features= [c for c in df.columns.values if c not in ['id','text','author','processed']]target = 'author'X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.33, random_state=42)X_train.head() è¾“å‡ºï¼š id processed length words words_not_stopword avg_word_length commas id19417 this panorama is indeed glorious and â€¦ 91 18 6 6.666667 1 id09522 there was a simple natural earnestness â€¦ 240 44 18 6.277778 4 id22732 who are you pray that i duc de lomelette â€¦ 387 74 38 5.552632 9 id10351 he had gone in the carriage to the nearest â€¦ 118 24 11 5.363636 0 id24580 there is no method in their proceedings â€¦ 71 13 5 7.000000 1 æŽ¥ä¸‹æ¥æ˜¯å…³é”®æ­¥éª¤ã€‚ æ ¹æ®ç‰¹å¾æ˜¯å¦ä¸ºæ•°å€¼åž‹ï¼Œåˆ›å»º ä¸¤ä¸ªselector transformers: TextSelectorï¼ŒNumberSelector selectorçš„ä½œç”¨ï¼šè¾“å…¥ä¸€ä¸ªcolumnï¼Œæ ¹æ®è¿™ä¸ªselector transformerï¼Œè¾“å‡ºå¾—åˆ°ä¸€ä¸ªæ–°column ç®€å•è¯´å°±æ˜¯ï¼Œåš data transformationï¼Œæ”¶é›†æƒ³è¦çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ text length è¾“å…¥ï¼š 1234567891011121314151617181920212223242526272829from sklearn.base import BaseEstimator, TransformerMixinclass TextSelector(BaseEstimator, TransformerMixin): """ Transformer to select a single column from the data frame to perform additional transformations on Use on text columns in the data """ def __init__(self, key): self.key = key def fit(self, X, y=None): return self def transform(self, X): return X[self.key] class NumberSelector(BaseEstimator, TransformerMixin): """ Transformer to select a single column from the data frame to perform additional transformations on Use on numeric columns in the data """ def __init__(self, key): self.key = key def fit(self, X, y=None): return self def transform(self, X): return X[[self.key]] å…ˆæ¥è¯•ä¸€ä¸‹ TextSelector å¥½ä¸å¥½ç”¨ã€‚ç”±å°å˜å¤§ï¼Œå…ˆåˆ›å»ºä¸€ä¸ªmini pipelineï¼Œä½œç”¨æ˜¯å…ˆä»Žæ•°æ®é›†ä¸­æŠ“å–ä¸€åˆ—æ•°æ®ï¼Œå†åštf-idfå¤„ç†å¹¶è¿”å›žç»“æžœã€‚ åˆ›å»ºè¿‡ç¨‹åªéœ€ä¼ é€’ä¸€ä¸ªæ ¼å¼å¦‚ï¼ˆåç§°ï¼Œå¯¹è±¡ï¼‰çš„å…ƒç»„ã€‚æ‹¬å·å·¦è¾¹æ˜¯åŠ¨ä½œçš„åç§°ï¼Œå³è¾¹å°±æ˜¯é€‰å–çš„åˆ—åã€‚æ‰€ä»¥è¿™ä¸ªmini pipelineå°±æ˜¯ä¸¤ä¸ªåŠ¨ä½œï¼Œselectingï¼ˆé€‰æ‹©ä¸€åˆ—ï¼‰å’Œtfidf-ingï¼ˆå¯¹è¿™åˆ—è¿›è¡Œtf-idfå¤„ç†ï¼‰ã€‚ æ‰§è¡Œpipelineçš„å‘½ä»¤ï¼Œå¯ä»¥è°ƒç”¨ text.fit() æ¥é€‚åº”è®­ç»ƒé›†ï¼Œtext.transform() æ¥åº”ç”¨äºŽè®­ç»ƒé›†ï¼Œæˆ–è€…text.fit_transform() æ¥æ‰§è¡Œä¸¤è€…ã€‚ ç”±äºŽå®ƒæ˜¯ä¸€ä¸ªæ–‡æœ¬ï¼Œå®ƒå°†è¿”å›žä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œè¾“å…¥ï¼š 123456789from sklearn.pipeline import Pipelinefrom sklearn.feature_extraction.text import TfidfVectorizertext = Pipeline([ ('selector', TextSelector(key='processed')), ('tfidf', TfidfVectorizer( stop_words='english')) ])text.fit_transform(X_train) è¾“å‡ºï¼š æŽ¥ä¸‹æ¥è¯•ä¸€ä¸‹ NumberSelector å¯¹äºŽæ•°å€¼åž‹çš„ç‰¹å¾å¤„ç†å¥½ä¸å¥½ç”¨ï¼ŒåŒæ ·ä¹Ÿå…ˆå»ºç«‹ä¸€ä¸ªmini pipelineæ¥è§‚å¯Ÿæ•ˆæžœã€‚ è¿™ä¸ªpipelineæ“ä½œå°±å®šä¸ºç®€å•çš„scalerï¼Œä¸€åˆ—åˆ—çš„è¿›è¡Œæ•°å€¼çš„StandardScalerã€‚å…ˆä»¥ lengthåˆ—ä¸ºä¾‹ï¼Œä»ç„¶æ˜¯ä¸¤ä¸ªæ­¥éª¤ï¼Œå…ˆé€‰åˆ—ï¼Œå³lengthåˆ—ï¼Œå†åšæ•°å€¼StandardScalerã€‚ï¼ˆStandardScaleræ˜¯æ•°æ®é¢„å¤„ç†çš„ä¸€ä¸ªå¸¸è§çš„æ•°å€¼ç¼©æ”¾æ“ä½œã€‚ï¼‰ è¾“å…¥ï¼š 12345678from sklearn.preprocessing import StandardScalerlength = Pipeline([ ('selector', NumberSelector(key='length')), ('standard', StandardScaler()) ])length.fit_transform(X_train) è¾“å‡ºï¼š æ ¹æ®è¾“å‡ºç»“æžœå¯ä»¥çœ‹å‡ºï¼Œpipelineè¿”å›žä¸€ä¸ªæˆ‘ä»¬æƒ³è¦çš„æ•°å€¼ç¼©æ”¾çŸ©é˜µã€‚ç„¶åŽæŠŠå‰©ä¸‹çš„æ•°å€¼ç‰¹å¾åˆ—éƒ½è¿›è¡Œç¼©æ”¾scaleræ“ä½œã€‚å½“ç„¶è¿™ä¸ªæ•°æ®å¤„ç†æ“ä½œä½ å¯ä»¥éšæ„æ›´æ”¹æˆå…¶ä»–å¯ç”¨çš„ã€‚ è¾“å…¥ï¼š 12345678910111213141516words = Pipeline([ ('selector', NumberSelector(key='words')), ('standard', StandardScaler()) ])words_not_stopword = Pipeline([ ('selector', NumberSelector(key='words_not_stopword')), ('standard', StandardScaler()) ])avg_word_length = Pipeline([ ('selector', NumberSelector(key='avg_word_length')), ('standard', StandardScaler()) ])commas = Pipeline([ ('selector', NumberSelector(key='commas')), ('standard', StandardScaler()), ]) åˆ›å»º FeatureUnionpipelineç®¡é“å¯å¤§å¯å°ï¼Œåˆå¤§åˆé•¿åˆç²—çš„pipelineä¹Ÿæ˜¯ç”±ä¸€ä¸ªä¸ªmini pipelinesç»„æˆçš„å˜›ã€‚ æŽ¥ä¸‹æ¥ä½¿ç”¨FeatureUnionæ¥è¿žæŽ¥ä¸Šé¢åšå¥½çš„pipelinesï¼Œå½¢æˆä¸€ä¸ªç±»ä¼¼å¤§çš„pipelineã€‚ è¯­æ³•æ“ä½œè¿˜æ˜¯æ ¼å¼å¦‚ï¼ˆåç§°ï¼Œå¯¹è±¡ï¼‰çš„å…ƒç»„ã€‚FeatureUnionæœ¬èº«ä¸æ˜¯pipelineï¼Œå®ƒåªæ˜¯ä¸€ä¸ªç»„åˆï¼Œæ‰€ä»¥éœ€è¦å¤šå†™ä¸€è¡Œä»£ç ï¼Œå°†å…¶å˜ä¸ºä¸€ä¸ªå¤§pipelineã€‚ç„¶åŽçš„äº‹æƒ…ï¼Œä½ æ‡‚çš„ï¼Œè¿˜æ˜¯fitï¼Œtransformï¼Œæˆ–è€…fit_transformæ“ä½œã€‚ è¾“å…¥ï¼š 1234567891011from sklearn.pipeline import FeatureUnionfeats = FeatureUnion([('text', text), ('length', length), ('words', words), ('words_not_stopword', words_not_stopword), ('avg_word_length', avg_word_length), ('commas', commas)])feature_processing = Pipeline([('feats', feats)])feature_processing.fit_transform(X_train) è¾“å‡ºï¼š ç”šè‡³å¯ä»¥åœ¨åˆšåˆšçš„å¤§pipelineå°¾å·´ä¸Šå†æ·»åŠ ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå³ä¸ä»…ä»…æ˜¯æ•°æ®è½¬åŒ–ï¼Œè€Œæ˜¯å¢žåŠ å»ºæ¨¡/é¢„æµ‹åŠŸèƒ½ã€‚è¿˜æ˜¯åŽŸæ¥çš„å¥—è·¯ï¼Œå†™å…ƒç»„ï¼Œå†pipelineä¸€ä¸‹ã€‚ å¯ä»¥å¾—åˆ°ç²—ç³™çš„ 63.8%çš„åˆ†ç±»ç²¾åº¦ã€‚å°è¯•ç‰›åˆ€ï¼Œä¸è¦å¤ªåœ¨æ„è¿™äº›ç»†èŠ‚~ è¾“å…¥ï¼š 1234567891011from sklearn.ensemble import RandomForestClassifierpipeline = Pipeline([ ('features',feats), ('classifier', RandomForestClassifier(random_state = 42)),])pipeline.fit(X_train, y_train)preds = pipeline.predict(X_test)np.mean(preds == y_test) è¾“å‡ºï¼š 10.638347260909935 å†çœ‹ PipelineçŽ°åœ¨å¯ä»¥å¾—å‡ºçš„ç»“è®ºå°±æ˜¯ï¼Œpipelineä¸ä»…èƒ½åšæ•°æ®é¢„å¤„ç†çš„æµæ°´çº¿ï¼Œæ›´æ˜¯èƒ½æŠŠæ•´ä¸ªå»ºæ¨¡å¥—è·¯åšæˆæµæ°´çº¿ï¼Œåªéœ€åœ¨pipelineçš„ç»“å°¾åŠ ä¸Šä¸€ä¸ªåˆ†ç±»å™¨ã€‚æŽ¥ä¸‹æ¥å°†åˆ›å»ºä¸€ä¸ªpipelineï¼Œå®Œæˆä¸Šé¢æ‰€æœ‰çš„å¤„ç†ï¼Œæœ€åŽç”¨éšæœºæ£®æž—åˆ†ç±»å™¨ã€‚ ä¼˜åŒ– Pipelineåˆ©ç”¨ Cross Validation å¯»æ‰¾æ›´ä¼˜çš„pipelineï¼Œå°±è¦å…ˆè§‚å¯Ÿpipelineçš„å±žæ€§ï¼Œå†è¿›è¡Œè¶…å‚æ•°è°ƒå‚ã€‚ è¾“å…¥ï¼š 1pipeline.get_params().keys() è¾“å‡ºï¼š è¿™äº›éƒ½æ˜¯pipelineç›¸å…³çš„å±žæ€§ï¼Œå³è¶…å‚æ•°ï¼Œè¿™äº›è¶…å‚æ•°çš„ç»„åˆå˜åŒ–ï¼Œè¶…å‚æ•°çš„æ•°å€¼å˜åŒ–éƒ½ä¼šå½±å“ä¸€ä¸ªpipelineå¥½ä¸å¥½ç”¨ã€‚åœ¨æ­¤åªä¸ºå±•ç¤ºæ“ä½œï¼Œå› æ­¤éšå¿ƒæƒ…æŒ‘é€‰å››ä¸ªè¶…å‚æ•°è¿›è¡Œè°ƒä¼˜ã€‚ä¼˜åŒ–æ–¹å¼ä¸ºGridSearchCVï¼Œå³ ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯æ³•ï¼Œé€‚ç”¨äºŽå°‘é‡çš„è¶…å‚æ•°ä¸ªæ•°å’Œå°‘é‡çš„æ•°å€¼å€™é€‰è°ƒä¼˜ã€‚ è¾“å…¥ï¼š 1234567891011from sklearn.model_selection import GridSearchCVhyperparameters = &#123; 'features__text__tfidf__max_df': [0.9, 0.95], 'features__text__tfidf__ngram_range': [(1,1), (1,2)], 'classifier__max_depth': [50, 70], 'classifier__min_samples_leaf': [1,2] &#125;clf = GridSearchCV(pipeline, hyperparameters, cv=5) # Fit and tune modelclf.fit(X_train, y_train) è¾“å‡ºï¼š è§‚å¯Ÿè°ƒä¼˜ç»“æžœï¼Œå³è¶…å‚æ•°æœ€ç»ˆé€‰æ‹©çš„æ•°å€¼ä¸ºå¤šå°‘ï¼Œè¾“å…¥ï¼š 1clf.best_params_ è¾“å‡ºï¼š éšè—èœå•æ“ä½œä¸ºè°ƒç”¨ refitï¼Œå¯è‡ªåŠ¨ä½¿ç”¨ä½¿ç”¨pipelineæ¥fitæ‰€æœ‰çš„è®­ç»ƒæ•°æ®ã€‚å¹¶å°†å…¶åº”ç”¨äºŽæµ‹è¯•é›†ã€‚ è¾“å…¥ï¼š 1234567#refitting on entire training data using best settingsclf.refitpreds = clf.predict(X_test)probs = clf.predict_proba(X_test)np.mean(preds == y_test) è¾“å‡ºï¼š 10.6425255338904364 è¿˜æ˜¯æœ‰ä¸€ç‚¹ç²¾åº¦çš„æé«˜çš„ã€‚ è¿›è¡Œé¢„æµ‹åšæ¨¡åž‹æ€»è¦æœ‰ç»“æžœçš„ï¼Œæœ€åŽå¯¹æ•°æ®é›†è¿›è¡Œpredictï¼Œçœ‹çœ‹æœªçŸ¥æ–‡æœ¬åˆ°åº•æ˜¯å“ªä½ä½œè€…å†™å‡ºæ¥çš„æ¦‚çŽ‡æ›´å¤§ã€‚ è¾“å…¥ï¼š 123456789101112submission = pd.read_csv('data/test.csv')#preprocessingsubmission = processing(submission)predictions = clf.predict_proba(submission)preds = pd.DataFrame(data=predictions, columns = clf.best_estimator_.named_steps['classifier'].classes_)#generating a submission fileresult = pd.concat([submission[['id']], preds], axis=1)result.set_index('id', inplace = True)result.head() è¾“å‡ºï¼š Pipeline æ€»ç»“ skleanæä¾›çš„pipelineæ¥å°†å¤šä¸ªå­¦ä¹ å™¨ç»„æˆæµæ°´çº¿ï¼Œé€šå¸¸æµæ°´çº¿çš„å½¢å¼ä¸ºï¼š å°†æ•°æ®æ ‡å‡†åŒ–çš„å­¦ä¹ å™¨â€”-ç‰¹å¾æå–çš„å­¦ä¹ å™¨â€”-æ‰§è¡Œé¢„æµ‹çš„å­¦ä¹ å™¨/åˆ†ç±»å™¨ é™¤äº†æœ€åŽä¸€ä¸ªå­¦ä¹ å™¨ä¹‹å¤–ï¼Œå‰é¢çš„æ‰€æœ‰å­¦ä¹ å™¨å¿…é¡»æä¾›transformæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”¨äºŽæ•°æ®è½¬ ï¼ˆä¾‹å¦‚ï¼š å½’ä¸€åŒ–ï¼Œæ­£åˆ™åŒ–ï¼Œä»¥åŠç‰¹å¾æå–ï¼‰ å‚è€ƒé“¾æŽ¥ A Deep Dive Into Sklearn Pipelines Work like a Pro with Pipelines and Feature Unions Pipelines + GridSearch = Awesome ML pipelines Using scikit-learn Pipelines and FeatureUnions ä¼˜ç§€çš„Transformersä¸ŽPipeline]]></content>
      <tags>
        <tag>pipeline</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Auto Machine Learningç¬”è®° - Bayesian Optimization]]></title>
    <url>%2F2018%2F07%2F31%2FAuto%20Hyperparameter%20Tuning%20-%20Bayesian%20Optimization%2F</url>
    <content type="text"><![CDATA[ä¼˜åŒ–å™¨æ˜¯æœºå™¨å­¦ä¹ ä¸­å¾ˆé‡è¦çš„ä¸€ä¸ªçŽ¯èŠ‚ã€‚å½“ç¡®å®šæŸå¤±å‡½æ•°æ—¶ï¼Œä½ éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ä½¿æŸå¤±å‡½æ•°çš„å‚æ•°èƒ½å¤Ÿå¿«é€Ÿæœ‰æ•ˆæ±‚è§£æˆåŠŸã€‚ä¼˜åŒ–å™¨å¾ˆå¤§ç¨‹åº¦å½±å“è®¡ç®—æ•ˆçŽ‡ã€‚è¶Šæ¥è¶Šå¤šçš„è¶…å‚æ•°è°ƒæ•´æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–æ–¹å¼å®Œæˆï¼Œä½¿ç”¨æ˜Žæ™ºçš„æœç´¢åœ¨æ›´çŸ­çš„æ—¶é—´å†…æ‰¾åˆ°æœ€ä½³è¶…å‚ç»„åˆï¼Œæ— éœ€åœ¨åˆå§‹è®¾ç½®ä¹‹å¤–è¿›è¡Œæ‰‹åŠ¨æ“ä½œã€‚è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰æ˜¯åŸºäºŽæ¨¡åž‹çš„è¶…å‚æ•°ä¼˜åŒ–ï¼Œå·²åº”ç”¨äºŽæœºå™¨å­¦ä¹ è¶…å‚æ•°è°ƒæ•´ï¼Œç»“æžœè¡¨æ˜Žè¯¥æ–¹æ³•å¯ä»¥åœ¨æµ‹è¯•é›†ä¸Šå®žçŽ°æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶æ¯”éšæœºæœç´¢éœ€è¦æ›´å°‘çš„è¿­ä»£ã€‚æ­¤å¤–ï¼ŒçŽ°åœ¨æœ‰è®¸å¤šPythonåº“å¯ä»¥ä¸ºä»»ä½•æœºå™¨å­¦ä¹ æ¨¡åž‹ç®€åŒ–å®žçŽ°è´å¶æ–¯è¶…å‚æ•°è°ƒæ•´ã€‚ 1. è¶…å‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ åœ¨æ¨¡åž‹å¼€å§‹å­¦ä¹ è¿‡ç¨‹ä¹‹å‰äººä¸ºè®¾ç½®å€¼çš„å‚æ•°ï¼Œè€Œä¸æ˜¯ï¼ˆåƒbiasã€weightsï¼‰é€šè¿‡è®­ç»ƒå¯å¾—åˆ°çš„å‚æ•°æ•°æ®ã€‚ è¿™äº›å‚æ•°å®šä¹‰å…³äºŽæ¨¡åž‹æ›´é«˜å±‚æ¬¡çš„æ¦‚å¿µï¼ˆæ¨¡åž‹å¤æ‚æ€§ã€å­¦ä¹ èƒ½åŠ›ç­‰ï¼‰ã€‚ æ¯”å¦‚è¯´éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­çš„å­¦ä¹ é€ŸçŽ‡/learning rateï¼Œå‡ºäºŽè®¡ç®—å¤æ‚åº¦å’Œç®—æ³•æ•ˆçŽ‡ç­‰ï¼Œæˆ‘ä»¬å¹¶ä¸èƒ½ä»Žæ•°æ®ä¸­ç›´æŽ¥å­¦ä¹ ä¸€ä¸ªæ¯”è¾ƒä¸é”™çš„å­¦ä¹ é€Ÿåº¦ã€‚ä½†å­¦ä¹ é€ŸçŽ‡å´åˆæ˜¯ååˆ†é‡è¦çš„ï¼Œè¾ƒå¤§çš„å­¦ä¹ é€ŸçŽ‡ä¸æ˜“ä»¤æ¨¡åž‹æ”¶æ•›åˆ°è¾ƒåˆé€‚çš„è¾ƒå°å€¼è§£ï¼Œè€Œè¾ƒå°çš„å­¦ä¹ é€ŸçŽ‡å´åˆå¸¸å¸¸ä»¤æ¨¡åž‹çš„è®­ç»ƒé€Ÿåº¦å¤§å¤§é™ä½Žã€‚å¯¹äºŽåƒå­¦ä¹ é€ŸçŽ‡è¿™æ ·çš„è¶…å‚æ•°ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦åœ¨è®­ç»ƒæ¨¡åž‹ä¹‹å‰è®¾å®šã€‚å› æ­¤ï¼Œå¯¹äºŽè¶…å‚æ•°ä¼—å¤šçš„å¤æ‚æ¨¡åž‹ï¼Œè°ƒè¶…å‚æŠ€èƒ½æ˜¾å¾—å¾ˆé‡è¦ã€‚ 2. å¸¸ç”¨çš„è°ƒè¶…å‚æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ Grid Search ç½‘æ ¼æœç´¢/ç©·ä¸¾æœç´¢ æœç´¢æ•´ä¸ªè¶…å‚æ•°ç©ºé—´ï¼Œåœ¨é«˜ç»´ç©ºé—´å®¹æ˜“é‡åˆ°ç»´åº¦ç¾éš¾ï¼Œä¸å®žç”¨ã€‚ ç½‘æ ¼æœç´¢æ˜¯ä¸€ç§æ˜‚è´µçš„æ–¹æ³•ã€‚å‡è®¾æˆ‘ä»¬æœ‰nä¸ªè¶…å‚æ•°ï¼Œæ¯ä¸ªè¶…å‚æ•°æœ‰ä¸¤ä¸ªå€¼ï¼Œé‚£ä¹ˆé…ç½®æ€»æ•°å°±æ˜¯2çš„Næ¬¡æ–¹ã€‚å› æ­¤ï¼Œä»…åœ¨å°‘é‡é…ç½®ä¸Šè¿›è¡Œç½‘æ ¼æœç´¢æ˜¯å¯è¡Œçš„ã€‚ ç½‘æ ¼æœç´¢å¯ä»¥å¹¶è¡ŒåŒ–ï¼Œä½¿å¾—ç½‘æ ¼æœç´¢åœ¨è¶³å¤Ÿçš„è®¡ç®—èƒ½åŠ›ä¸‹æ›´åŠ å¯è¡Œã€‚ æ¯æ¬¡trialä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œä¸èƒ½åˆ©ç”¨å…ˆéªŒçŸ¥è¯†é€‰æ‹©ä¸‹ä¸€ç»„è¶…å‚æ•°ã€‚ Random Search éšæœºæœç´¢ ç¨€ç–çš„ç®€å•æŠ½æ ·ï¼Œè¯•éªŒä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œä¸èƒ½åˆ©ç”¨å…ˆéªŒçŸ¥è¯†é€‰æ‹©ä¸‹ä¸€ç»„è¶…å‚æ•°ã€‚ è¶…å‚é€šè¿‡å¹¶è¡Œé€‰æ‹©ï¼Œä½†è¯•éªŒæ¬¡æ•°è¦å°‘å¾—å¤šï¼Œè€Œæ€§èƒ½å´ç›¸å½“ã€‚ä¸€äº›è¶…å‚å¯èƒ½ä¼šäº§ç”Ÿè‰¯å¥½çš„æ€§èƒ½ï¼Œå¦ä¸€äº›ä¸ä¼šã€‚ Heuristic Tuning æ‰‹åŠ¨è°ƒå‚ ç»éªŒæ³•ï¼Œè€—æ—¶é•¿ã€‚ Automatic Hyperparameter Tuning è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜ è‡ªåŠ¨è¶…å‚æ•°è°ƒæ•´å½¢æˆäº†å…³äºŽè¶…å‚æ•°è®¾ç½®å’Œæ¨¡åž‹æ€§èƒ½ä¹‹é—´å…³ç³»çš„çŸ¥è¯†ï¼Œèƒ½åˆ©ç”¨å…ˆéªŒçŸ¥è¯†é€‰æ‹©ä¸‹ä¸€ç»„è¶…å‚æ•°ã€‚ é¦–å…ˆåœ¨å¤šä¸ªé…ç½®ä¸­æ”¶é›†æ€§èƒ½ï¼Œç„¶åŽè¿›è¡Œä¸€äº›æŽ¨æ–­å¹¶ç¡®å®šæŽ¥ä¸‹æ¥è¦å°è¯•çš„é…ç½®ã€‚ç›®çš„æ˜¯åœ¨æ‰¾åˆ°æœ€ä½³çŠ¶æ€æ—¶å°½é‡å‡å°‘è¯•éªŒæ¬¡æ•°ã€‚ è¿™ä¸ªè¿‡ç¨‹æœ¬è´¨ä¸Šæ˜¯é¡ºåºçš„ï¼Œä¸å®¹æ˜“å¹¶è¡ŒåŒ–ã€‚ è°ƒæ•´è¶…å‚æ•°çš„å¤§å¤šæ•°æ–¹æ³•éƒ½å±žäºŽåŸºäºŽé¡ºåºæ¨¡åž‹çš„å…¨å±€ä¼˜åŒ–ï¼ˆSMBOï¼‰ã€‚è¿™äº›æ–¹æ³•ä½¿ç”¨ä»£ç†å‡½æ•°æ¥é€¼è¿‘çœŸæ­£çš„é»‘ç›’å‡½æ•°ã€‚SMBOçš„å†…éƒ¨å¾ªçŽ¯æ˜¯å¯¹è¯¥æ›¿ä»£å“çš„ä¼˜åŒ–ï¼Œæˆ–è€…å¯¹ä»£ç†è¿›è¡ŒæŸç§è½¬æ¢ã€‚æœ€å¤§åŒ–æ­¤ä»£ç†çš„é…ç½®å°†æ˜¯ä¸‹ä¸€ä¸ªåº”è¯¥å°è¯•çš„é…ç½®ã€‚SMBOç®—æ³•åœ¨ä¼˜åŒ–æ›¿ä»£å“çš„æ ‡å‡†ä»¥åŠä»–ä»¬æ ¹æ®è§‚å¯ŸåŽ†å²å¯¹æ›¿ä»£å“è¿›è¡Œå»ºæ¨¡çš„æ–¹å¼ä¸Šæœ‰æ‰€ä¸åŒã€‚æœ€è¿‘åœ¨æ–‡çŒ®ä¸­æå‡ºäº†å‡ ç§ç”¨äºŽè¶…å‚æ•°çš„SMBOæ–¹æ³•ï¼š Bayesian Optimizationä½¿ç”¨é«˜æ–¯è¿‡ç¨‹å¯¹ä»£ç†è¿›è¡Œå»ºæ¨¡ï¼Œé€šå¸¸ä¼˜åŒ– Expected Improvement(EI)ï¼Œè¿™æ˜¯æ–°è¯•éªŒå°†åœ¨å½“å‰æœ€ä½³è§‚å¯Ÿä¸Šæ”¹è¿›çš„é¢„æœŸæ¦‚çŽ‡ã€‚é«˜æ–¯è¿‡ç¨‹æ˜¯å‡½æ•°çš„åˆ†å¸ƒã€‚æ¥è‡ªé«˜æ–¯è¿‡ç¨‹çš„æ ·æœ¬æ˜¯æ•´ä¸ªå‡½æ•°ã€‚è®­ç»ƒé«˜æ–¯è¿‡ç¨‹æ¶‰åŠå°†æ­¤åˆ†å¸ƒæ‹Ÿåˆåˆ°ç»™å®šæ•°æ®ï¼Œä»¥ä¾¿ç”ŸæˆæŽ¥è¿‘è§‚å¯Ÿæ•°æ®çš„å‡½æ•°ã€‚ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼Œå¯ä»¥è®¡ç®—æœç´¢ç©ºé—´ä¸­ä»»ä½•ç‚¹çš„EIã€‚æŽ¥ä¸‹æ¥å°†å°è¯•ç»™å‡ºæœ€é«˜çš„EIã€‚è´å¶æ–¯ä¼˜åŒ–é€šå¸¸ä¸ºè¿žç»­è¶…å‚æ•°ï¼ˆä¾‹å¦‚learning rate, regularization coefficientâ€¦ï¼‰æä¾› non-trivial/off-the-grid å€¼ï¼Œå¹¶ä¸”åœ¨ä¸€äº›å¥½çš„æ•°æ®é›†ä¸Šå‡»è´¥äººç±»è¡¨çŽ°ã€‚Spearmintæ˜¯ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„è´å¶æ–¯ä¼˜åŒ–å®žçŽ°ã€‚ SMACä½¿ç”¨éšæœºæ£®æž—å¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œå»ºæ¨¡ï¼Œä»Žéšæœºæ£®æž—è®¤ä¸ºæœ€ä¼˜çš„åŒºåŸŸï¼ˆé«˜EIï¼‰ä¸­æŠ½å–ä¸‹ä¸€ä¸ªç‚¹ã€‚ TPEæ˜¯SMACçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå…¶ä¸­ä¸¤ä¸ªåˆ†ç¦»çš„æ¨¡åž‹ç”¨äºŽæ¨¡æ‹ŸåŽéªŒã€‚ä¼—æ‰€å‘¨çŸ¥çš„TPEå®žçŽ°æ˜¯hyperoptã€‚ 3. æ¦‚å¿µè§£é‡Š é«˜æ–¯è¿‡ç¨‹ä¸Šå›¾é‡Œé‚£ä¹ˆå¤šçº¿å°±æ˜¯é«˜æ–¯è¿‡ç¨‹çš„ä½“çŽ°ã€‚è¦ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼Œéœ€è¦ä¸€ç§èƒ½çµæ´»åœ°åœ¨ç›®æ ‡å‡½æ•°ä¸Šå»ºç«‹åˆ†å¸ƒçš„æ–¹æ³•ã€‚è¿™æ¯”åœ¨å®žæ•°ä¸Šå»ºç«‹åˆ†å¸ƒæ›´æ£˜æ‰‹ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¿™æ ·çš„åˆ†å¸ƒæ¥è¡¨ç¤ºæˆ‘ä»¬å¯¹æ¯ä¸ªxçš„f(x)çš„ä¿¡å¿µã€‚å¦‚æžœxåŒ…å«è¿žç»­çš„è¶…å‚æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»ä¸ºf(x)å»ºæ¨¡æ— é™å¤šçš„xï¼Œå³æž„é€ å®ƒçš„åˆ†å¸ƒã€‚å¯¹äºŽè¿™ä¸ªé—®é¢˜ï¼Œé«˜æ–¯è¿‡ç¨‹æ˜¯ä¸€ç§ä¼˜é›…çš„æ–¹æ³•ã€‚å®žé™…ä¸Šï¼Œé«˜æ–¯è¿‡ç¨‹ç”Ÿæˆå¤šç»´é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶ä¸”å­˜åœ¨è¶³å¤Ÿçµæ´»ä»¥æ¨¡æ‹Ÿä»»ä½•ç›®æ ‡å‡½æ•°çš„æ¨¡æ ·ã€‚ Prior Function å…ˆéªŒå‡½æ•°åŸºäºŽæ¦‚çŽ‡åˆ†å¸ƒï¼Œç”¨äºŽæè¿°ç›®æ ‡å‡½æ•°çš„åˆ†å¸ƒï¼Œæ‹Ÿåˆç›®æ ‡å‡½æ•°æ›²çº¿ã€‚ä¸åŒçš„åˆ†å¸ƒï¼ŒPFä¸åŒï¼Œæ•ˆæžœæ˜¯ä¸ä¸€æ ·çš„ã€‚ Acquisition Function æ”¶èŽ·å‡½æ•° = max(mean + var)ç”¨äºŽä»Žå€™é€‰é›†ä¸­é€‰æ‹©ä¸€ä¸ªæ–°çš„ç‚¹ã€‚è´å¶æ–¯ä¼˜åŒ–çš„æ•ˆæžœä¸ŽAFçš„è®¾è®¡æœ‰è¾ƒå¤§çš„å…³ç³»ï¼Œç”±äºŽæ­¤ç±»functionå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ï¼Œå› æ­¤åœ¨é€‰ç‚¹æ—¶ï¼Œéœ€è€ƒè™‘ä¸èƒ½è¿‡æ—©è¿›å…¥å±€éƒ¨æœ€ä¼˜ã€‚AFè®¡ç®—EIï¼Œç”¨æ¥é€‰æ‹©ä¸‹ä¸€ä¸ªé‡‡æ ·ç‚¹ã€‚ meanå‡å€¼å¤§ï¼šå¤šåŽ»é‡‡æ ·è¿™äº›ç‚¹ä¼šå¸®åŠ©æˆ‘ä»¬æ›´å¥½çš„äº†è§£è¿™ä¸ªå‡½æ•°å½¢æ€ã€‚ varæ–¹å·®å¤§ï¼šè¡¨ç¤ºæˆ‘ä»¬å¯¹è¯¥ç‚¹çš„äº†è§£ç”šå°‘ã€‚ é‡‡æ ·ç‚¹æ¯ä¸€ä¸ªé‡‡æ ·ç‚¹å°±æ˜¯åŽŸç†è§£æžé‡Œçš„é»‘ç‚¹ã€‚æ¯ä¸ªé‡‡æ ·ç‚¹æ˜¯åŸºäºŽå‰é¢nä¸ªç‚¹çš„å¤šå˜é‡é«˜æ–¯åˆ†å¸ƒçš„å‡è®¾ä»¥åŠæœ€å¤§åŒ–AFè€Œå¾—åˆ°çš„ï¼ŒçŽ°ç›®å‰ä¸ºæ­¢è®¤ä¸ºçš„yçš„æœ€å¤§å€¼æœ€å¯èƒ½å‡ºçŽ°çš„ä½ç½®ã€‚ ä¸€å¼€å§‹ï¼Œé‡‡æ ·æ•°æ®å°‘ï¼Œç®—æ³•ä¼šé‡‡æ ‡å‡†å·®å¤§çš„ç‚¹ã€‚é‡‡æ ·ä¸€å®šæ•°ç›®åŽï¼Œæ ‡å‡†å·®çš„å€¼ä¼šä¸‹é™å¾ˆå¤šï¼Œæ­¤æ—¶é‡‡æ ·ç‚¹çš„é€‰æ‹©å°±æ›´å¤šçš„å—åˆ°å‡å€¼çš„å½±å“ï¼Œé‡‡æ ·ç‚¹å°±æ›´å¤§æ¦‚çŽ‡çš„å‡ºçŽ°åœ¨çœŸæ­£æœ€å¤§å€¼é™„è¿‘ã€‚ 4. Bayesian Optimizer åŽŸç†è§£æžè´å¶æ–¯ä¼˜åŒ–åŸºäºŽé«˜æ–¯è¿‡ç¨‹ã€‚ ä¸Šå›¾2ä¸ªevaluationsé»‘ç‚¹ï¼Œæ˜¯ä¸¤æ¬¡è¯„ä¼°åŽæ˜¾ç¤ºæ›¿ä»£æ¨¡åž‹çš„åˆå§‹å€¼ä¼°è®¡ï¼Œä¼šå½±å“ä¸‹ä¸€ä¸ªç‚¹çš„é€‰æ‹©ï¼Œç©¿è¿‡è¿™ä¸¤ä¸ªç‚¹çš„æ›²çº¿å¯ä»¥ç”»å‡ºéžå¸¸å¤šæ¡ï¼Œå¦‚ä¸Šä¸Šå›¾ çº¢è‰²è™šçº¿æ›²çº¿æ˜¯å®žé™…çœŸæ­£çš„ç›®æ ‡å‡½æ•° é»‘è‰²å®žçº¿æ›²çº¿æ˜¯ä»£ç†æ¨¡åž‹çš„ç›®æ ‡å‡½æ•°çš„å‡å€¼ ç°è‰²åŒºåŸŸæ˜¯ä»£ç†æ¨¡åž‹çš„ç›®æ ‡å‡½æ•°çš„æ–¹å·® åªæœ‰ä¸¤ä¸ªç‚¹ï¼Œæ‹Ÿåˆçš„æ•ˆæžœç¨å·®ï¼Œæ ¹æ®ä¸‹æ–¹çš„ç´«è‰²çš„EIæ›²çº¿ï¼Œæœ€å·¦ä¾§çš„æœ€å¤§å€¼EIä¸ºä¸‹ä¸€ä¸ªç‚¹ 3ä¸ªevaluationsé»‘ç‚¹ ç°è‰²åŒºåŸŸæ˜¯ä»£ç†æ¨¡åž‹çš„ç›®æ ‡å‡½æ•°çš„æ–¹å·®ï¼Œé»‘ç‚¹è¶Šå¤šï¼Œç°è‰²åŒºåŸŸé¢ç§¯è¶Šå°ï¼Œè¯¯å·®è¶Šå° æ ¹æ®ä¸‹æ–¹çš„ç´«è‰²çš„EIæ›²çº¿ï¼Œå·¦ä¾§çš„æœ€å¤§å€¼EIä¸ºç¬¬å››ä¸ªæ‹Ÿåˆç‚¹ 4ä¸ªevaluationsé»‘ç‚¹ é»‘ç‚¹è¶Šå¤šï¼Œç°è‰²åŒºåŸŸé¢ç§¯è¶Šå°ï¼Œè¯¯å·®è¶Šå°ï¼Œä»£ç†æ¨¡åž‹è¶ŠæŽ¥è¿‘çœŸå®žæ¨¡åž‹çš„ç›®æ ‡å‡½æ•° æ ¹æ®ä¸‹æ–¹çš„ç´«è‰²çš„EIæ›²çº¿ï¼Œæœ€å¤§å€¼EIä¸ºç¬¬äº”ä¸ªæ‹Ÿåˆç‚¹ï¼ŒåŒç†ç±»æŽ¨â€¦ 8ä¸ªé»‘ç‚¹ é»‘è‰²ä»£ç†æ›²çº¿å·²ç»ååˆ†æŽ¥è¿‘çº¢è‰²çœŸå®žç›®æ ‡å‡½æ•°ï¼Œç°è‰²åŒºåŸŸä¹Ÿè¶Šæ¥è¶Šå°ï¼Œæ‹Ÿåˆæ•ˆæžœä¸é”™ã€‚ 5. Bayesian Optimizer åŸºæœ¬æ€æƒ³ä¸€å¥è¯æ€»ç»“ï¼šå»ºç«‹ç›®æ ‡å‡½æ•°çš„æ¦‚çŽ‡æ¨¡åž‹ï¼Œå¹¶ç”¨å®ƒæ¥é€‰æ‹©æœ€æœ‰å¸Œæœ›çš„è¶…å‚æ•°æ¥è¯„ä¼°çœŸå®žçš„ç›®æ ‡å‡½æ•°ã€‚åŸºæœ¬æ€æƒ³æ˜¯ï¼šåˆ©ç”¨å…ˆéªŒçŸ¥è¯†é€¼è¿‘æœªçŸ¥ç›®æ ‡å‡½æ•°çš„åŽéªŒåˆ†å¸ƒä»Žè€Œè°ƒèŠ‚è¶…å‚ã€‚èŠ±ä¸€ç‚¹æ—¶é—´é€‰æ‹©ä¸‹ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»¥å‡å°‘å¯¹ç›®æ ‡å‡½æ•°çš„è°ƒç”¨ã€‚ å»ºç«‹ä»£ç†æ¨¡åž‹çš„ç›®æ ‡å‡½æ•°ï¼ˆPrior Function/å…ˆéªŒå‡½æ•°ï¼‰ æ‰¾åˆ°åœ¨ä»£ç†ä¸Šè¡¨çŽ°æœ€ä½³çš„è¶…å‚æ•°ï¼ˆåˆ©ç”¨EIå€¼ï¼Œæ ¹æ®Acquisition Functionå¾—å‡ºEIï¼‰ å°†è¿™äº›è¶…å‚æ•°åº”ç”¨äºŽçœŸæ­£çš„ç›®æ ‡å‡½æ•° æ›´æ–°åŒ…å«æ–°ç»“æžœçš„ä»£ç†æ¨¡åž‹ é‡å¤æ­¥éª¤2-4ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°æˆ–æ—¶é—´ åŸºäºŽé¡ºåºæ¨¡åž‹çš„ä¼˜åŒ–æ–¹æ³•ï¼ˆSMBOï¼‰æ˜¯è´å¶æ–¯ä¼˜åŒ–çš„å½¢å¼åŒ–ã€‚é¡ºåºæ˜¯æŒ‡ä¸€ä¸ªæŽ¥ä¸€ä¸ªåœ°è¿è¡Œè¯•éªŒï¼Œæ¯æ¬¡é€šè¿‡åº”ç”¨è´å¶æ–¯æŽ¨ç†å’Œæ›´æ–°æ¦‚çŽ‡æ¨¡åž‹ï¼ˆä»£ç†ï¼‰æ¥å°è¯•æ›´å¥½çš„è¶…å‚æ•°ã€‚ 6. Bayesian Optimizer åœ¨pythonä¸­çš„åŒ…Pythonä¸­æœ‰å‡ ä¸ªè´å¶æ–¯ä¼˜åŒ–åº“ï¼Œå®ƒä»¬åœ¨ç›®æ ‡å‡½æ•°çš„ä»£ç†ç®—æ³•ä¸Šæœ‰æ‰€ä¸åŒã€‚ Spearmintï¼ˆé«˜æ–¯è¿‡ç¨‹ä»£ç†ï¼‰ SMACï¼ˆéšæœºæ£®æž—å›žå½’ï¼‰ Hyperoptï¼ˆTree Parzen Estimator-TPEï¼‰ 7. Bayesian Optimizer ä¼˜ç‚¹ èƒ½åˆ©ç”¨å…ˆéªŒçŸ¥è¯†é«˜æ•ˆåœ°è°ƒèŠ‚è¶…å‚æ•°ï¼Œæ¯ä¸ªè¯•éªŒä¸ç‹¬ç«‹ï¼Œå‰ä¸€ä¸ªæŽ¨åŠ¨ä¸‹ä¸€ä¸ªé€‰æ‹© é€šè¿‡å‡å°‘è®¡ç®—ä»»åŠ¡è€ŒåŠ é€Ÿå¯»æ‰¾æœ€ä¼˜å‚æ•°çš„è¿›ç¨‹ ä¸ä¾èµ–äººä¸ºçŒœæµ‹æ‰€éœ€çš„æ ·æœ¬é‡ä¸ºå¤šå°‘ï¼Œä¼˜åŒ–æŠ€æœ¯åŸºäºŽéšæœºæ€§ï¼Œæ¦‚çŽ‡åˆ†å¸ƒ åœ¨ç›®æ ‡å‡½æ•°æœªçŸ¥ä¸”è®¡ç®—å¤æ‚åº¦é«˜çš„æƒ…å†µä¸‹æžå…¶å¼ºå¤§ é€šå¸¸é€‚ç”¨äºŽè¿žç»­å€¼çš„è¶…å‚ï¼Œä¾‹å¦‚ learning rate, regularization coefficient åœ¨æµ‹è¯•é›†è¡¨çŽ°ä¼˜ç§€äºŽæ‰‹å·¥è°ƒå‚ç»“æžœï¼Œæ³›åŒ–æ€§/é²æ£’æ€§å¥½ ä¸æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼š EIçš„è®¡ç®—æ ¹æ® Acquisition Functionæ”¶èŽ·å‡½æ•°è®¡ç®—æ‰€å¾—ï¼š æŽ¢ç´¢å’Œå¼€å‘æ˜¯è§£é‡ŠçŽ°è±¡å’Œä¼˜åŒ–ç®—æ³•çš„å¸¸ç”¨æœ¯è¯­ã€‚ å¯¹äºŽè´å¶æ–¯ä¼˜åŒ–ï¼Œä¸€æ—¦æ‰¾åˆ°å±€éƒ¨æœ€ä¼˜è§£ï¼Œä¼šåœ¨è¿™ä¸ªåŒºåŸŸä¸æ–­é‡‡æ ·ï¼Œå¾ˆå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚ä¸ºäº†å‡è½»è¿™ä¸ªï¼Œè´å¶æ–¯ä¼˜åŒ–ä¼šä½¿ç”¨â€œ æ”¶èŽ·å‡½æ•°Acquisition Function â€ æ¥å¹³è¡¡â€œæŽ¢ç´¢â€å’Œâ€œå¼€å‘â€ï¼Œä¸‹ä¸€ä¸ªç‚¹çš„é€‰æ‹©è¦åœ¨è¿™ä¸¤è€…ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ ä¸‹ä¸€ä¸ªé€‰æ‹©ç‚¹ï¼ˆxï¼‰åº”è¯¥å…·æœ‰é«˜å‡å€¼ï¼ˆå¼€å‘ï¼‰å’Œé«˜æ–¹å·®ï¼ˆæŽ¢ç´¢ï¼‰ã€‚ 8. å…¶ä»–ä¼˜ç§€æ–‡ç« ä¸Žè®ºæ–‡é“¾æŽ¥ðŸ”— è´å¶æ–¯ä¼˜åŒ–: ä¸€ç§æ›´å¥½çš„è¶…å‚æ•°è°ƒä¼˜æ–¹å¼ å“ˆä½›æ•™æï¼šA Tutorial on Bayesian Optimization for Machine Learning Shallow Understanding on Bayesian Optimization è°·æ­Œcloudmlä¹Ÿåœ¨ç”¨è´å¶æ–¯ä¼˜åŒ– A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning Practical Bayesian Optimization of Machine Learning Algorithms Automated Machine Learning Hyperparameter Tuning in Python A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning Introduction to Bayesian Optimization å†™çš„ä¸ä¸€å®šå…¨å¯¹ï¼Œå¦‚æžœæœ‰è¯´é”™çš„åœ°æ–¹ï¼Œæ¬¢è¿ŽæŒ‡æ­£ã€‚]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Bayesian Optimizer</tag>
        <tag>hyperparameter tuning</tag>
        <tag>automl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Auto Machine Learningç¬”è®° - Auto-Sklearn]]></title>
    <url>%2F2018%2F07%2F26%2FAutoML%2F</url>
    <content type="text"><![CDATA[auto-sklearnï¼š åŸºäºŽsklearn/ AutoML æ–¹å‘/ å…è´¹è‡ªåŠ¨æœºå™¨å­¦ä¹ æœåŠ¡/ GitHubå¼€æº/ 2.4k+ stars!!!â­é€‚è¯»äººç¾¤ï¼šæœ‰æœºå™¨å­¦ä¹ ç®—æ³•åŸºç¡€ å­¦å®Œ Machine Learningï¼Œåˆè¦å­¦ Auto Machine LearningðŸ™„ 1. auto-sklearn èƒ½ auto åˆ°ä»€ä¹ˆåœ°æ­¥ï¼Ÿåœ¨æœºå™¨å­¦ä¹ ä¸­çš„åˆ†ç±»æ¨¡åž‹ä¸­ï¼š å¸¸è§„ ML framework å¦‚ä¸‹å›¾ç°è‰²éƒ¨åˆ†ï¼šå¯¼å…¥æ•°æ®-æ•°æ®æ¸…æ´—-ç‰¹å¾å·¥ç¨‹-åˆ†ç±»å™¨-è¾“å‡ºé¢„æµ‹å€¼ autoéƒ¨åˆ†å¦‚ä¸‹å›¾ç»¿è‰²æ–¹æ¡†ï¼šåœ¨ML framework å·¦è¾¹æ–°å¢ž meta-learningï¼Œåœ¨å³è¾¹æ–°å¢ž build-ensembleï¼Œå¯¹äºŽè°ƒè¶…å‚æ•°ï¼Œç”¨çš„æ˜¯è´å¶æ–¯ä¼˜åŒ–ã€‚ è‡ªåŠ¨å­¦ä¹ æ ·æœ¬æ•°æ®: meta-learningï¼ŒåŽ»å­¦ä¹ æ ·æœ¬æ•°æ®çš„æ¨¡æ ·ï¼Œè‡ªåŠ¨æŽ¨èåˆé€‚çš„æ¨¡åž‹ã€‚æ¯”å¦‚æ–‡æœ¬æ•°æ®ç”¨ä»€ä¹ˆæ¨¡åž‹æ¯”è¾ƒå¥½ï¼Œæ¯”å¦‚å¾ˆå¤šçš„ç¦»æ•£æ•°æ®ç”¨ä»€ä¹ˆæ¨¡åž‹å¥½ã€‚ è‡ªåŠ¨è°ƒè¶…å‚ï¼šBayesian optimizerï¼Œè´å¶æ–¯ä¼˜åŒ–ã€‚ è‡ªåŠ¨æ¨¡åž‹é›†æˆ: build-ensembleï¼Œæ¨¡åž‹é›†æˆï¼Œåœ¨ä¸€èˆ¬çš„æ¯”èµ›ä¸­éƒ½ä¼šç”¨åˆ°çš„æŠ€å·§ã€‚å¤šä¸ªæ¨¡åž‹ç»„åˆæˆä¸€ä¸ªæ›´å¼ºæ›´å¤§çš„æ¨¡åž‹ã€‚å¾€å¾€èƒ½æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚ CASH problem: AutoML as a Combined Algorithm Selection and Hyperparameter optimization (CASH) problem ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€èˆ¬çš„åˆ†ç±»æˆ–è€…å›žå½’çš„æœºå™¨å­¦ä¹ æ¨¡åž‹å³å°†æˆ–è€…å·²ç»å®žçŽ°äº†ä½Žé—¨æ§›æˆ–è€…é›¶é—¨æ§›ç”šè‡³å…è´¹å»ºæ¨¡çš„ç¨‹åº¦ã€‚å…¶å®žæœºå™¨å­¦ä¹ çš„æ¯ä¸ªæ­¥éª¤éƒ½å¯ä»¥å‘ç€è‡ªåŠ¨åŒ–æ–¹å‘å‘å±•ï¼Œè€Œä¸”è‡ªåŠ¨åŒ–çš„æ–¹å¼åˆæœ‰å¾ˆå¤šç§ã€‚æœºå™¨å­¦ä¹ è‡ªåŠ¨åŒ–çš„éš¾ç‚¹è¿˜æ˜¯åœ¨æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹è¿™äº›æŠ€å·§ï¼Œè‡³äºŽæ¨¡åž‹ç­›é€‰ã€æ¨¡åž‹é›†æˆå’Œè¶…å‚æ•°è°ƒå‚å·²ç»æœ‰æ¯”è¾ƒæˆç†Ÿå¯ç”¨çš„ä»£ç äº†ã€‚æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯ äººäººéƒ½å¯ä»¥ç”¨å¾—èµ·æœºå™¨å­¦ä¹ ç³»ç»ŸðŸ™‚ æœ‰æ²¡æœ‰å¾ˆgoogleï¼ 2. ç›®å‰æœ‰å“ªäº›å…¬å¸åœ¨åšAutoMLï¼Œgithubä¸Šåˆæœ‰å“ªäº›å¼€æºé¡¹ç›®ï¼Ÿä¸šç•Œåœ¨ automl ä¸Šçš„è¿›å±•ï¼š Google: Cloud AutoML, Googleâ€™s Prediction API Microsoft: Custom Vision, Azure Machine Learning Amazon: Amazon Machine Learning others: BigML.com, Wise.io, SkyTree.com, RapidMiner.com, Dato.com, Prediction.io, DataRobot.com githubä¸Šçš„å¼€æºé¡¹ç›®ï¼š auto-sklearn (2.4k stars!)ï¼Œè®ºæ–‡é“¾æŽ¥ï¼šEfficient and Robust Automated Machine Learning ClimbsRocks/auto_mlï¼Œå¯ä»¥è¯»ä¸€ä¸‹ä»£ç å­¦ä¹ å¦‚ä½•å†™pipeline autokerasï¼ŒåŸºäºŽkerasçš„ automl å‘å¼€æºé¡¹ç›® 3. auto-sklearnçš„æ•´ä½“æ¡†æž¶äº†è§£ä¸€ä¸‹ï¼Ÿå‘ƒâ€¦å…ˆå‡‘æ´»çœ‹å§ï¼Œå…·ä½“çš„å¯ä»¥åˆ°githubä¸Šç¿»çœ‹æ–‡ä»¶ç»“æž„ã€‚æ¡†æž¶çš„ä¸»è½´åœ¨ç¬¬äºŒåˆ—ï¼Œç¬¬äºŒåˆ—çš„ç²¾åŽåœ¨pipelineï¼Œpipelineçš„é‡ç‚¹åœ¨componentsï¼šï¼ˆæˆªè‡³ç›®å‰å·²ç»è¶…å‡ºè®ºæ–‡å†™çš„å†…å®¹äº†ï¼Œè®ºæ–‡é“¾æŽ¥ï¼Œç›¸ä¿¡ä¼šè¶Šæ¥è¶Šä¸°å¯Œï¼‰ 16 classifiersï¼ˆå¯ä»¥è¢«æŒ‡å®šæˆ–è€…ç­›é€‰ï¼Œinclude_estimators=[â€œrandom_forestâ€, ]ï¼‰ adaboost, bernoulli_nb, decision_tree, extra_trees, gaussian_nb, gradient_boosting, k_nearest_neighbors, lda, liblinear_svc, libsvm_svc, multinomial_nb, passive_aggressive, qda, random_forest, sgd, xgradient_boosting 13 regressorsï¼ˆå¯ä»¥è¢«æŒ‡å®šæˆ–è€…ç­›é€‰ï¼Œexclude_estimators=Noneï¼‰ adaboost, ard_regression, decision_tree, extra_trees, gaussian_process, gradient_boosting, k_nearest_neighbors, liblinear_svr, libsvm_svr, random_forest, ridge_regression, sgd, xgradient_boosting 18 feature preprocessing methodsï¼ˆè¿™äº›è¿‡ç¨‹å¯ä»¥è¢«æ‰‹åŠ¨å…³é—­å…¨éƒ¨æˆ–è€…éƒ¨åˆ†ï¼Œinclude_preprocessors=[â€œno_preprocessingâ€, ]ï¼‰ densifier, extra_trees_preproc_for_classification, extra_trees_preproc_for_regression, fast_ica,feature_agglomeration, kernel_pca, kitchen_sinks, liblinear_svc_preprocessor, no_preprocessing, nystroem_sampler, pca, polynomial, random_trees_embedding, select_percentile, select_percentile_classification, select_percentile_regression, select_rates, truncatedSVD 5 data preprocessing methodsï¼ˆè¿™äº›è¿‡ç¨‹ä¸èƒ½è¢«æ‰‹åŠ¨å…³é—­ï¼‰ balancing, imputation, one_hot_encoding, rescaling, variance_thresholdï¼ˆçœ‹åˆ°è¿™é‡Œå·²ç»æœ‰ç‚¹æƒŠå–œäº†ï¼ç‚¹è¿›åŽ»æœ‰ä¸å°‘å†…å®¹ï¼‰ more than 110 hyperparameterså…¶ä¸­å‚æ•°include_estimators,è¦æœç´¢çš„æ–¹æ³•,exclude_estimators:ä¸ºä¸æœç´¢çš„æ–¹æ³•.ä¸Žå‚æ•°include_estimatorsä¸å…¼å®¹è€Œinclude_preprocessors,å¯ä»¥å‚è€ƒæ‰‹å†Œä¸­çš„å†…å®¹ auto-sklearnæ˜¯åŸºäºŽsklearnåº“ï¼Œå› æ­¤ä¼šæœ‰æƒŠè‰³å¼ºå¤§çš„æ¨¡åž‹åº“å’Œæ•°æ®/ç‰¹å¾é¢„å¤„ç†åº“ï¼Œä¸“ä¸šå‡ºèº«çš„è®¾å®šã€‚ 4. meta-learning æ˜¯ä»€ä¹ˆæ“ä½œï¼ŸðŸ”—poster What is MI-SMBO?Meta-learning Initialized Sequential Model-Based Bayesian Optimization What is meta-learning?Mimics human domain experts: use configurations which are known to work well on similar datasets ä»¿ç…§äººèƒ½ç§¯ç´¯ç»éªŒçš„åšæ³•ï¼Œä½¿æœºå™¨æœ‰[é…ç½®ç©ºé—´]åŽ»è®°å½•å®ƒä»¬çš„ç»éªŒå€¼ï¼Œæœ‰ç‚¹åƒè¿ç§»å­¦ä¹  é€‚ç”¨çš„ç¨‹åº¦ï¼Œæ ¹æ®æ•°æ®çš„ç›¸ä¼¼åº¦ meta-learning: warmstart the Bayesian optimization procedure ä¹Ÿå°±æ˜¯å­¦ä¹ ç®—æ³•å·¥ç¨‹å¸ˆçš„å»ºæ¨¡ä¹ æƒ¯ï¼Œæ¯”å¦‚çœ‹åˆ°ä»€ä¹ˆç±»åž‹çš„æ•°æ®å°±ä¼šæ˜Žç™½å¥—ç”¨ä»€ä¹ˆæ¨¡åž‹æ¯”è¾ƒé€‚åˆï¼ŒåŽ»ç”Ÿäº§å¯¹äºŽæ•°æ®çš„ metafeaturesï¼š å·¦è¾¹ï¼šé»‘è‰²çš„éƒ¨åˆ†æ˜¯æ ‡å‡†è´å¶æ–¯ä¼˜åŒ–æµç¨‹ï¼Œçº¢è‰²çš„æ˜¯æ·»åŠ meta-learningçš„è´å¶æ–¯ä¼˜åŒ– å³è¾¹ï¼šæœ‰ Metafeatures for the Iris datasetï¼Œæè¿°æ•°æ®é•¿ä»€ä¹ˆæ ·çš„featuresï¼Œä¸‹é¢çš„å…¬å¼æ˜¯è®¡ç®—æ•°æ®é›†ä¸Žæ•°æ®é›†çš„ç›¸ä¼¼åº¦çš„ï¼Œåªè¦å‘çŽ°ç›¸ä¼¼çš„æ•°æ®é›†ï¼Œå°±å¯ä»¥æ ¹æ®ç»éªŒæ¥æŽ¨èå¥½ç”¨çš„åˆ†ç±»å™¨ã€‚å†æ¥å¼ å¤§å›¾æ„Ÿå—ä¸‹metafeaturesåˆ°åº•é•¿å•¥æ ·ï¼šðŸ”—è®ºæ–‡é“¾æŽ¥ï¼šInitializing Bayesian Hyperparameter Optimization via Meta-LearningðŸ”—supplementary.pdf 5. auto-sklearn å¦‚ä½•å®žçŽ° è‡ªåŠ¨è¶…å‚æ•°è°ƒå‚ï¼Ÿæ¦‚å¿µè§£é‡Š SMBO: Sequential Model-based Bayesian/Global Optimizationï¼Œè°ƒè¶…å‚çš„å¤§å¤šæ•°æ–¹æ³•åŸºäºŽSMBO SMAC: Sequential Model-based Algorithm Configurationï¼Œæœºå™¨å­¦ä¹ è®°å½•ç»éªŒå€¼çš„é…ç½®ç©ºé—´ TPE: Tree-structured Parzen Estimator è¶…å‚æ•°è°ƒå‚æ–¹æ³•ï¼š Grid Search ç½‘æ ¼æœç´¢/ç©·ä¸¾æœç´¢åœ¨é«˜ç»´ç©ºé—´ä¸å®žç”¨ã€‚ Random Search éšæœºæœç´¢å¾ˆå¤šè¶…å‚æ˜¯é€šè¿‡å¹¶è¡Œé€‰æ‹©çš„ï¼Œå®ƒä»¬ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚ä¸€äº›è¶…å‚ä¼šäº§ç”Ÿè‰¯å¥½çš„æ€§èƒ½ï¼Œå¦ä¸€äº›ä¸ä¼šã€‚ Heuristic Tuning æ‰‹åŠ¨è°ƒå‚ç»éªŒæ³•ï¼Œè€—æ—¶é•¿ã€‚ï¼ˆä¸çŸ¥é“ç»éªŒæ³•çš„è‹±æ–‡æ˜¯å¦å¯ä»¥è¿™æ ·è¡¨ç¤ºï¼‰ Automatic Hyperparameter Tuning Bayesian Optimization èƒ½åˆ©ç”¨å…ˆéªŒçŸ¥è¯†é«˜æ•ˆåœ°è°ƒèŠ‚è¶…å‚æ•° é€šè¿‡å‡å°‘è®¡ç®—ä»»åŠ¡è€ŒåŠ é€Ÿå¯»æ‰¾æœ€ä¼˜å‚æ•°çš„è¿›ç¨‹ ä¸ä¾èµ–äººä¸ºçŒœæµ‹æ‰€éœ€çš„æ ·æœ¬é‡ä¸ºå¤šå°‘ï¼Œä¼˜åŒ–æŠ€æœ¯åŸºäºŽéšæœºæ€§ï¼Œæ¦‚çŽ‡åˆ†å¸ƒ åœ¨ç›®æ ‡å‡½æ•°æœªçŸ¥ä¸”è®¡ç®—å¤æ‚åº¦é«˜çš„æƒ…å†µä¸‹æžå…¶å¼ºå¤§ é€šå¸¸é€‚ç”¨äºŽè¿žç»­å€¼çš„è¶…å‚ï¼Œä¾‹å¦‚ learning rate, regularization coefficient SMAC TPE åœ¨ auto-sklearn é‡Œï¼Œä¸€ç›´å‡ºçŽ°çš„ bayesian optimizer å°±æ˜¯ç­”æ¡ˆã€‚æ˜¯åˆ©ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œè‡ªåŠ¨è°ƒå‚çš„ã€‚ðŸ”—å…·ä½“çš„è´å¶æ–¯ä¼˜åŒ–åŽŸç†é“¾æŽ¥ðŸ”—è®ºæ–‡é“¾æŽ¥ 6. auto-sklearn å¦‚ä½•å®žçŽ° è‡ªåŠ¨æ¨¡åž‹é›†æˆï¼Ÿå®˜æ–¹å›žç­”ï¼šautomated ensemble construction: use all classifiers that were found by Bayesian optimizationç›®å‰åœ¨åº“ä¸­æœ‰16ä¸ªåˆ†ç±»å™¨ï¼Œæ ¹æ®è´å¶æ–¯ä¼˜åŒ–æ‰¾å‡ºæœ€ä½³åˆ†ç±»å™¨ç»„åˆï¼Œæ¯”å¦‚æ˜¯ï¼ˆ0.4 random forest + 0.2 sgd + 0.4 xgboost)å¯ä»¥æ ¹æ®fitå®Œçš„åˆ†ç±»å™¨æ‰“å°ç»“æžœçœ‹æœ€ç»ˆçš„æ¨¡åž‹æ˜¯ç”±ä»€ä¹ˆåˆ†ç±»å™¨ç»„æˆï¼Œä»¥åŠå®ƒä»¬çš„å‚æ•°æ•°å€¼ï¼š12345import autoskleran.classificationautoml = autosklearn.classification.AutoSklearnClassifier()automl.fit(X_train, y_train)automl.show_models() æ‰“å°automl.show_models()å°±èƒ½æ‰“å°å‡ºæ‰€è°“çš„è‡ªåŠ¨é›†æˆæ¨¡åž‹æœ‰å“ªäº›ï¼Œæƒé‡åˆ†å¸ƒï¼Œä»¥åŠè¶…å‚æ•°æ•°å€¼ã€‚ 7. å¦‚ä½•ä½¿ç”¨ auto-sklearnï¼Ÿ é€‚ç”¨ç³»ç»Ÿï¼šLinux ðŸ‘installation å®˜æ–¹æ–‡æ¡£ðŸ”— æŽ¥å£æ–‡æ¡£ðŸ”— ä¸¾ä¸ªæ —å­ðŸ”— å†ä¸¾ä¸ªæ —å­ðŸ”— ä½¿ç”¨å¥—è·¯å¦‚ä¸‹ï¼š 1234567# 4è¡Œä»£ç æžå®šimport autosklearn.classificationautoml = autosklearn.classification.AutoSKlearnClassifier()automl.fit(X_train, y_train) predictions = automl.predict(X_test) # æ‰“å°å‡º0ï¼Œ1ç»“æžœpredictions_prob = automl.predict_proba(X_test) # æ‰“å°å‡º0-1ä¹‹é—´çš„æ¦‚çŽ‡å€¼ äº²æµ‹ X_train, y_train å†…ä¸èƒ½å«æœ‰éžæ•°å€¼åž‹æ•°æ®ï¼Œæ¯”å¦‚Male/Femaleå­—æ¯å°±æŠ¥é”™ã€‚ è®­ç»ƒé›†æœ‰å“ªäº›ç‰¹å¾ï¼Œæµ‹è¯•é›†å°±å¿…é¡»æœ‰å“ªäº›ç‰¹å¾ï¼Œå¯ä»¥ç†è§£ä¸ºä¸åšç‰¹å¾ç­›é€‰ï¼Œæ‰€ä»¥æœ€åˆå¯¼å…¥è®­ç»ƒé›†çš„ç‰¹å¾è¶Šç²—ç³™è¶Šå¥½ã€‚ 1automl.cv_results_ ä¼šæ‰“å°å‡ºéžå¸¸éžå¸¸å¤šçš„ä¸œè¥¿ï¼Œè€å¿ƒçœ‹ï¼Œä¼šæ‰¾åˆ°ç±»ä¼¼ä¸‹é¢çš„è§„å¾‹ã€‚ 123456789101112131415161718automl.sprint_statistics()# æ‰“å°ç»“æžœå¦‚ä¸‹ï¼š# &apos;auto-sklearn results:# Dataset name: 46b7545efa67d8cd76f70e71eb67b72e # Metric: accuracy# Best validation score: 0.955932 # Number of target algorithm runs: 1278# Number of successful target algorithm runs: 1252# Number of crashed target algorithm runs: 23# Number of target algorithms that exceeded the time limit: 3 # Number of target algorithms that exceeded the memory limit: 0&apos;automl._get_automl_class()# æ‰“å°ç»“æžœ# autosklearn.automl.AutoMLClassifier å…¶ä»–å¯ä»¥å°è¯•çš„æ“ä½œï¼š 12345automl.score(X,y)automl.get_models_with_weights()automl.get_configuration_space(X,y) 8. auto-sklearn ç›®å‰æœ‰ä»€ä¹ˆç¼ºç‚¹ ä¸æ”¯æŒæ·±åº¦å­¦ä¹ ï¼Œä½†æ˜¯è²Œä¼¼ä¼šæœ‰AutoNetå‡ºæ¥ï¼Œåƒè°·æ­Œçš„cloud AutoMLé‚£æ · è®¡ç®—æ—¶é•¿å¾€å¾€ä¸€ä¸ªå°æ—¶ä»¥ä¸Š åœ¨æ•°æ®æ¸…æ´—è¿™å—è¿˜éœ€è¦äººä¸ºå‚ä¸Žï¼Œç›®å‰å¯¹éžæ•°å€¼åž‹æ•°æ®ä¸å‹å¥½ 9. AutoML çš„å‘å±•æƒ…å†µéšç€è¿™ä¸¤å¤©è°·æ­Œå‘å¸ƒå®ƒä»¬çš„ Cloud AutoML å„ç§æƒŠè‰³çš„åŠŸèƒ½ï¼Œå¯¹äºŽè¿™å—çš„å…³æ³¨åº¦ä¼šè¶Šæ¥è¶Šé«˜çš„å§~machine learningçš„æ¯”èµ›å·²ç»ä¸è¶³ä¸ºå¥‡å•¦ï¼ŒçŽ°åœ¨å·²ç»æœ‰å¾ˆå¤šæœ‰å…³AutoMLçš„æ¯”èµ›äº†ï¼šhttp://automl.chalearn.org]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>auto ml</tag>
        <tag>auto sklearn</tag>
        <tag>hyperparameter</tag>
        <tag>model ensemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[25å²å­¦åˆ°çš„25ä»¶äº‹æƒ…]]></title>
    <url>%2F2018%2F07%2F12%2F25%E5%B2%81%E5%AD%A6%E5%88%B0%E7%9A%8425%E4%BB%B6%E4%BA%8B%E6%83%85%2F</url>
    <content type="text"><![CDATA[è¿™æ˜¯æˆ‘æ¯”è¾ƒå–œæ¬¢çœ‹çš„ä¸€ç±»æ–‡ç« ï¼Œä¹Ÿç®—æ˜¯æˆ‘çš„ä¸‰è§‚/ ä»·å€¼è§‚çš„å¹´åº¦æ€»ç»“ã€‚24å²çš„æˆ‘ï¼Œå–œæ¬¢æ¸…é›¶ï¼Œå–œæ¬¢åŽ»æŽ¨ç¿»è¿‡åŽ»çš„è‡ªå·±ï¼Œå–œæ¬¢æŒ£æ‰Žï¼Œå–œæ¬¢æŠ˜è…¾ã€‚ Collecting what Iâ€™ve learned in my 24â€¦â€¦100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24G 1G/s Initializingâ€¦â€¦ 25å²åˆå§‹å€¼è®¾ç½®å¦‚ä¸‹ï¼š 1. æ²¡äººèƒ½å¤Ÿå®šä¹‰ä½ ï¼Œé™¤äº†ä½ è‡ªå·±ä½ å¯ä»¥æ˜¯ä¸€ä¸ªåŽ¨å¸ˆã€æ¨¡ç‰¹ã€æ‘„å½±å¸ˆã€éŸ³ä¹å®¶ã€å·¥ç¨‹å¸ˆï¼Œæˆ–è€…æ–œæ é’å¹´ï¼›ä½ å¯ä»¥æ˜¯ä¹è§‚çš„ã€åæ‰§çš„ã€å†…å¿ƒæ‰­æ›²çš„ã€å¼€æœ—çš„ï¼Œæˆ–è€…å¤šé‡äººæ ¼çš„ï¼›å®šä¹‰ä¸€ä¸ªäººçš„ç»´åº¦éžå¸¸å¤šã€‚è¿™å°±æ„å‘³ç€ï¼Œåœ¨ä½ æœ‰é™çš„ç”Ÿå‘½é‡Œï¼Œ ä½ å¯ä»¥è‡ªå®šä¹‰è‡ªå·±çš„ç»´åº¦é‡çº§ã€‚ä»Šå¤©çš„ä½ å¯ä»¥æ˜¯3ç»´çš„ï¼Œä½ æ˜¯ä¸€ä¸ªä¼šæ‹ç…§çš„/ç»å¸¸ç…®å¥½åƒæ—©é¤çš„/ç®—æ³•å·¥ç¨‹å¸ˆã€‚æ˜Žå¤©çš„ä½ å°±å¯ä»¥æ˜¯4ç»´çš„ï¼Œæ˜Žå¤©æ˜¯ä¸€ä¸ªä¼šæ‹ç…§çš„/ç»å¸¸ç…®å¥½åƒæ—©é¤çš„/ä¼šç”»ç”»çš„/ç®—æ³•å·¥ç¨‹å¸ˆã€‚ ä½ å¯ä»¥è‡ªå®šä¹‰ç»´åº¦çš„æŽ’åˆ—ç»„åˆã€‚ä¸Šä¸ªæœˆçš„ä½ æ˜¯ä¼šæ‰“æž¶å­é¼“çš„å‰ç«¯å·¥ç¨‹å¸ˆï¼Œè¿™ä¸ªæœˆå°±å¯ä»¥åŽ»å°è¯•æˆä¸ºä¸€ä¸ªä¼šè·³èˆžçš„ç¾Žå¦†åšä¸»ã€‚ å¦‚æžœä½ è§‰å¾—åˆ«äººå®šä¹‰äº†ä½ ï¼Œæ¯”å¦‚ä½ è¯´çˆ¶æ¯ä¸ºä½ å‡†å¤‡äº†ä»¥åŽçš„é“è·¯ï¼Œä»–ä»¬å¸Œæœ›ä½ æˆä¸ºä¸€ä¸ªå…¬åŠ¡å‘˜äº‘äº‘ä¹‹ç±»çš„ã€‚é¦–å…ˆï¼Œè¯·ä½ è®°ä½ï¼Œä¹Ÿè¯·ä½ æ‰¿è®¤ï¼ æ˜¯ä½ è‡ªå·±å…è®¸ï¼Œæ˜¯ä½ æŽˆäºˆåˆ«äºº/æŽˆäºˆä½ çˆ¶æ¯å®šä¹‰ä½ çš„æƒåŠ›ã€‚ åªè¦ä½ ä¸å…è®¸ï¼Œåªè¦ä½ æ•¢å¯¹æŠ—ä½ çš„å¥´æ€§ï¼Œä½ å°±èƒ½ä¸»å®°ä½ è‡ªå·±ã€‚ ä½ åšçš„äº‹æƒ…ï¼Œå®šä¹‰äº†ä½ æ˜¯ä¸ªä»€ä¹ˆæ ·çš„äººã€‚ æˆ‘å½“ç„¶çŸ¥é“è½¬åž‹å’Œå˜åŒ–æ˜¯éœ€è¦ä»˜å‡ºä»£ä»·çš„ï¼Œå¯å°±æ˜¯å› ä¸ºæœ‰æŒ‘æˆ˜æ€§æ‰å¥½çŽ©çš„å˜›ã€‚æ™®é€šçŽ©å®¶æ‰çŽ©æ ‡é…ï¼Œé«˜çº§çŽ©å®¶éƒ½å¼€å¯è‡ªå®šä¹‰æ¨¡å¼ã€‚ 2. è‡ªä¿¡çš„äººæ°¸è¿œæ˜¯ç¾Žçš„æ¯ä¸ªäººå¿ƒä¸­çš„ç¾Žåº”è¯¥æ˜¯ä¸ä¸€æ ·çš„ï¼Œæ— è®ºæ˜¯è‚¤è‰²æ·±æµ…/å‘åž‹é•¿çŸ­/çœ¼ç›å¤§å°/è…°è…¿ç²—ç»†ï¼Œåªè¦æ˜¯ä¸å½±å“ä½ çš„å¥åº·çš„èº«æï¼Œéƒ½æ˜¯ç¾Žçš„ã€‚ä¸è¦é™·å…¥æ— é™å¦å®šè‡ªå·±çš„å¾ªçŽ¯ä¸­ï¼Œç®€ç›´æ˜¯è‡ªæˆ‘æ¯ç­ã€‚å¤šè‚¯å®šè‡ªå·±ã€‚ä¸»æµä¸ä¸€å®šæ˜¯æ­£ç¡®çš„é€‰æ‹©ï¼Œæ¯•ç«ŸçœŸç†è¿˜æŽŒæ¡åœ¨å°‘æ•°äººæ‰‹ä¸­å‘¢ã€‚é‚£äº›çœŸæ­£çˆ±ä½ çš„äººï¼Œä¼šå‘çŽ°ä½ çš„ç¾Žï¼Œä»–ä»¬æ›´å…³å¿ƒä½ å¿«ä¸å¿«ä¹ï¼Œå¥ä¸å¥åº·ã€‚è‡³å°‘æˆ‘çš„å¶åƒä¸ä¸€å®šéƒ½é•¿çš„å¥½çœ‹ï¼Œä½†ä¸€å®šæ˜¯è¶…çº§è‡ªä¿¡çš„ã€‚è‡ªä¿¡çš„äººå°±æ˜¯ç‰›é€¼å“„å“„çš„ï¼ 3. å¹´é¾„è¿™ä¸ªæ•°å­—ï¼Œä»€ä¹ˆéƒ½è¯´æ˜Žä¸äº†å¹´é¾„ä»£è¡¨ä¸äº†ä¸€ä¸ªäººçš„å¥åº·çŠ¶å†µï¼Œä»£è¡¨ä¸äº†æ€§æ ¼ï¼Œæ›´ä»£è¡¨ä¸äº†ä¸€ä¸ªäººçš„å­¦è¯†ç¨‹åº¦ã€‚æœ‰æ—¶å€™ï¼Œä¸å¬è€äººè¨€ï¼Œèƒ½å¼€å¿ƒä¸ªå¥½å‡ å¹´ã€‚å¦‚æžœä½ è‡ªå·±æ‹¿å¹´é¾„è¯´äº‹ï¼Œ30å²æƒ³è½¬è¡Œå¾ˆçŠ¹è±«ï¼Ÿ40å²ä¸è¯¥é‡å›žå¤§å­¦ï¼Ÿ50å²ä¸è¯¥å¯»æ‰¾è½°è½°çƒˆçƒˆçš„çˆ±æƒ…ï¼Ÿæœ‰äº›é—¨æ§›æ˜¯äººä¸ºå®šçš„ï¼Œå› ä¸ºä»–ä»¬æƒ³ä¿æŠ¤è‡ªå·±çš„åœˆå­ã€‚ä¸è¦è®©åˆ«äººå‘Šè¯‰ä½ ä»€ä¹ˆè¯¥åšä»€ä¹ˆä¸è¯¥åšï¼Œè®©ä»–ä»¬æžæ¸…æ¥šä»–ä»¬è‡ªå·±çš„äººç”Ÿå°±ä¸é”™äº†ã€‚åˆ«äººä¸èƒ½æ‹¦ç€ä½ åšæƒ³åšçš„äº‹ï¼Œä½ è‡ªå·±ä¹Ÿåˆ«æ‹¦ç€è‡ªå·±ã€‚ ä»€ä¹ˆæ—¶å€™ç»“æŸä¸€æ®µç³Ÿç³•çš„æ„Ÿæƒ…éƒ½ä¸æ™šã€‚ä»€ä¹ˆæ—¶å€™å¼€å§‹ä¸€åœºç¾Žå¥½çš„å¾ç¨‹éƒ½ä¸æ™šã€‚ä»€ä¹ˆéƒ½æ˜¯ä»Žé›¶å¼€å§‹çš„ï¼Œä½ ç€æ€¥ä¹Ÿæ²¡ç”¨ï¼Œä¸€æ—¦å¼€å§‹ï¼Œä¸€æ—¦åŽ»åšäº†ï¼Œå°±ä¸é‚£ä¹ˆç„¦è™‘äº†ã€‚ å¹´é¾„æ˜¯ä½ è‡ªå·±å¼ºåŠ ç»™è‡ªå·±çš„å‡å¦‚30å²çš„ä½ æœ‰ä¸€å¤©å¤±å¿†äº†ï¼Œè¢«å‘ŠçŸ¥æˆ25å²ï¼Œåˆæœ‰ä½•è¿å’Œæ„Ÿå‘¢ï¼Ÿ å¹´é¾„è¿™ä¸œè¥¿å¤ªè™šäº†ï¼Œjust let it go ~åŒç†ï¼Œæˆç»©å¥½çš„ä¸ä¸€å®šäººå“å¥½ï¼Œåæ°”å¤§ä¸ä¸€å®šä»£è¡¨ä¼˜ç§€ã€‚ 4. å¾ˆå¤šäº‹æƒ…ï¼Œæ²¡æœ‰å”¯ä¸€çš„æ ‡å‡† å¾ˆå¤šäº‹æƒ…éƒ½æ²¡æœ‰å”¯ä¸€æ ‡å‡†ï¼Œç”šè‡³æ²¡æœ‰å¯¹é”™ã€‚ä½ è¦åŠ¨è„‘å­ï¼ŒåŒæ—¶ä¸è¦è®©å¸¸è¯†ï¼ˆåˆ«äººçš„è¯´æ³•ï¼‰å½±å“ä½ çš„åˆ¤æ–­ã€‚æ¯”å¦‚ä¸Šä¸ä¸Šå¤§å­¦è¿™ä»¶äº‹ï¼Œä¸Šå¤§å­¦å¥½åƒæ˜¯é¡ºå…¶è‡ªç„¶çš„ä¸€ä¸ªå¿…é€‰é¡¹ï¼Œå¾ˆå¤šäººä¼šé—®18å²ä¸åŽ»ä¸Šå¤§å­¦åŽ»å¹²å˜›ï¼Ÿä½†æ˜¯ï¼Œä¸ä¸Šå¤§å­¦æ˜¯é”™çš„å—ï¼ŸçŽ°åœ¨å¾ˆå¤šåäººç”¨ä»–ä»¬è‡ªå·±çš„äººç”Ÿè¯æ˜Žäº†ä¸ä¸Šå¤§å­¦ä¸ä¸€å®šæ˜¯ä¸ªä¸æ­£ç¡®çš„é€‰æ‹©ã€‚ åœ¨æ²¡æœ‰å¯¹é”™ä¹‹åˆ†ï¼Œæ²¡æœ‰æ˜¯éžé»‘ç™½çš„æ—¶å€™ï¼Œè‡ªå·±çš„ä¸‰è§‚/ä»·å€¼è§‚å°±æ˜¯ä¸€æŠŠæ ‡æ†å°ºå­ã€‚ä½ è§‰å¾—è¿™æ ·æ˜¯æ­£ç¡®çš„ï¼ŒåŒæ—¶åˆ«äººä¹Ÿå¯èƒ½è§‰å¾—è¿™æ ·æ˜¯é”™è¯¯çš„ã€‚ æ­¤æ—¶ä¸è¦è§‰å¾—è¿™ä¸–ç•Œä¸Šçš„äººé»‘ç™½ä¸åˆ†äº†ï¼Œæ­¤æ—¶è¦ç”¨ä½ çš„çŸ¥è¯†åº“å½¢æˆä½ çš„è§‚ç‚¹ï¼Œå¹¶ä¸”èƒ½ç”¨äº‹å®žåŽ»è¯æ˜Žä½ çš„è§‚ç‚¹ã€‚ä½ å¯ä»¥è´¨ç–‘è‡ªå·±ï¼Œåé©³è‡ªå·±æ¥åŠ å¼ºè‡ªå·±çš„è§‚ç‚¹ï¼Œç»è¿‡è€ƒéªŒåŽä½ æœ‰ä¸€ä¸ªä¸ªåšå®šçš„æƒ³æ³•ï¼Œå½¢æˆä½ å¿ƒä¸­çš„å°ºå­ï¼Œå†åŽ»è¡¡é‡çœ¼å‰çš„è¿™ä¸ªä¸–ç•Œã€‚ å¦‚æžœä¹‹åŽçœŸçš„é‡åˆ°æŽ¨ç¿»ä½ ä¸‰è§‚çš„äº‹æƒ…ï¼Œé‚£ä¹ˆä½ éœ€è¦é‡æ–°æ€è€ƒäº†ã€‚ä½ æ­£åœ¨æŽ¥è§¦ä¸€ä¸ªæ›´å¤§çš„ä¸–ç•Œã€‚ 5. æ‰¾å·¥ä½œï¼ŒæŒºçœ‹ç¼˜åˆ†çš„æ‰¾å·¥ä½œï¼Œå®ƒè¡¡é‡çš„å› ç´ å¤ªå¤šäº†ï¼Œä½ ä¸ä¸€å®šæˆç»©å¥½é¡¹ç›®å¤šå°±èƒ½åŽ»ä½ æƒ³åŽ»çš„åœ°æ–¹ï¼Œæ€»æœ‰é˜´å·®é˜³é”™çš„äº‹æƒ…å‘ç”Ÿã€‚å½“è¢«æ‹’ç»çš„æ—¶å€™ï¼Œåˆ«å¤ªå½“å›žäº‹ï¼Œå› ä¸ºè¿™çœŸçš„å¾ˆæ­£å¸¸ã€‚å½“ç„¶æœ‰äº›åæ€å·¥ä½œè¿˜æ˜¯è¦åšçš„ï¼Œæ¯”å¦‚è‡ªå·±æ˜¯å¦è¿˜ç¼ºå“ªéƒ¨åˆ†çš„èƒ½åŠ›éœ€è¦è¡¥é½~ 6. å¾ˆå¤šäº‹æƒ…éƒ½æ˜¯çº¸è€è™Žï¼Œä½ åŽ»åšå°±çŸ¥é“äº†ç»å¸¸å‘Šè¯‰è‡ªå·±ï¼Œå¯¹äºŽæƒ³åšçš„äº‹ï¼Œæœ‰60%çš„è‡ªä¿¡å°±å¤Ÿäº†ã€‚ä¸è¦ç­‰ä»€ä¹ˆéƒ½è¦å‡†å¤‡å¥½ï¼Œè¦ä¸Žè¿™ä¸ªä¸–ç•Œå¯¹è¯ï¼Œå‘Šè¯‰å¤–ç•Œæˆ‘æƒ³åšè¿™ä¸ªï¼Œè€Œä¸”æˆ‘è¿˜ä¼šåšçš„å¾ˆä¸é”™ã€‚å½“ç„¶ä¹Ÿå¯ä»¥é—·å£°é—·å“çš„åŽ»åšï¼Œæ€»ä¹‹åŽ»åšå°±å¯¹äº†ï¼å¾ˆå¤šä¸œè¥¿çœ‹èµ·æ¥å¥½åƒå¾ˆé«˜å¤§ä¸Šï¼Œå¾ˆå¤šäººçœ‹èµ·æ¥å¥½åƒå¾ˆç‰›é€¼ï¼Œç­‰ä½ ä¸€æ­¥æ­¥é€¼è¿‘ä»–ä»¬çš„æ—¶å€™ä½ å°±ä¸ä¼šé‚£ä¹ˆè§‰å¾—äº†ã€‚æ‰€ä»¥æ‰ä¼šæœ‰å¾ˆå¤šåŽ‰å®³çš„äººè§‰å¾—è‡ªå·±æ˜¯éª—å­è¿™ç§å‡è±¡ï¼Œåˆ«äººè¯´ä½ åŽ‰å®³çš„æ—¶å€™ï¼Œåˆ«å¤ªå½“å›žäº‹äº†ã€‚åŒæ—¶ï¼Œè¦å­¦ä¼šå€ŸåŠ› ä¸è¦é‡å¤é€ è½®å­ã€‚ 7. äº«å—å­¤å•ä¸ä¸ºäº†åˆç¾¤è€Œåˆç¾¤ï¼Œæ—¶é—´å¾ˆå®è´µï¼Œä¸è¦èŠ±åœ¨è¿Žåˆåˆ«äººä¸Šã€‚æŠŠæ—¶é—´èŠ±åœ¨å“ªé‡Œï¼Œä½ å°±å±žäºŽå“ªé‡Œã€‚èŠ±æ—¶é—´åŽ»å’Œè‡ªå·±ç¢°æ’žï¼Œæœ‰æ‰€åå¼¹æ‰æ˜¯å¯¹è‡ªå·±çš„è®¤çŸ¥çš„å¼€å§‹ã€‚æ›¾ç»çš„æˆ‘å¯¹å¤–ç•Œçš„è¦æ±‚å¾ˆä½Žå¾ˆä½Žï¼Œå› ä¸ºæˆ‘ä¸çŸ¥é“è‡ªå·±æ˜¯è°ï¼Œè‡ªå·±çš„ä»·å€¼æœ‰å¤šé«˜ï¼Œä¸‰kä¸€ä¸ªæœˆçš„å®žä¹ ä»Žæ¥ä¸è§‰å¾—å§”å±ˆã€‚ç­‰åˆ°ä¸æ–­ä¸ŽåŽ‰å®³çš„äººç¢°æ’žä¹‹åŽå¼€å§‹æ„è¯†åˆ°è‡ªå·±å‡å€¼çš„ç©ºé—´ï¼Œæ‰ä¼šåœ¨æ­£å¼æ‰¾å·¥ä½œæ—¶å¯ä»¥è°ˆåˆ¤è‡ªå·±çš„ç†æƒ³æ”¶å…¥ã€‚å¤ªå¤šæ—¶å€™ï¼Œçš„ç¡®éœ€è¦å…¶ä»–äººçš„é™ªä¼´ï¼Œéœ€è¦å¤–ç•Œçš„è‚¯å®šï¼Œä½†æ˜¯ä¹Ÿè¶Šæ¥è¶Šè§‰å¾—ï¼Œå‘å†…å¯»æ±‚è‚¯å®šæ‰æ˜¯æ­£äº‹ã€‚å¯»æ‰¾è‡ªå·±å¯¹è‡ªå·±çš„è‚¯å®šï¼Œæ‰¾åˆ°è‡ªå·±å¯¹è‡ªå·±çš„é‚£ä¸€ä»½æ”¯æŒï¼ŒçœŸçš„æ¯”å…¶ä»–ä»»ä½•äººçš„åˆ†é‡éƒ½é‡ï¼ä¸ä»…è¦åšè‡ªå·±ï¼Œè¿˜è¦åšä¸€ä¸ªèƒ½ç…§äº®è‡ªå·±çš„äººã€‚çŽ°åœ¨çš„è‡ªå·±æœ‰ç‚¹åç¤¾äº¤çš„çŠ¶æ€ï¼Œåªè¦æœ‰ç©ºæ›´å–œæ¬¢ä¸€ä¸ªäººå‘†ç€ï¼Œæ›´å–œæ¬¢åŽ»äº†è§£è‡ªå·±åˆ°åº•åœ¨æƒ³ä»€ä¹ˆï¼Œå–œæ¬¢ä»€ä¹ˆï¼Œæƒ³åšä»€ä¹ˆã€‚ 8. ä¸è¦æ€¥ç€say no å¤šè¯´why not? ä¸ºä»€ä¹ˆä¸å‘¢ï¼Ÿå‰å‡ å¹´ç»å¸¸è¢«æŒ¤å‡ºèˆ’é€‚åœˆå­ï¼Œè¢«åŠ¨æŽ¥å—æ”¹å˜æœ‰æ—¶å€™ä¹Ÿä¸æ˜¯åäº‹ã€‚è·³å‡ºä¸€ä¸ªç†Ÿæ‚‰çš„åœˆå­ï¼Œä¼šè¿«ä½¿æˆ‘åŽ»é è¿‘è·Ÿæˆ‘å–œå¥½æ›´æŽ¥è¿‘çš„åœˆå­ï¼Œä¼šè®©æˆ‘æ›´æŽ¥è¿‘çœŸæ­£å±žäºŽæˆ‘è‡ªå·±çš„åœˆå­ã€‚ ç†Ÿäººæœªå¿…é€‚åˆåŒè·¯ã€‚ è¿˜æœ‰å¾ˆå¤šæœªçŸ¥çš„äº‹æƒ…ä»Žå››é¢å…«æ–¹å‘ä½ é è¿‘ï¼Œä¸è¦æ€¥ç€say noï¼Œå¯ä»¥è¯•è¯•å¼ºè¿«è‡ªå·±è¯´yesã€‚æ¯”å¦‚æœ‹å‹é—®ä½ å‘¨æœ«è¦ä¸è¦ä¸€èµ·å¥èº«ï¼Œå…ˆåŽ»ä½“éªŒä¸€ä¸‹ä¹Ÿæ— å¦¨ã€‚æ¯”å¦‚æˆ‘å°±ä¼šå¼ºè¿«è‡ªå·±ä»Šå¤©å‡ºé—¨å°±åŽ»åƒä¸€å®¶æ²¡åƒè¿‡çš„é¤åŽ…ã€‚å¥½çŽ©çš„æ˜¯ï¼Œå¤§å¤šæ•°æƒ…å†µæˆ‘è¿˜è›®å–œæ¬¢åˆšåˆšæŽ¥è§¦çš„ä¸œè¥¿ã€‚ æœ‰æ—¶ä¹Ÿå¯ä»¥å­¦ä¼šä¸»åŠ¨ç¦»å¼€ï¼Œä¸»åŠ¨ç¦»å¼€èˆ’é€‚åœˆï¼Œä¸»åŠ¨åŽ»å°è¯•æ–°çš„ä½“éªŒã€‚æˆ‘çŽ°åœ¨å¤©å¤©è¿‡ç€ä¸èˆ’é€‚çš„ç”Ÿæ´»ï¼Œæ¯å¤©éƒ½åœ¨æŽ¨ç¿»æ˜¨å¤©çš„è‡ªå·±ï¼Œå°è¯•æ–°ä¸œè¥¿ï¼Œä½†æ˜¯æˆ‘çŸ¥é“æˆ‘æ­£åœ¨èµ°ä¸Šå¡è·¯ã€‚ 9. äº‹æƒ…æ˜¯ä¸€ä»¶ä»¶åšå®Œçš„æ¯æ¬¡é¢å‰å †ç€å¾ˆå¤šTo-do listæ—¶ï¼Œæˆ‘éƒ½ä¸€ééè·Ÿè‡ªå·±è¿™æ ·å¼ºè°ƒï¼Œäº‹æƒ…æ˜¯ä¸€ä»¶ä»¶åšå®Œçš„ã€‚å¹³æ—¶å¯ä»¥è®­ç»ƒè‡ªå·±ä¸ºéš¾çš„å¤§çš„äº‹æƒ…æ…¢æ…¢åšé“ºåž«ã€‚ 10. çœ‹ä¸œè¥¿çœ‹æœ¬è´¨ï¼Œä¸è¦å¾’æœ‰è™šè¡¨å¾ˆåæ„Ÿç½‘ä¸Šå¤©å¤©å–ç„¦è™‘çš„é‚£ä¸€å¥—ï¼Œè¯´ä½ å†ä¸å­¦ç¼–ç¨‹ï¼Œå°±è¦è¢«æœºå™¨äººæ·˜æ±°äº†ï¼è¯´çš„è·ŸçœŸçš„ä¼¼çš„ï¼Œéš¾é“ç¼–ç¨‹æ˜¯ä¸ªå…¨æ°‘çš„èŒä¸šå—ï¼Ÿè¿™æ˜¯ä¸€ä¸ªåˆ†å·¥åˆä½œçš„ç¤¾ä¼šï¼Œæ€Žä¹ˆå°±å˜æˆäº†åªéœ€è¦æ•²ä»£ç çš„äº†ï¼Ÿ å¤šæƒ³æƒ³å…¬ä¼—å·ã€å¤§Vä»¬ä¸ºä»€ä¹ˆè¯´è¿™æ ·çš„è¯ä¸ºä»€ä¹ˆå–ç„¦è™‘ï¼Ÿå› ä¸ºä»–æ˜¯å–è¯¾çš„ï¼Œä½ ä¼šçœ‹è§æ–‡ç« æœ€åŽæœ‰ä¸ª999å…ƒçš„pythonè¯¾ç¨‹ã€‚è¿™åŠ¨æœºï¼Œå¾ˆæ˜Žæ˜¾äº†å§ã€‚ç¼–ç¨‹è¿˜å¥½å¤šç§è¯­è¨€å‘¢ï¼Œä½ é—®é—®ä»–å­¦å“ªä¸ªå¥½ã€‚å†™è¿™ç¯‡æ–‡ç« çš„äººä¹Ÿæœªå¿…ä¼šå†™ä¸ªhello worldã€‚ ä¸éœ€è¦æ—¥ç§¯æœˆç´¯çš„åŒ äººç²¾ç¥žçš„ä¸œè¥¿å°±æ˜¯æ²¡ä»€ä¹ˆå¯å¹å˜˜çš„å–é…’æ³¡å§çº¹èº«ã€è·³ä¼žæ½œæµ·å†²æµªï¼Œè¿™äº›çœ‹ä¼¼å¾ˆé…·å…¶å®žé›¶é—¨æ§›ï¼Œå°è¯•ä¸€æ¬¡å°±èƒ½è£…Xçš„äº‹å…¶å®žä¹Ÿå°±é‚£æ ·ã€‚é‚£ç§åˆ°70å²è¿˜åœ¨æ•²ä»£ç åšé¡¹ç›®çš„æ‰é…·ï¼Œé‚£ç§20å¹´å¦‚ä¸€æ—¥ç»™å®¶äººåšé¥­çš„å¦ˆå¦ˆä»¬æ‰é…·ï¼Œé‚£ç§åšæŒåšä¸€ä»¶äº‹çš„äººï¼Œæ‰é…·ã€‚è¦çŸ¥é“ä»€ä¹ˆå«å†·é™ï¼Œä»€ä¹ˆå«æ²‰æ·€ï¼Œä»€ä¹ˆå«ä¸æµ®èºï¼Œä»€ä¹ˆå«åšæŒã€‚ 11. ä½ æœ‹å‹åœˆå†åŽ‰å®³ï¼Œåˆæ€Žä¹ˆæ ·å‘¢å†åŽ‰å®³éƒ½æ˜¯åˆ«äººçš„åŽ‰å®³ï¼Œåªè¦è‡ªå·±ä¸åŽ»åšï¼Œå°±ç®—æœ‹å‹åœˆéƒ½æ˜¯å¤§ç‰›ï¼Œåˆèƒ½æ²¾å¤šå°‘å…‰å‘¢ã€‚è€Œä¸”å¤§ç‰›ä»¬åˆä¸æ˜¯å‚»å­ã€‚å¾ˆå¤šäººå–œæ¬¢æŠ±å¤§è…¿ï¼Œè®©åˆ«äººæŠŠä½ çš„äº‹ä¹Ÿåšäº†ã€‚æŠ±çš„äº†ä¸€æ—¶ï¼Œä¸å¦‚è‡ªå·±æˆä¸ºå¤§è…¿ã€‚æŽŒæ¡è‡ªå·±äººç”Ÿå‘½è„‰çš„æ„Ÿè§‰çœŸçš„å¾ˆå¥½ã€‚ 12. åšä½ å–œæ¬¢çš„äº‹ï¼Œä½ ä¼šå‘å…‰æ‰¾åˆ°è‡ªå·±æƒ³åšçš„äº‹ï¼Œå¾ˆå®¹æ˜“ã€‚æ‰¾åˆ°åŽåšæŒåŽ»åšè¿™ä»¶äº‹ï¼Œä¸å®¹æ˜“ã€‚æ‰€ä»¥æ‰¾ä¸€æ ·ç»ˆèº«çƒ­çˆ±çš„äº‹æƒ…ï¼Œå¯ä»¥èŠ±ä¸€è¾ˆå­åŽ»åšçš„äº‹æƒ…ï¼Œå¾ˆé‡è¦ã€‚Nobody says it was easy, but itâ€™s important.äººæ˜¯è¦æœ‰ä¸»å¿ƒéª¨çš„ï¼ŒçŸ¥é“åœ¨å“ªé‡Œæ‰¾åˆ°è‡ªå·±ï¼Œé‡Šæ”¾è‡ªå·±ã€‚ 13. ç»å¸¸è®°å½•è‡ªå·±çš„æƒ³æ³•å› ä¸ºå°±æ˜¯é‚£ä¹ˆä¸€ä¸‹å­ï¼Œä½ æ„Ÿè§‰å—åˆ°äº†ç¥žçš„æŒ‡ç‚¹ï¼Œä½†æ˜¯ä½ ä»¥åŽå´å†ä¹Ÿæƒ³ä¸èµ·æ¥äº†ã€‚æˆ‘è‡ªå·±æœ‰å†™æ—¥è®°çš„ä¹ æƒ¯ï¼Œå­¦ä¹ ç®—æ³•æ—¶ç»å¸¸åšç¬”è®°ï¼Œè€Œä¸”æ¯ä¸ªæœˆã€ç”šè‡³æ¯å¤©éƒ½æœ‰to-do-listã€‚åˆ—è¡¨ä¸Šä¸€å®šæœ‰æˆªæ­¢æ—¥æœŸï¼Œå¯ä»¥æ‹–å»¶ï¼Œä½†ä¸€å®šä¼šåŽ»åšå®Œã€‚è®°å½•æƒ³æ³•ï¼Œæ˜¯ä¸€ä¸ªä¸Žè‡ªå·±å¯¹è¯çš„è¿‡ç¨‹ï¼Œæ˜¯ä¸€ä¸ªäº†è§£ã€æˆé•¿çš„è¿‡ç¨‹ã€‚ 14. çŠ¯å›°äº†å°±è¶´ä¸‹ç¡è§‰ä¸è¦å¼ºè¿«è‡ªå·±ä¸ç¡ï¼Œåˆ«æªå¤´å‘ï¼Œåˆ«æ‹¿ç¬”æˆ³å¤§è…¿ã€‚å¦‚æžœèƒ½åšåˆ°æ¸…é†’ä½ æ—©å°±ä¸çžŒç¡äº†ã€‚ 15. ä¸è¦è½»æ˜“ç›¸ä¿¡äººä¸–ç•ŒçœŸçš„å¾ˆå¤§ï¼Œè¿œæ¯”ä½ æƒ³çš„é‚ªæ¶çš„å¤šï¼Œè¿œæ¯”ä½ æƒ³çš„ç¾Žå¥½çš„å¤šï¼Œæˆ‘è§‰å¾—å¥½äººæ›´å¤šã€‚åŒæ—¶ï¼Œä¸–ç•Œä¸Šæœ‰å¾ˆå¤šåäººï¼Œä¸è¦å¿æ°”åžå£°ã€‚ 16. ä¸“ä¸šçŸ¥è¯†å¾ˆé‡è¦æœ‰ä¸€æŠ€ä¹‹é•¿å¾ˆé‡è¦ï¼Œè‡³å°‘å¯¹æˆ‘æ¥è¯´æ˜¯è¿™æ ·çš„ã€‚å¦‚æžœå†ç»™æˆ‘ä¸€ä¸ªæœºä¼šï¼Œæˆ‘è¿˜æ˜¯ä¼šçˆ±æŸ¯è¥¿å®šç†çˆ±æ‹‰æ ¼æœ—æ—¥ä¸­å€¼å®šç†çˆ±ç‰›é¡¿-èŽ±å¸ƒå°¼èŒ¨çˆ±é«˜æ–¯è¿‡ç¨‹çˆ±æ•°å­¦ï¼Œåˆ·å¤šå°‘è¯¾åŽé¢˜æˆ‘éƒ½æ„¿æ„ã€‚è„‘å­æ˜¯ä¸ªå¥½ä¸œè¥¿ï¼Œè¶Šç”¨è¶Šæœ‰æ„æ€ã€‚æœ‰ä¸€ä¸ªè¯´æ³•æ˜¯è„‘å­åˆ†ä¸¤ä¸ªåŒºï¼šå¿«æ€è€ƒå’Œæ…¢æ€è€ƒã€‚ç¬¬ä¸€ä¸ªæ˜¯ç›´æŽ¥åæ˜ ï¼Œåƒæ¡ä»¶åå°„ï¼Œæ¯”å¦‚é—®ä½ 1+1ç­‰äºŽå‡ ã€‚ç¬¬äºŒä¸ªæ˜¯éœ€è¦é›†ä¸­æ³¨æ„åŠ›ï¼Œæ¯”å¦‚23Ã—12ç­‰äºŽå‡ ã€‚æŽ¨èä¸€æœ¬ä¹¦ï¼ŒThink fast and slow.æå‡ä¸“ä¸šçŸ¥è¯†æ„å‘³ç€ï¼Œç»å¸¸åšä¸€äº›éœ€è¦ç¬¬äºŒè„‘å­çš„æ€è€ƒã€‚å¾ˆåæ„Ÿç”¨å‡­ç›´è§‰æ¥è¯´äº‹çš„ã€‚ 17. å¤šæ€è€ƒæ­»äº¡60å¹´ï¼Œè½¬çž¬å³é€ã€‚ç­‰ä½ 60å²çš„æ—¶å€™ä½ å°†ä¼šè¿™æ ·å‘Šè¯‰è‡ªå·±ã€‚å¤šæ€è€ƒæ­»äº¡çš„å¥½å¤„æ˜¯ï¼Œä½ ä¼šåå‘çœ‹å¾…ä½ çš„äººç”Ÿï¼Œä½ ä»Žäººä¹‹å°†æ­»çš„é‚£ä¸€åˆ»å›žåˆ°æ´»è¹¦ä¹±è·³çš„çŽ°åœ¨ï¼Œä½ çš„å†…å¿ƒä¼šå……æ»¡æ„Ÿæ¿€ï¼Œä½ ä¸ä¼šè®¡è¾ƒé‚£ä¹ˆå¤šã€‚ä½ ä¼šæ‰¾åˆ°å¯¹ä½ æœ€é‡è¦çš„ä¸œè¥¿ã€‚ä¸€ä¸ªäººåªèƒ½æ´»ä¸€æ¬¡ï¼Œä¸€å®šè¦æ˜Žç™½è¿™ä¸ªæ„å‘³ç€ä»€ä¹ˆã€‚ 18. æ¸…é›¶ï¼Œæ°¸è¿œçªç ´è‡ªå·±å¶åƒï¼šMusk / é›·å†› 19. é’±å¾ˆé‡è¦ï¼Œç†è´¢å¾ˆé‡è¦å¥³æ€§æœ‰ç»æµŽæ”¶å…¥å¾ˆé‡è¦ï¼Œç”šè‡³è¯´ï¼Œå¥³æ€§ä¸€å®šè¦æœ‰è‡ªå·±çš„ç»æµŽæ”¶å…¥ã€‚è¿™æ‰æ˜¯è°ˆå¹³ç­‰çš„æ¡ä»¶ã€‚ 20. èº«ä¸Šæœ€æ€§æ„Ÿçš„åœ°æ–¹ï¼Œæ˜¯è„‘å­ðŸ˜Š21. åšé•¿è¿œè®¡åˆ’ æƒ³æ˜Žå¤©åˆ°è¾¾ï¼Œä»Šå¤©å°±å¾—å‡ºå‘ã€‚å¦‚æžœä½ èƒ½æ˜Žç™½è¿™å¥è¯ï¼Œå°±æ‡‚æˆ‘æƒ³è¯´ä»€ä¹ˆäº†ã€‚ åšé•¿è¿œè®¡åˆ’ï¼Œ3-5å¹´ï¼Œç”šè‡³10å¹´ï¼Œå†å€’æŽ¨çœ‹çœ‹çŽ°åœ¨åº”è¯¥åšä»€ä¹ˆã€‚å¾ˆå¤šäº‹æƒ…ä½ é¢„æƒ³ä¸åˆ°ï¼Œä½†æ˜¯ä½ è¿˜æ˜¯è¦æƒ³ã€‚ 22. å®³æ€•ä»€ä¹ˆå°±ç›´é¢ä»€ä¹ˆé€ƒé¿å¯è€»ä½†æœ‰ç”¨ï¼Œæ›¾ç»é™ªä¼´äº†æˆ‘å¾ˆé•¿ä¸€æ®µæ—¶é—´ã€‚åŽæ¥æˆ‘æ‰å‘çŽ°ï¼Œè¯´è¿™å¥è¯çš„äººæ˜¯æ²¡æœ‰ç›´é¢äº‹å®žçš„å‹‡æ°”ã€‚å› ä¸ºä½ èº²è¿‡çš„ä¸œè¥¿è¿˜æ˜¯ä¼šå›žå¤´æ¥æ‰¾ä½ çš„ã€‚è¿™ç§æƒ…å†µåœ¨æˆ‘åšæ•°å­¦é¢˜ç›®çš„æ—¶å€™å°¤å…¶æ˜Žæ˜¾ï¼Œä¸ä¼šåšçš„é¢˜ä¼šä¸€ç›´å‡ºçŽ°ï¼Œç›´åˆ°æˆ‘çœŸæ­£å¼„æ‡‚ã€‚ç±»æ¯”ç”Ÿæ´»è€ƒé¢˜ã€‚ 23. å¸®åŠ©ä»–äººæ—¶ï¼Œä¸è¦æƒ³å›žæŠ¥å¦‚æžœä½ æƒ³å¸®åˆ«äººï¼Œä¸€å®šæ˜¯è¦å‡ºäºŽä½ å®Œå…¨è‡ªæ„¿ï¼Œä¸è¦æƒ³ç€äººå®¶æ¥æ—¥æ¶Œæ³‰ç›¸æŠ¥ã€‚çœçœå§ï¼Œå¾ˆå¤šæƒ…å†µæ ¹æœ¬ä¸ä¼šæœ‰ä»»ä½•å›žéŸ³ï¼Œç”šè‡³è°¢è°¢éƒ½æ²¡æœ‰ã€‚ä½ è¦äº«å—å¸®åŠ©åˆ«äººçš„è¿™ä¸ªè¿‡ç¨‹ï¼Œè¿™æ˜¯ä½ åšè¿™ä¸ªåŠ¨ä½œçš„å”¯ä¸€å›žæŠ¥ã€‚å¦‚æ­¤æ²¡æœ‰æœŸå¾…ï¼Œä½ æ‰ä¸ä¼šå¤±æœ›ã€‚ 24. çˆ¶æ¯èƒ½é™ªä¼´ä½ çš„æ—¶é—´æ˜¯æœ‰é™çš„æ¯æ¬¡æƒ³åˆ°è¿™ä¸ªçœŸçš„ä¼šå¾ˆæ— åŠ›ã€‚æ•²å®Œè¿™ä¸€å¥ï¼Œå°±åŽ»å®šå›žå®¶çš„æœºç¥¨ã€‚ 25. æ­¤ç”Ÿï¼Œæ— éœ€ä»–äººåº‡ä½‘]]></content>
      <tags>
        <tag>life lesson</tag>
        <tag>birthday</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æˆ‘å–œæ¬¢çš„ä¸ªäººç½‘ç«™]]></title>
    <url>%2F2018%2F07%2F12%2F%E6%88%91%E5%96%9C%E6%AC%A2%E7%9A%84%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99-updating%2F</url>
    <content type="text"><![CDATA[æˆ‘æ­å»ºä¸ªäººç½‘ç«™çš„åˆè¡·æ˜¯å› ä¸ºï¼š å·¥ä½œé€‰æ‹©äº†æœºå™¨å­¦ä¹ æ–¹å‘ï¼Œæ—¥å¸¸ç†è®ºå­¦ä¹ ï¼Œå†™ä»£ç ï¼Œæœ‰ä¸€ä¸ªè®°å½•çš„è¿‡ç¨‹å¾ˆé‡è¦ æ²¡æœ‰çœŸæ­£å¼„æ‡‚çš„é—®é¢˜ä¼šé‡å¤é‡åˆ°ï¼Œç¿»çœ‹è‡ªå·±åŽŸæ¥çš„ç¬”è®°çš„é¢‘çŽ‡éžå¸¸é«˜ å–œæ¬¢æœ‰è®¾è®¡æ„Ÿæœ‰åˆ›æ„æœ‰é£Žæ ¼çš„äº‹ç‰©ï¼Œå–œæ¬¢ä¹±æ¶‚ä¹±ç”»ä¹±å†™ï¼Œå–œæ¬¢ä»Ž0åˆ°1 éœ€è¦æœ‰ä¸€ä¸ªæ·±åº¦æ€è€ƒçš„åœ°æ–¹ ä¹‹å‰æœ‰å†™æ–‡ç« ä»‹ç»å¦‚ä½•æ­å»ºè‡ªå·±çš„ä¸ªäººç½‘ç«™ï¼šðŸ”—å¦‚ä½•æ­å»ºè‡ªå·±çš„ä¸ªäººç½‘ç«™ï¼ˆä¸Šï¼‰ðŸ”—å¦‚ä½•æ­å»ºè‡ªå·±çš„ä¸ªäººç½‘ç«™ï¼ˆä¸‹ï¼‰ å°±åœ¨å‚è€ƒåˆ«äººçš„å­¦ä¹ ç¬”è®°çš„è¿‡ç¨‹ä¸­ï¼Œå‘çŽ°å¥½å†…å®¹å¾ˆå¤šéƒ½æ¥æºäºŽä»–ä»¬çš„ä¸ªäººç½‘ç«™æˆ–è€…CSDNã€‚è€Œä¸”ï¼Œä¸ªäººç½‘ç«™çš„ä½œç”¨ä¸å•å•æ˜¯ç†è®ºå­¦ä¹ ç¬”è®°ï¼Œä¹Ÿå¯ä»¥æœ‰è‡ªå·±ç”Ÿæ´»çš„è®°å½•ï¼Œå’Œå…¶ä»–å†…å®¹çš„åˆ†äº«ã€‚å½“è‡ªå·±æœ‰æ„è¯†çš„åŽ»è§‚å¯Ÿåˆ«äººçš„ç½‘ç«™åŽï¼Œé‡è§ä¼˜ç§€çš„ä¸ªäººç½‘ç«™åƒæ˜¯æŒ–åˆ°å®ä¸€æ ·å¼€å¿ƒðŸ˜è™½ç„¶å¾ˆå¤šå†…å®¹è¿˜æ˜¯åŸºäºŽæœºå™¨å­¦ä¹ çš„ã€‚å…¥é€‰çš„åŽŸå› ï¼šå¯èƒ½æ˜¯é¡µé¢è®¾è®¡å¾ˆæŠ“äººï¼Œå¯èƒ½æ˜¯æ–‡å­—è´¨é‡ç‰¹åˆ«é«˜ï¼Œå¯èƒ½æ˜¯ç”Ÿæ´»ä¸‰è§‚æœ‰å…±é¸£ï¼Œæ€»ä¹‹éƒ½æ˜¯å€¼å¾—å­¦ä¹ çš„~ä¸æƒ³å·å·ç§è—ï¼Œæ‹¿å‡ºæ¥è·Ÿå¤§å®¶ä¸€èµ·åˆ†äº«~ å¦‚æžœä½ æ˜¯ï¼š æƒ³æˆä¸ºä¸€ä¸ªæ‹¥æœ‰é²œæ˜Žé£Žæ ¼çš„ä¸ªäººç½‘ç«™åšä¸» å–œæ¬¢æ¬£èµç¾Žå¥½çš„äº‹ç‰© å–œæ¬¢ä¸æ–­çš„æŽ¢ç´¢è‡ªå·± é‚£ä¹ˆè¿™ç¯‡æ–‡ç« å°±å¾ˆé€‚åˆä½ äº†â¤ æ­£æ–‡ï¼šä¸»è¦ä»¥ã€ç½‘ç«™é“¾æŽ¥ã€‘-ã€ç®€ä»‹ã€‘-ã€å¸å¼•æˆ‘çš„åœ°æ–¹ã€‘æ¥ä»‹ç»å®ƒä»¬ ä¸ªäººé£Žæ ¼æ¯”è¾ƒå¼ºçƒˆçš„ 1. ðŸ”—pluskid.orgè¿™æ˜¯ä¸€ä¸ªå„ç§å¼€å¤–æŒ‚çš„MITé«˜æç”Ÿåšä¸»:å­¦æœ¯ã€æ—…è¡Œã€ç»˜ç”»ã€æ‘„å½±ã€å†™è¯—â€¦ ðŸ”—freemind.pluskid.orgè¿™æ˜¯ä»–çš„æŠ€æœ¯åšå®¢ï¼Œä¸»è¦è®°å½•ä¸€äº›æœºå™¨å­¦ä¹ ã€ç¨‹åºè®¾è®¡å†…å®¹ã€‚æ”¾å‡ºä¸€ç¯‡è¢«å®žåŠ›åœˆç²‰çš„æ–‡ç« ï¼š2017 é£Žè¡£ ä¸‹é¢æ˜¯ä»–åœ¨æ—…é€”ä¸­çš„travel sketches:ä¸‹é¢æ˜¯ä»–çš„å¹³æ—¶ä¸€äº›ç»˜ç”»å’Œæ‘„å½±ä½œå“ï¼š 2. ðŸ”—www.kennethreitz.orgè¿™ä½å…„å¼Ÿå› ä¸ºä»–çš„å‡è‚¥åŠ±å¿—æ•…äº‹å·²ç»åœ¨æŠ€æœ¯åœˆå†…å¤–éƒ½å¤§åé¼Žé¼Žäº†ï¼Œä¸‹é¢æ˜¯å†™ä»–çš„ä¸€äº›æ–‡ç« ï¼šæœ‰æ„æ€æ˜¯ä»–çš„èœå•æ ï¼Œè¡¨æ˜Žäº†ä»–çš„å¤šé‡èº«ä»½ï¼šç¨‹åºå‘˜ã€æ‘„å½±å¸ˆã€å‡ºä¸“è¾‘ã€æ‹è§†é¢‘ã€æ¼”è®²ã€å‘è¡¨è®ºæ–‡ã€æ—…è¡Œã€è¿˜æœ‰vlogï¼ŒçœŸå¿ƒå¾ˆæ½®äº†è€Œä¸”æ¯ä¸ªæ¨¡å—çš„å†…å®¹éƒ½ç›¸å½“çš„ä¸°å¯Œï¼ 3. ðŸ”—www.ruanyifeng.com é˜®ä¸€å³°çš„ä¸ªäººç½‘ç«™ç¬¬ä¸€æ¬¡å¯¹é˜®ä¸€å³°è€å¸ˆäº§ç”Ÿå¥½å¥‡å¿ƒæ˜¯æœ‰æœ‹å‹è¯´æˆ‘æ˜¯å¥³ç‰ˆé˜®ä¸€å³°ðŸ˜‚ï¼Œæ€»æ„Ÿè§‰è¿™ä¸ªåå­—å¥½åƒå“ªé‡Œè§è¿‡ã€‚å¾ˆå¿«å°±å‘çŽ°æ—©åœ¨é˜…è¯»ã€Šé»‘å®¢å’Œç”»å®¶ã€‹çš„æ—¶å€™å°±åœ¨å°é¢è§è¿‡é˜®ä¸€å³°è€å¸ˆçš„åå­—ï¼Œè™½ç„¶æˆ‘å¼ºè¿«è‡ªå·±è¯»çš„æ˜¯è‹±æ–‡ç‰ˆã€‚ä¸ºä»€ä¹ˆç‰¹åˆ«å°Šæ•¬é˜®ä¸€å³°è€å¸ˆæ˜¯å› ä¸ºï¼Œé‡ç‚¹çœ‹ä¸‹å›¾çš„å‡ ä¸ªæ•°å­—ï¼š 1710ç¯‡æ–‡ç«  å¹³å‡ä¸¤ä¸‰å¤©æ›´æ–°ä¸€ç¯‡æ–‡ç«  2æœ¬ä¹¦ï¼šã€Šæœªæ¥ä¸–ç•Œçš„å¹¸å­˜è€…ã€‹å’Œã€Šå‰æ–¹çš„è·¯ã€‹ 16ä¸ªæ–‡ç« åˆ†ç±»ï¼šç®—æ³•ä¸Žæ•°å­¦ã€åˆ›ä¸šã€ç§‘æŠ€ã€è‹±è¯­ã€å¼€å‘è€…æ‰‹å†Œã€æ•£æ–‡â€¦å‡­è‰¯å¿ƒè¯´ï¼Œæ¯ç¯‡æ–‡ç« çš„è´¨é‡éƒ½åœ¨çº¿ï¼Œçœ‹åº•ä¸‹çš„è¯„è®ºæ•°å°±çŸ¥é“äº†ã€‚è‡³äºŽæŽ’ç‰ˆè®¾è®¡â€¦emâ€¦äººæ— å®ŒäººðŸ˜‚ æŽ¥ä¸‹æ¥æ˜¯ä¸€äº›åˆ—æœºå™¨å­¦ä¹ ã€ç¨‹åºè®¾è®¡ç­‰å¼ºç›¸å…³çš„æŠ€æœ¯åšå®¢ï¼šå¦‚æžœæ¯ä¸ªåšå®¢éƒ½è¿›è¡Œå›¾æ–‡ä»‹ç»ä¼šé€ æˆç¾éš¾çš„ï¼Œæ‰€ä»¥ä¸‹é¢çš„æŠ€æœ¯åšå®¢å°±åªæ”¾é“¾æŽ¥å’Œç®€çŸ­ä»‹ç»äº†ï¼Œç›¸ä¿¡æˆ‘çš„è¯å°±è¯·æ”¾å¿ƒçš„ç‚¹å¼€å§ï¼ ðŸ”—dnc1994.com ï¼Œåœ¨ Kaggle é¦–æˆ˜ä¸­è¿›å…¥å‰ 10% ðŸ”—godweiyang.com ï¼Œæœºå™¨å­¦ä¹ ï¼Œæ•°å­¦è¯¾ç¨‹ç¬”è®° ðŸ”—geekplux.comï¼Œä¼˜ç§€çš„å‰ç«¯ï¼Œä¼˜ç§€çš„å¯è§†åŒ–é¡¹ç›® ðŸ”—redstonewill.github.io ï¼Œä¸»è¦ä»‹ç»å´æ©è¾¾è¯¾ç¨‹ç¬”è®°ï¼Œæœ‰å…¬ä¼—å· ðŸ”—liuchi.coding.me ï¼Œä¼˜ç§€çš„é¢ç»å†™æ‰‹ ðŸ”—d0evi1.com ï¼Œæ·±åº¦å­¦ä¹ ï¼Œå†™çš„å¾ˆä»”ç»† ðŸ”—yihui.name ï¼Œè°¢ç›Šè¾‰ï¼Œå°±èŒäºŽ RStudioï¼Œabouté¡µé¢å†™çš„å¾ˆæœ‰è¶£ ðŸ”—shuang0420.com ï¼Œå¾é˜¿è¡¡ï¼Œå¤§é‡NLPå­¦ä¹ ç¬”è®° ðŸ”—yongyuan.name ï¼Œä¸ªäººç½‘ç«™æœ‰ç®€åŽ†å¯å‚è€ƒï¼Œå›¾åƒå¤„ç†æ–¹å‘ ðŸ”—WILDML ï¼ŒDenny Britzï¼ŒGoogle Brain Teamï¼Œ æ·±åº¦å­¦ä¹ ï¼ŒNLP ðŸ”—PHILIPP SINGER ï¼ŒSenior Data Scientistï¼Œkaggleå¤§ç¥ž ðŸ”—Andrej Karpathy ï¼ŒDirector of AI at Teslaï¼ŒStanford Computer Science Ph.D. student ðŸ”—colahâ€™s blog ï¼Œå¯¹æœºå™¨å­¦ä¹ çš„ä¸€äº›æ¦‚å¿µè§£é‡Šçš„å¾ˆæ¸…æ¥šï¼Œå°¤å…¶LSTMé‚£ç¯‡ ðŸ”—å°åœŸåˆ€ ï¼Œåˆ€ç¥žçš„ç½‘ç«™ ðŸ”—Daniel Balle ï¼Œå¹²å‡€çš„ç½‘ç«™ï¼Œåªå‰©ç®€åŽ†å’Œä½œå“ ðŸ”—Hux Blog ï¼Œå‰ç«¯å·¥ç¨‹å¸ˆï¼Œç½‘ç«™å¾ˆå¥½çœ‹ï¼Œå¼€å‘äº†hexoä¸»é¢˜ ðŸ”—reuixiy ï¼Œæ„Ÿè§‰æ˜¯æŒºä¼šæŠ˜è…¾çš„ä¸€ä¸ªåšä¸» ðŸ”—liaopeiyuan.github.io ï¼Œè¢«å°é¢å¸å¼• ðŸ”—Nadieh Bremer ï¼ŒæƒŠè‰³çš„ä¸ªäººæ•°æ®å¯è§†åŒ–åˆ†æžå¸ˆï¼Œæœ‰ç©ºä¸€å®šè¦å­¦ä¹ ä¸€ä¸‹ å†™åœ¨æœ€åŽ å»ºç«‹ç½‘ç«™æ˜¯æœ€ç®€å•çš„äº‹ï¼Œè°éƒ½å¯ä»¥èŠ±ä¸¤ä¸‰ä¸ªå°æ—¶æ­ä¸€ä¸ªã€‚éš¾çš„æ˜¯ç»´æŠ¤ç½‘ç«™çš„è¿‡ç¨‹ã€‚å¾ˆå¤šè‹¦å¿ƒç»è¥çš„ä¸ªäººç½‘ç«™éƒ½æ˜¯æ¯”è¾ƒå‡„å‡‰ï¼Œæ— äººé—®æ´¥ï¼Œå¯¼è‡´å¾ˆå¤šä¼˜ç§€çš„ä¸ªäººç½‘ç«™çš„åœæ›´ã€‚æ˜¾ç„¶çš„ï¼Œå¤§ä¼—éƒ½å–œæ¬¢ç¢Žç‰‡çŸ­æ–‡ï¼Œæ¯”å¦‚å¾®åšï¼Œæ¯”å¦‚å¾®ä¿¡å…¬ä¼—å·ã€‚ä½†æˆ‘è¿˜æ˜¯è®¤ä¸ºæ·±åº¦æ€è€ƒå¾ˆé‡è¦ï¼Œä½ è¿˜è®°ä¸è®°å¾—é‚£ä¸ªååœ¨å›¾ä¹¦é¦†ä¸€ä¸ªæœˆç†¬å‡ºä¸€ç¯‡è®ºæ–‡çš„ä½ ï¼Ÿ åªæœ‰å–œæ¬¢çš„äº‹æƒ…æ‰ä¼šåšæŒä¸æ–­çš„åŽ»åšï¼Œå¸Œæœ›è‡ªå·±èƒ½åœ¨ç½‘ç«™ç»´æŠ¤è¿™ä»¶äº‹ä¸ŠåšæŒä¸‹åŽ» åšè¿‡çš„äº‹ä¸ä¼šæ²¡æœ‰ç”¨ï¼ŒåŠŸä¸å”æ è¿˜æœ‰ï¼Œè°è¯´æžæŠ€æœ¯çš„éƒ½æ˜¯å‘†å­ï¼Œå•ªå•ªå•ªæ‰“è„¸ å…³æ³¨è¿™äº›ä¸ªäººç½‘ç«™çš„å¥½å¤„æœ‰å¾ˆå¤šï¼šå¯ä»¥çœ‹çœ‹åŒè¡Œæˆ–è€…ä½ å–œæ¬¢çš„äººéƒ½åœ¨å¿™ä»€ä¹ˆï¼Œä¸‹ä¸€æ­¥æœ‰ä»€ä¹ˆæ‰“ç®—ï¼Œç”šè‡³å¯ä»¥äº’ç›¸åšæŒé¼“åŠ±ã€‚æˆ‘å°±å¾ˆæœŸå¾…è®¤è¯†è¿™äº›ç½‘é¡µèƒŒåŽçš„äººðŸ˜Š ä¼šæŒç»­æ›´æ–°çš„â€¦ä¸‹ä¸€ä¸ªä¸Šæ¦œçš„ä¼šæ˜¯è°å‘¢ï¼Ÿ å†™åœ¨æœ€æœ€åŽç›®å‰å…³äºŽç½‘ç«™æŽ¨èçš„æ›´æ–°å·¥ä½œå·²ç»æŒªè‡³â€œFriendsâ€é¡µé¢ï¼Œæœ¬æ–‡ä¸å†æ›´æ–°ï¼Œä¼ é€ï¼šå‹æƒ…é“¾æŽ¥]]></content>
      <tags>
        <tag>personal website</tag>
        <tag>design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å‚åŠ CCF-GAIR(2018)çš„ä½“éªŒæ€»ç»“]]></title>
    <url>%2F2018%2F07%2F10%2F%E5%8F%82%E5%8A%A0CCF-GAIR-2018-%E7%9A%84%E4%BD%93%E9%AA%8C%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[warming-up video, æŽ¨èè§‚çœ‹~â¤ CCF-GAIRï¼Œå…¨çƒäººå·¥æ™ºèƒ½ä¸Žæœºå™¨äººå³°ä¼šï¼Œç”±ä¸­å›½è®¡ç®—æœºå­¦ä¼šï¼ˆCCFï¼‰ä¸»åŠžï¼Œé›·é”‹ç½‘ã€é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰æ‰¿åŠžã€‚ ä¸€å¹´ä¸€åº¦çš„äººå·¥æ™ºèƒ½åˆ†äº«å¤§ä¼šï¼Œ2018å¹´æ˜¯ç¬¬ä¸‰å±Šã€‚ CCF-GAIR 2018 æä¾›1ä¸ªä¸»ä¼šåœºå’Œ11ä¸ªä¸“åœºï¼š 2018å±Šå‚ä¼šçš„ä»·å€¼ï¼š ä¸‰å¤©çš„è¡Œç¨‹ï¼Œ11ä¸ªä¸“åœºï¼ŒåŸºæœ¬æ¶µç›–ç›®å‰äººå·¥æ™ºèƒ½çš„åº”ç”¨åœºæ™¯ æ¯ä¸ªä¸“åœºçš„æ¼”è®²å†…å®¹å¾ˆåŽšå®žï¼Œ100å¤šä½æ¼”è®²å˜‰å®¾éƒ½æ˜¯å®žæ‰“å®žçš„åŽ‰å®³ï¼Œå›¾çµå¥–å¾—ä¸»ã€ä¸­ç§‘é™¢é™¢å£«åŠè—¤æ ¡æ•™æŽˆç­‰å…·ä½“çš„å˜‰å®¾å¯å‚é˜…ï¼šCCF-GAIR 2018 å®˜ç½‘ æŽ¥ä¸‹æ¥å°±æŒ‰ç…§æˆ‘pickçš„ä¸“åœºæ¥è¿›è¡Œå­¦ä¹ å’Œæ€»ç»“ï¼š æ­£æ–‡ - å‚åŠ  CCF-GAIR çš„ç¬”è®°[AIå‰æ²¿ä¸“åœº] ===========================èµ°å‘çœŸæ­£çš„äººå·¥æ™ºèƒ½ ï¼ˆå¼ é’¹ï¼Œä¸­å›½ç§‘å­¦é™¢é™¢å£«ï¼Œæ¸…åŽå¤§å­¦æ•™æŽˆï¼‰ä¸­å¿ƒæ€æƒ³ï¼šçŽ°åœ¨ç¦»çœŸæ­£çš„äººå·¥æ™ºèƒ½è¿˜æœ‰ä¸€æ®µå¾ˆé•¿çš„è·¯ç›®å‰äººå·¥æ™ºèƒ½å–å¾—æˆåŠŸçš„å› ç´ ï¼š å¤§æ•°æ® è®¡ç®—èƒ½åŠ›æé«˜ ä¼˜ç§€çš„ç®—æ³• å¿…é¡»å»ºç«‹åœ¨åˆé€‚çš„åº”ç”¨åœºæ™¯ä¸‹ ç›®å‰çš„äººå·¥æ™ºèƒ½çš„å®žçŽ°æœ‰5ä¸ªé™åˆ¶ï¼šï¼ˆé‡ç‚¹ï¼ï¼ðŸ“ï¼‰ having rich data or knowledgeä¸°å¯Œçš„æ•°æ®æˆ–è€…çŸ¥è¯†ï¼ˆå…³äºŽè¿™ç‚¹ä¸èƒ½åŒæ„æ›´å¤šï¼æ²¡æ•°æ®å°±å«ä½ å»ºæ¨¡çš„éƒ½æ˜¯è€æµæ°“ï¼ï¼ï¼‰ certain information ç¡®å®šæ€§ä¿¡æ¯ perfect information å®Œå…¨ä¿¡æ¯ â€˜staticâ€™ é™æ€çš„ single task and finite domain å•ä»»åŠ¡ï¼Œæœ‰é™é¢†åŸŸï¼ˆAlphaGoä¹Ÿåªä¼šä¸‹è±¡æ£‹è€Œå·²å˜›ï¼äººèƒ½åšçš„å¤ªå¤šå¤ªå¤šäº†ï¼‰ å¼ é’¹ï¼š å¤§å®¶å¸¸å¸¸å…³å¿ƒä»€ä¹ˆæ ·çš„å·¥ä½œä¼šè¢«æœºå™¨æ‰€æ›¿ä»£ï¼Œæˆ‘å¯ä»¥æ˜Žç¡®å‘Šè¯‰å¤§å®¶ï¼Œæ»¡è¶³è¿™ 5 ä¸ªæ¡ä»¶çš„å·¥ä½œï¼Œæ€»æœ‰ä¸€å¤©ä¼šè¢«è®¡ç®—æœºå–ä»£ï¼Œå°±æ˜¯é‚£äº›ç…§ç« åŠžäº‹ï¼Œä¸éœ€è¦ä»»ä½•çµæ´»æ€§çš„å·¥ä½œã€‚ ä¸ºä»€ä¹ˆæœ‰è¿™ 5 ä¸ªé™åˆ¶ï¼ŸåŽŸå› åœ¨äºŽæˆ‘ä»¬çŽ°åœ¨çš„äººå·¥æ™ºèƒ½æ˜¯æ²¡æœ‰ç†è§£çš„äººå·¥æ™ºèƒ½ã€‚ æ™ºèƒ½ä½“çŽ°åœ¨æŽ¨ç†èƒ½åŠ›ä¸Šã€‚ä½†æ˜¯å¾ˆä¸å¹¸ï¼ŒçŽ°åœ¨çš„å¯¹è¯ç³»ç»ŸæŽ¨ç†èƒ½åŠ›éƒ½å¾ˆå·®ã€‚åŸºæœ¬åšæ³•æ˜¯å»ºç«‹ä¸€ä¸ªå¸¸è¯†å›¾è°±ï¼Œç”¨è¿™ä¸ªå›¾è°±å¸®åŠ©ç†è§£æå‡ºçš„ã€Œé—®é¢˜ã€ï¼ŒåŒæ—¶åˆ©ç”¨å¸¸è¯†å›¾è°±å¸®åŠ©äº§ç”Ÿåˆé€‚çš„ç­”æ¡ˆã€‚ è¿™ç§é€šè¿‡æ•°æ®é©±åŠ¨åšå‡ºæ¥çš„ç³»ç»Ÿï¼Œå®ƒçš„æ€§èƒ½è·Ÿäººç±»å·®åˆ«éžå¸¸å¤§ï¼Œé²æ£’æ€§å¾ˆå·®ï¼Œå¾ˆå®¹æ˜“å—å¹²æ‰°ï¼Œä¼šå‘ç”Ÿé‡å¤§çš„é”™è¯¯ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ ·æœ¬ã€‚ è¿™æ ·çš„ç³»ç»Ÿåªæ˜¯ä¸€ä¸ªæœºæ¢°çš„åˆ†ç±»å™¨ï¼Œæ ¹æœ¬ä¸æ˜¯æ„ŸçŸ¥ç³»ç»Ÿã€‚ äººç±»çš„æœ€å¤§çš„ä¼˜ç‚¹æ˜¯ã€Œå°é”™ä¸æ–­ã€å¤§é”™ä¸çŠ¯ã€ï¼Œæœºå™¨æœ€å¤§çš„ç¼ºç‚¹æ˜¯ã€Œå°é”™ä¸çŠ¯ï¼Œä¸€çŠ¯å°±çŠ¯å¤§é”™ã€ å¤§å®¶ä¸ºä»€ä¹ˆè¿™ä¹ˆé‡è§†äººå·¥æ™ºèƒ½ï¼Ÿå› ä¸ºæˆ‘ä»¬æ°¸è¿œåœ¨è·¯ä¸Šï¼Œè¿™å°±å¸å¼•æˆ‘ä»¬åŽ»è§£å†³è¿™äº›é—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä¸€æ—¦è§£å†³äº†ï¼Œäººç±»çš„ç¤¾ä¼šè¿›æ­¥ã€äººç±»çš„ç”Ÿæ´»å°±ä¼šå‘ç”Ÿæœ¬è´¨ä¸Šçš„æ”¹å˜ã€‚ AIèµ‹èƒ½ï¼Œäººæœºå…±èž ï¼ˆå¼ å»ºä¼Ÿï¼Œå¾·å›½æ±‰å ¡ç§‘å­¦é™¢é™¢å£«ï¼Œæ±‰å ¡å¤§å­¦å¤šæ¨¡å¼ç³»ç»Ÿç ”ç©¶æ‰€æ‰€é•¿ï¼Œå›½å®¶åƒäººè®¡åˆ’ä¸“å®¶ï¼‰å•æ¨¡æ€å­¦ä¹ åº”ç”¨ ï¼ˆæŽ¨èã€Šæœºæ™ºè¿‡äººã€‹èŠ‚ç›®ï¼Œå¯“æ•™äºŽä¹ï¼‰ å›¾åƒå¤„ç†ä¸­ï¼Œå¯¹äºŽæ¨¡ç³Šçš„å›¾åƒï¼Œäººç±»åœ¨æ¨¡ç³Šä¿¡æ¯ä¸‹è¿ç”¨çŸ¥è¯†ã€è¿ç”¨å¤–æŽ¨çš„èƒ½åŠ›å¼ºäºŽæœºå™¨äºº çœ‹å›¾å†™è¯—ï¼šå¾®è½¯æœºå™¨äººå°å†° å£°éŸ³çš„å•æ¨¡æ€å­¦ä¹  æœºå™¨äººé˜…ç‰‡ï¼ŒåŒ»ç–—è¿ç”¨ï¼Œæœºå™¨äººåœ¨æœ‰é™çŽ¯å¢ƒé‡Œçš„å¤§æ•°æ®å­¦ä¹ èƒ½åŠ›éžå¸¸å¼º è·¨æ¨¡æ€å­¦ä¹ åº”ç”¨ è·¨æ¨¡æ€çš„åŠ¨æ€é€‚åº”æœºåˆ¶ä¾‹å¦‚ï¼Œé€šè¿‡å‘çŽ°è€é¼ åœ¨å­¦ä¹ å‰å’Œå­¦ä¹ åŽçš„ç¥žç»å…ƒçš„å˜åŒ–ï¼Œå¸Œæœ›æ€»ç»“å‡ºæœªæ¥æ›´å¥½çš„å¸¦æœ‰å±€éƒ¨è®°å¿†çš„æ·±åº¦ç¥žç»æ¨¡åž‹ã€‚ è·¨æ¨¡æ€çš„æ³›åŒ–å’Œé¢„æµ‹ è·¨æ¨¡æ€çš„äººæœºäº¤äº’æ–¹é¢å¦‚ä½•è®©æœºå™¨äººé€šè¿‡è§†è§‰ã€è¯­è¨€çš„å…±åŒå­¦ä¹ ï¼Œæ›´å¥½åœ°ç†è§£æ¦‚å¿µï¼Œç†è§£ä»–ä»¬ä¸­é—´çš„å…³ç³»ã€‚ é€šè¿‡å¤šæ¨¡æ€çš„å­¦ä¹ ï¼ŒåŒ…æ‹¬æœªæ¥çš„åˆ¶è¯ã€ç§‘å­¦å®žéªŒï¼Œéƒ½å¯ä»¥é€šè¿‡æœºå™¨äººè¿›è¡Œå¤§é‡çš„åŠ é€Ÿï¼Œåœ¨æœºå™¨äººåº”ç”¨æ¯”è¾ƒå…¸åž‹çš„ç“¶é¢ˆé—®é¢˜é‡Œï¼Œé€šè¿‡å¤šæ¨¡æ€çš„å­¦ä¹ å®žçŽ°äº†æœºå™¨äººçš„çµå·§æ“ä½œï¼ŒåŒ…æ‹¬æŠ“å–ã€æ³¨å°„ç­‰ç­‰ã€‚ æœªæ¥å…³é”®æŠ€æœ¯ä¸¾ä¾‹ åŒºå—é“¾æŠ€æœ¯äº§ç”Ÿå¯é å­¦ä¹ æ•°æ® æ—¥å¸¸å¤æ‚çŽ¯å¢ƒçš„å¿«é€Ÿç†è§£ æŸ”æ€§è½»åž‹æœºå™¨æ‰‹è‡‚å’Œçµå·§æ‰‹çš„è®¾è®¡ åŸºäºŽä¼ æ„Ÿçš„ç²¾ç»†çµå·§æ“ä½œ æœºå™¨äººé«˜å±‚çŸ¥è¯†çš„èŽ·å–å’Œå­¦ä¹  è‡ªç„¶äººæœºäº¤äº’ è·¨å¹³å°è¿è¡Œçš„å¼€æºè½¯ä»¶ â€¦ Driving Safety and Autonomy through Advanced Sensing Technologies and Efficient Big Data Analyticsï¼ˆMircea Graduï¼Œ2018 æ±½è½¦å·¥ç¨‹å­¦ä¼šï¼ˆSAE Internationalï¼‰ä¸»å¸­ï¼ŒVelodyne LiDAR é«˜çº§å‰¯æ€»è£å…¼CQOï¼‰ è¿™ä¸ªvideoå±•ç¤ºäº†Velodyne LiDARåœ¨è‡ªåŠ¨é©¾é©¶ä¸ŠæŠ€æœ¯å®žçŽ°çš„å¯è§†åŒ–æƒ…å†µ Rethinking Deep Learning: Back to the Challenges of Computer Visionï¼ˆé©¬æ¯…ï¼ŒåŠ å·žå¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ç”µå­å·¥ç¨‹ä¸Žè®¡ç®—æœºç³»æ•™æŽˆï¼Œ ACM ã€IEEE Fellowï¼‰è¿™æ˜¯æ•´åœºå¤§ä¼š100å¤šå¤§ç‰›é‡Œæˆ‘æœ€å–œæ¬¢çš„å˜‰å®¾ï¼Œä¸æ˜¯å› ä¸ºä»–titleæœ€é«˜æˆ–è€…æ˜¯é¢œå€¼æœ€é«˜ï¼Œè€Œæ˜¯å› ä¸ºä»–æ•¢äºŽè¡¨è¾¾è‡ªå·±çš„è§‚ç‚¹ï¼Œèƒ½è·³å‡ºåœˆå­æ€è€ƒå…ˆæ”¾å‡ºä»–çš„è§‚ç‚¹ï¼šCan we all please agree: With back-propagation and sufficient computation cost, deep models can fit or over-fit any finite (polynomial #) samples. So-learned models are useful when such finite samples cover almost all cases of interest or importance. But let us be honest that: Wishful designs and empirical validations are not mathematical proofs.(however big data are always negligibly small to infinity!) Learning is about finding the simplest generative model from finite samples.(without which there is no â€œstability, robustness, invariance, or generalizabilityâ€.) And also realize that: Deep networks are trying to find low-dimensional structures, too!(hence the theory of subspace, sparsity, low-dimensional models is foundational.) ä¸‹é¢è¯´ä¸€ä¸‹æˆ‘çš„æ€»ç»“ï¼š å…³äºŽæ·±åº¦å­¦ä¹ é¢„å…ˆç§‘æ™®ï¼ŒæŒ‰ç…§é¡ºåºæ˜¯å‰è€…åŒ…å«åŽè€…ï¼šäººå·¥æ™ºèƒ½-æœºå™¨å­¦ä¹ -æ·±åº¦å­¦ä¹ ä¹Ÿå°±æ˜¯ï¼Œæœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ã€‚ ä½œä¸ºä¸€ä¸ªæ•°å­¦å‡ºèº«çš„å·¥ç¨‹å¸ˆï¼Œæˆ‘éžå¸¸ç†è§£å¹¶ä¸”æ¯”è¾ƒèµžåŒé©¬æ¯…è€å¸ˆå¯¹å¾…æ·±åº¦å­¦ä¹ çš„è§‚ç‚¹ã€‚å°±æ˜¯ä¸ç›²ç›®çš„å´‡æ‹œæ·±åº¦å­¦ä¹ ã€‚çŽ°åœ¨çš„æ·±åº¦å­¦ä¹ ï¼Œå„ç§ç¥žç»ç½‘ç»œçš„ç«çˆ†ï¼Œæ˜¯æ²¡æœ‰å®Œæ•´çš„æ•°å­¦è®ºè¯ä½œä¸ºæ”¯æ’‘çš„ã€‚è€Œæ˜¯æ”¾åœ¨çŽ°å®žåœºæ™¯ä¸­æ¯”è¾ƒå¥½ç”¨ï¼Œæ¯”å¦‚äººè„¸è¯†åˆ«ï¼Œå°±è¿™ä¹ˆè¢«å¤§ä¼—ä»¥åŠå­¦æœ¯åœˆå­æŽ¥å—äº†ã€‚ä½†æ˜¯å›¾åƒåšç»†å¾®è‚‰çœ¼ä¸å¯è§çš„æ”¹åŠ¨ï¼Œæœºå™¨å¯èƒ½å°±è¯†åˆ«ä¸å‡ºäººè¿˜æ˜¯ç‹—ï¼Œäººå´ä¸€å®šä¸ä¼šçŠ¯è¿™ä¹ˆä½Žçº§çš„é”™è¯¯ï¼Œæ·±åº¦å­¦ä¹ å®ƒçš„ä¸å¯è§£é‡Šæ€§æˆ–è€…å¼±è§£é‡Šæ€§è¿˜æ˜¯å®ƒçš„ç—›ç‚¹ã€‚ æ·±åº¦å­¦ä¹ æ˜¯â€œç»éªŒâ€çš„å­¦ä¹ ä¸Žåº”ç”¨ï¼Œè€Œä¼ ç»Ÿçš„åƒåŽ‹ç¼©æ„ŸçŸ¥(compressed sensing)å¾€å¾€æ˜¯ä¸¥è°¨çš„æ•°å­¦å»ºæ¨¡ï¼Œå³æ‰€è°“çš„model-basedã€‚ æ·±åº¦å­¦ä¹ æœ‰äº›éƒ¨åˆ†å…¶å®žæ˜¯æ ¹æ¤äºŽä¼ ç»Ÿçš„æ¨¡åž‹ä¸Žæ–¹æ³•ï¼Œåƒç»å¸¸æçš„dropoutæ‰”å‚æ•°ï¼Œå…¶å®žä¹Ÿæ˜¯ç¨€ç–æ€§çš„ä¸€ç§ä½“çŽ°ã€‚ æ·±åº¦å­¦ä¹ çš„æœ‰äº›ä¸œè¥¿å°±æ˜¯ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ï¼Œåªä¸è¿‡æ˜¯æ¢äº†ä¸€ä¸ªåå­—è€Œå·²ã€‚åƒç»å¸¸æåˆ°çš„å‰é¦ˆã€åé¦ˆï¼Œå°¤å…¶æ˜¯æ ¹æ®ç»“æžœå¾€å‰æŽ¨çš„åé¦ˆä¸å°±æ˜¯æœºå™¨å­¦ä¹ é‡Œé¢çš„ç›‘ç£å­¦ä¹ å—ï¼Ÿå‰é¦ˆä¸€ç›´å¾€å‰èµ°ï¼Œç›¸å½“äºŽæ²¡æœ‰åˆ©ç”¨æ ‡ç­¾ä¿¡æ¯ï¼Œä¸å°±æ˜¯æ— ç›‘ç£å­¦ä¹ å—ï¼Ÿ æ·±åº¦å­¦ä¹ ç½‘ç»œå±‚æ•°è¶Šæ·±è¶Šå¥½çš„è§‚ç‚¹å€¼å¾—å•†æ¦·ï¼ŒPCANETå°±æ˜¯ä¸€ä¸ªä¾‹è¯ï¼ˆæ²¡çœ‹è¿‡â€¦ï¼‰ã€‚æ·±åº¦å­¦ä¹ æ·±çš„åº”è¯¥æ˜¯ç†è®ºï¼Œæ˜¯ç†è§£ï¼Œè€Œéžå•çº¯çš„åŠ å±‚æ•°ã€‚é©¬è€å¸ˆè®²äº†ä¸€å¥çˆ±å› æ–¯å¦è€äººå®¶çš„è¯ï¼Œå¤§æ¦‚æ˜¯â€Everything should be made as simple as possible, but not simplerâ€ ä½†æ˜¯è¯åˆè¯´å›žæ¥ï¼Œä¸Žå…¶ç­‰å¾…è§£é‡Šæ€§æ›´å¼ºçš„ç®—æ³•è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½å’Œæ•ˆæžœï¼ŒçŽ°åœ¨çš„æ·±åº¦å­¦ä¹ éƒ½æ˜¯æœªæ¥ä¼˜ç§€ç®—æ³•çš„åž«è„šçŸ³ï¼Œå¦‚æžœä¸èƒ½ä¸€æ­¥åˆ°è¾¾æœªæ¥ï¼Œé‚£ä¸å°±åº”è¯¥å…ˆåˆ©ç”¨çŽ°æœ‰çš„èµ„æºåšçŽ°åœ¨èƒ½åšçš„äº‹å—ï¼Ÿ å…³äºŽè€ƒå¤ã€æ–‡çŒ®é˜…è¯»æˆ‘ä»¬çŽ°åœ¨ä¸€èˆ¬åªè¯»æœ€æ–°çš„æ–‡çŒ®ï¼Œé©¬è€å¸ˆçš„ä¸€ä¸ªè§‚ç‚¹ä¹Ÿéžå¸¸å€¼å¾—æ€è€ƒï¼šä½ æƒ³åˆ°çš„ã€é‡åˆ°çš„é—®é¢˜è€ç¥–å®—å¯èƒ½æ—©å°±æƒ³åˆ°ã€é‡åˆ°äº†ï¼Œç”šè‡³å¯èƒ½ç»™å‡ºäº†è¿˜ä¸é”™çš„è§£æ³•ï¼ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬è¯»æ–‡çŒ®çš„æ—¶å€™ä¸è¦ä»…ä»…å±€é™äºŽæ–°æ–‡çŒ®ï¼Œä¹ŸåŽ»å°è¯•ä¸€ä¸‹è€ä¸€ç‚¹çš„ç”šè‡³50ã€60å¹´ä»£çš„æ–‡çŒ®ï¼Œè™½ç„¶å¤„ç†çš„é—®é¢˜å¾ˆå¯èƒ½å®Œå…¨ä¸åŒï¼Œä½†è¿™å¹¶ä¸ä»£è¡¨ç€æˆ‘ä»¬ä¸èƒ½ä»Žæ—§æ–‡çŒ®ä¸­èŽ·å¾—å¯å‘ã€‚ä¸è¦é‡å¤é€ è½®å­ï¼Œå°±æ˜¯è¿™ä¸ªæ„æ€å—ï¼Ÿ å…³äºŽæ€è€ƒç‰©ç†æœ‰è´¨é‡å®ˆæ’ï¼Œæ•°å­¦æœ‰éš¾åº¦å®ˆæ’ã€‚åŒæ—¶ï¼Œä½œä¸ºä¸€ä¸ªæŠ€æœ¯äººå‘˜ï¼Œè¡¨è¾¾èƒ½åŠ›ååˆ†é‡è¦ï¼Œä¸Žåˆ«äººåˆ†äº«ï¼Œé¦–å…ˆè¦å¼•èµ·åˆ«äººå¯¹ä½ çš„é¡¹ç›®çš„å…´è¶£ã€å…±é¸£ï¼Œå…¶æ¬¡èƒ½å¤Ÿè§£é‡Šæ˜Žç™½æ€è·¯ä¸Žæ¡†æž¶ï¼Œæœ€åŽè¦æ•¢äºŽè¡¨è¾¾è‡ªå·±çš„è§‚ç‚¹ã€‚è€Œä¸”åœ¨å¼€å‘ç®—æ³•æˆ–è€…å¼€å‘æ¨¡åž‹çš„æ—¶å€™ï¼Œä¸èƒ½ä¸€å¤´æ‰Žè¿›åŽ»æ²‰è¿·ç®—æ³•ç»“æž„çš„ä¼˜ç¾Žï¼Œè€Œæ˜¯æ—¶ä¸æ—¶è¦å‡ºæ¥å®¡åº¦ï¼Œæ€è€ƒåº”ç”¨çš„åœºåˆï¼Œæ–¹å‘å’Œè¿œè§æœ‰æ—¶å€™æ›´é‡è¦ã€‚å¾ˆå–œæ¬¢é©¬æ¯…è€å¸ˆè¿™ç§å¯¹ä¸‡äº‹ä¿ç•™ç†æ€§çš„è´¨ç–‘çš„æ€åº¦ï¼Œåšæ•°å­¦çš„å°±æ˜¯è¦æœ‰è¿™ç§å¼ºè¿«ç—‡æ‰å¯çˆ±å§ðŸ˜ [ä»¿ç”Ÿæœºå™¨äººä¸“åœº] ===========================ä»¿ç”Ÿç²˜é™„å’Œè½¯å¾®åž‹æœºå™¨äººï¼ˆMetin Sitti ï¼Œå¾·å›½é©¬å…‹æ–¯æ™®æœ—å…‹æ™ºèƒ½ç³»ç»Ÿç ”ç©¶æ‰€çš„ç‰©ç†æ™ºèƒ½éƒ¨é—¨å…ƒè€çº§æˆå‘˜åŠè´Ÿè´£äººï¼ŒnanoGriptech å…¬å¸åˆ›å§‹äººï¼ŒåŽŸå¡å†…åŸºæ¢…éš†å¤§å­¦çš„æœºæ¢°å·¥ç¨‹å­¦æ•™æŽˆï¼‰ä»–é¦–å…ˆä»‹ç»äº†è‡ªå·±æ—©æœŸå…³äºŽå£è™Žçš„é»ç€åŽŸç†çš„ä»¿ç”Ÿå­¦ç ”ç©¶ã€‚å£è™Žçš„æŒ‡å¤´ä¸Šæœ‰çº¤ç»´ï¼Œæä¾›äº†å¾ˆå¼ºçš„å¸é™„åŠ›ï¼Œè€Œä¸éœ€è¦åƒæ˜†è™«ä¸€æ ·éœ€è¦é»æ¶²ï¼Œæ˜¯ä¸€ç§å¹²æ€§é»æ€§ã€‚è¿·å¦¹è§’åº¦â¤â¤ è§†é¢‘å†…æœ‰ä¸€ä¸ªå°è™«å­ä¸€æ ·çš„ä¸æ˜Žç‰©ä½“å¯¹ä¸å¯¹~~è¿™æ˜¯ä¸€ç§å¤§çº¦ä¸ƒåˆ†ä¹‹ä¸€è‹±å¯¸é•¿çš„æœºå™¨äººï¼Œå®ƒé¦–å…ˆçœ‹èµ·æ¥åªä¸è¿‡æ˜¯ä¸€æ¡æ©¡çš®çŠ¶çš„å°æ¡çŠ¶ç‰©ã€‚ç„¶åŽå®ƒå¼€å§‹ç§»åŠ¨ã€‚å®ƒå¯ä»¥èµ°è·¯ï¼Œè·³è·ƒï¼Œçˆ¬è¡Œï¼Œç¿»æ»šå’Œæ¸¸æ³³ã€‚å®ƒç”šè‡³çˆ¬å‡ºæ¸¸æ³³æ± ï¼Œä»Žæ°´çŽ¯å¢ƒå˜æˆå¹²ç‡¥çš„çŽ¯å¢ƒã€‚å…·ä½“æ“ä½œå¯ä»¥ç‚¹å‡»ä¸‹é¢çš„è§†é¢‘ï¼š æœºå™¨äººåŽŸåž‹è¶³å¤Ÿå°ï¼Œå¯ä»¥åœ¨èƒƒæˆ–æ³Œå°¿ç³»ç»Ÿä¸­ç§»åŠ¨ã€‚ è¯¥æœºå™¨äººå°šæœªåœ¨äººä½“ä¸­è¿›è¡Œè¿‡æµ‹è¯•ï¼Œä½†å…¶ç›®æ ‡æ˜¯æ”¹å–„å…¶ç”¨äºŽåŒ»ç–—ç”¨é€”ã€‚ ä¾‹å¦‚ï¼Œå°†è¯ç‰©è¾“é€åˆ°ä½“å†…çš„ç›®æ ‡ã€‚æ¯”å¦‚æœç”¨è¯ç‰©åŽï¼Œè¯ç‰©èƒ½ç›´æŽ¥è·‘åŽ»ä½ ä½“å†…ä¼¤å£æˆ–è€…éœ€è¦è¯ç‰©çš„åœ°æ–¹ï¼Œè€Œä¸æ˜¯æ•£è½åœ¨è‚ èƒƒçš„å„å¤„ã€‚ Sittiåšå£«è¯´ï¼Œè¿™é¡¹ç ”ç©¶æœ€ä¸å¯»å¸¸çš„æ˜¯ï¼Œè¿™ç§â€œæžç®€ä¸»ä¹‰æœºå™¨äººâ€å¯ä»¥å®žçŽ°â€œåœ¨å¤æ‚çŽ¯å¢ƒä¸­å¯¼èˆªçš„æ‰€æœ‰ä¸åŒç±»åž‹çš„è¿åŠ¨å¯èƒ½æ€§â€ã€‚ æŽ¥ä¸‹æ¥æ€»ç»“ä¸€ä¸‹è¿™ä¸ªå°æœºå™¨äººçš„å­˜åœ¨æ„ä¹‰ï¼Œåˆ«çœ‹ä½“ç§¯å°ï¼Œå…¶å®žæŒºæœ‰è¶£ä¹ŸæŒºæœ‰ç”¨çš„ï¼š æœºå™¨äººæ˜¯ç”±ä»€ä¹ˆæž„æˆçš„ï¼Œå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ Sittiï¼šæœºå™¨äººç”±å¼¹æ€§æ©¡èƒ¶åˆ¶æˆï¼Œé‡Œé¢å……æ»¡äº†è®¸å¤šç£æ€§å°é¢—ç²’ã€‚ç¼–ç¨‹å¯ä»¥æ“ä½œè¿™äº›ç²’å­çš„ç£æ€§ï¼Œä»¥ä¾¿ä»Žå¤–éƒ¨ï¼Œå½“æˆ‘ä»¬æ–½åŠ ç£åœºæ—¶ï¼Œå¼¹æ€§éž˜å½¢æœºå™¨äººå°†å…¶å½¢çŠ¶æ”¹å˜ä¸ºæˆ‘ä»¬æƒ³è¦çš„ä»»ä½•ä¸œè¥¿ã€‚ ç„¶åŽå®ƒåšæ‰€æœ‰è¿™äº›ä¸åŒçš„åŠ¨ä½œã€‚å½“ä½ çœ‹åˆ°è¿™ä¸ªå°ä¸œè¥¿çˆ¬è¡Œå’Œè·³è·ƒä»¥åŠæ‰€æœ‰è¿™äº›ä¸œè¥¿æ—¶ï¼Œå®ƒçœ‹èµ·æ¥åƒä¸€ä¸ªç”Ÿç‰©ã€‚ è¿™ä¸ªæœºå™¨äººçš„æœªæ¥åœ¨å“ªé‡Œï¼Ÿ Sittiï¼šç›®å‰çš„ä¸»è¦ç›®æ ‡ä¹‹ä¸€æ˜¯å°†è¿™ä¸ªå¾®å°çš„è½¯æœºå™¨äººæ”¾å…¥æˆ‘ä»¬çš„æ¶ˆåŒ–ç³»ç»Ÿæˆ–æ³Œå°¿ç³»ç»Ÿ - ä»¥åŠå°†æ¥çš„è¡€ç®¡ç³»ç»Ÿ - å¹¶ä½¿å…¶èƒ½å¤Ÿåœ¨æ‰€æœ‰è¿™äº›å¤æ‚çš„ç»„ç»‡ä¸­å¯¼èˆªï¼Œè¿™äº›è¡¨é¢å®Œå…¨å……æ»¡äº†æµä½“æˆ–åŠå¡«å……ï¼Œæˆ–æ— æµä½“ã€‚ å¦‚æžœä½ çœ‹ä¸€ä¸‹æˆ‘ä»¬æ‰€æ‹¥æœ‰çš„åŒ»ç–—è®¾å¤‡ï¼Œæœ€å°çš„æ˜¯ç›´å¾„ä¸ºæ¯«ç±³çš„å¯¼ç®¡ï¼Œå®ƒä»¬æ€»æ˜¯è¢«æŸç¼šä½ã€‚å› æ­¤ï¼Œåˆ¶é€ è¿™ä¸ªå°åž‹æœºå™¨äººçš„ä¸»è¦ç›®æ ‡æ˜¯çœŸæ­£è¿›å…¥æˆ‘ä»¬ä½“å†…éš¾ä»¥è§¦åŠç”šè‡³ä¸å¯èƒ½åˆ°è¾¾çš„åŒºåŸŸï¼Œå¹¶ä¸”ä¾µå…¥æœ€å°ã€‚ æœºå™¨äººå·²ç»è¶³å¤Ÿå°ï¼Œå¯ä»¥ç”¨äºŽæˆ‘ä»¬çš„æ¶ˆåŒ–ç³»ç»Ÿå’Œæ³Œå°¿ç³»ç»Ÿã€‚æˆ‘ä»¬æƒ³è¦æ›´å°ï¼Œç”šè‡³å‡ åå¾®ç±³ï¼Œè¿™æ ·æˆ‘ä»¬å‡ ä¹Žå¯ä»¥åˆ°è¾¾ä½ ä½“å†…çš„ä»»ä½•åœ°æ–¹ã€‚ æœ‰æœä¸€æ—¥è¿™ä¸ªå°æœºå™¨äººèƒ½å¤Ÿæä¾›è¯ç‰©ï¼Ÿ Sittiï¼šæˆ‘ä»¬ä¸€ç›´åœ¨æŽ¢ç´¢çš„åŠŸèƒ½ä¹‹ä¸€æ˜¯å¦‚ä½•åœ¨ä½“å†…è¾“é€å¯èƒ½æ˜¯è¯ç‰©çš„è´§ç‰©ã€‚æœ‰ä¸åŒçš„æ–¹å¼ã€‚é€šè¿‡æ”¹å˜å½¢çŠ¶ï¼Œæˆ‘ä»¬å¯ä»¥æŠ“ä½è´§ç‰©ï¼Œç„¶åŽé€šè¿‡æ‰“å¼€å½¢çŠ¶æ¥äº¤ä»˜è´§ç‰©ã€‚ ç¬¬äºŒç§æ–¹å¼æ˜¯æˆ‘ä»¬åœ¨æœºå™¨äººä¸Šåšä¸€ä¸ªå°å£è¢‹ï¼Œåªæ‰“å¼€ä¸€ä¸ªæˆ‘ä»¬å¯ä»¥æŽ§åˆ¶çš„ç‰¹æ®Šå½¢çŠ¶å˜åŒ–ã€‚ çµæ„Ÿæ¥è‡ªæ°´æ¯ï¼Œæ¯›æ¯›è™«å’Œå…¶ä»–åŠ¨ç‰©çš„è¿åŠ¨ï¼Ÿ Sittiï¼šåŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬é‡‡å–äº†æ‰€æœ‰è¿™äº›çµæ„Ÿï¼Œå¹¶å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªæœºå™¨äººã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨è¿™é¡¹ç ”ç©¶ä¸­è§£å†³çš„å¦ä¸€ä¸ªç§‘å­¦æŒ‘æˆ˜ï¼šå¦‚ä½•å°†æ¯›è™«ï¼Œæ°´æ¯å’Œæ‰€æœ‰è¿™äº›ä¸åŒçš„ï¼Œå°çš„ï¼ŒæŸ”è½¯çš„ç”Ÿç‰©ç»“åˆåˆ°ä¸€ä¸ªç›¸å¯¹æžç®€ä¸»ä¹‰çš„æœºå™¨äººä¸­ï¼Œå¯ä»¥å®žçŽ°åœ¨å¤æ‚çŽ¯å¢ƒä¸­å¯¼èˆªçš„æ‰€æœ‰ä¸åŒç±»åž‹çš„è¿åŠ¨ã€‚ å¦‚æžœå®ƒåœ¨ä½“å†…è¿·å¤±äº†æ€Žä¹ˆåŠžï¼Ÿ Sittiï¼šè¿™ä¸ªç‰ˆæœ¬ä½œä¸ºä¸€ä¸ªæ•´ä½“æœºå™¨äººä¸æ˜¯å®Œå…¨å¯ç”Ÿç‰©é™è§£çš„ã€‚æˆ‘ä»¬æ­£åœ¨ç ”ç©¶çš„ä¸€ä¸ªé¡¹ç›®æ˜¯åˆ¶é€ ä¸€ä¸ªå®Œå…¨å¯ç”Ÿç‰©é™è§£çš„æœºå™¨äººã€‚æœ€åŽï¼Œæœºå™¨äººå°†è¢«èº«ä½“æº¶è§£ï¼Œæ²¡æœ‰å‰¯ä½œç”¨ï¼Œæ²¡æœ‰æ¯’æ€§ï¼Œä¹Ÿæ²¡æœ‰ä»»ä½•ææ–™ä¼šå¯¼è‡´èº«ä½“å‡ºçŽ°ä»»ä½•é—®é¢˜ã€‚è¿™æ˜¯æˆ‘ä»¬å°ç»„çš„ä¸»è¦ç›®æ ‡ä¹‹ä¸€ã€‚è¿™æ˜¯å¯èƒ½çš„ã€‚æˆ‘çš„æ„æ€æ˜¯ï¼Œæˆ‘ä»¬çš„å¼¹æ€§ä½“åœ¨ä½“å†…å®Œå…¨å¯é™è§£ã€‚æˆ‘ä»¬çš„ç£æ€§çº³ç±³ç²’å­å¯åœ¨ä½“å†…å®Œå…¨é™è§£ã€‚è¿™åªæ˜¯æ•´åˆå®ƒä»¬çš„é—®é¢˜ã€‚ æ™ºèƒ½ä»¿ç”Ÿæœºå™¨é±¼ ï¼ˆè°¢å¹¿æ˜Ž ï¼ŒåŒ—äº¬å¤§å­¦å·¥å­¦é™¢æ•™æŽˆï¼‰èƒ½å®žçŽ°åŽç©ºç¿»/åž‚ç›´æ—‹è½¬çš„é±¼ï¼šå®žçŽ°æœºå™¨é±¼ç¾¤ï¼šè¯¦ç»†é“¾æŽ¥ðŸ”— æœºå™¨äººæ¯”è¾ƒå¥½ç†è§£ï¼Œæœ‰æ²¡æœ‰è·Ÿæˆ‘ä¸€æ ·å¥½å¥‡æœºå™¨é±¼æ˜¯å¹²å˜›ç”¨çš„ï¼Ÿé±¼è·Ÿæµ·æ´‹æœ‰å¯†ä¸å¯åˆ†çš„å…³ç³»ï¼Œç›®å‰çš„ä»¿ç”Ÿé±¼èƒ½åšåˆ°çš„æœ‰ï¼š è‡ªåŠ¨å¸¦å›žæŒ‡å®šæµ·æ´‹åŸŸçš„æ°´æ ·æœ¬ï¼Œä¾›æ°´è´¨æ£€æµ‹ç”¨ï¼Œå°±ä¸éœ€è¦äººåˆ’èˆ¹è¿‡åŽ»é‡‡æ°´æ ·äº† èƒ½å¸¦é¢†ç”Ÿç‰©é±¼ç¾¤æ¸¸åŠ¨ï¼Œåœ¨æ•é±¼æœŸé—´ï¼Œå¯ä»¥æ”¾å‡ºæ‰€éœ€é±¼ç¾¤æ ·å­çš„é±¼æŠŠç”Ÿç‰©é±¼ç¾¤å¸¦é¢†åˆ°æ•çŒŽåŒºåŸŸï¼Œå¯ä»¥æœ‰æ•ˆç­›é€‰æŽ‰ä¸éœ€è¦çš„é±¼ç§ã€‚å½“ç„¶åŸºäºŽåˆç†æ•çŒŽçš„èŒƒå›´ã€‚ è£…æœ‰æ‘„åƒå¤´çš„é±¼å¯ä»¥æ—¥å¸¸ç›‘æŽ§æµ·æ´‹å¯è§èŒƒå›´å†…çš„æƒ…å†µ åšæµ·æ´‹çš„æ¸…æ´å·¥ï¼ŒæŠŠäººç±»ä¸¢è¿›åŽ»çš„å¤§é‡åžƒåœ¾å›žæ”¶ï¼Œå°¤å…¶æ˜¯å°†å¡‘æ–™è¢‹/å¡‘æ–™ç“¶è¿™ç§ä¸å¯é™è§£ç‰©å¸¦ç¦»æµ·æ´‹ç”Ÿç‰© ä»¿ç”Ÿé±¼å¯å¤§å¯å°ï¼Œä¹Ÿå¯ä»¥åšçš„å¾ˆå¤§ ç”±äºŽå¯¹æµ·æ´‹æœ‰èŽ«åçš„å–œæ¬¢ï¼Œæ‰€ä»¥è¿˜æ˜¯å¾ˆæœŸå¾…ä»¿ç”Ÿé±¼çš„åº”ç”¨çš„ã€‚åŒæ—¶ï¼Œå¼€å‘å‡ºè¶Šå¼ºå¤§çš„ä»¿ç”Ÿæœºå™¨ï¼Œå°±è¶Šæ„Ÿæ…¨é€ ç‰©ä¸»çš„ä¼Ÿå¤§ï¼æ— è®ºæˆ‘å¤šä¹ˆçƒ­çˆ±ç§‘æŠ€ï¼Œä¹Ÿæ¯”ä¸ä¸Šæˆ‘çƒ­çˆ±å¤§è‡ªç„¶~ ä»¿äººæœºå™¨äººå…³é”®æŠ€æœ¯ç ”ç©¶ ï¼ˆç†Šè“‰ï¼Œæµ™æ±Ÿå¤§å­¦æ™ºèƒ½ç³»ç»Ÿä¸ŽæŽ§åˆ¶ç ”ç©¶æ‰€æœºå™¨äººå®žéªŒå®¤ä¸»ä»»ï¼‰è¯¦ç»†é“¾æŽ¥ðŸ”— å½“å‰æ™ºèƒ½æœºå™¨äººå‘å±•è‹¥å¹²æŒ‘æˆ˜æ€§é—®é¢˜ ï¼ˆçŽ‹ç”°è‹—ï¼ŒåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦æ•™æŽˆï¼Œé•¿æ±Ÿå­¦è€…ç‰¹è˜æ•™æŽˆï¼‰è¯¦ç»†é“¾æŽ¥ðŸ”— [AI+ä¸“åœº] ===========================æœºå™¨å­¦ä¹ çš„å¯è§£é‡Šæ€§ä¸Žè‡ªåŠ¨æœºå™¨å­¦ä¹ ï¼ˆèƒ¡ä¾  ï¼Œç¾Žå›½å¾·å·žå†œå·¥å¤§å­¦æ•°æ®æŒ–æŽ˜å®žéªŒå®¤ä¸»ä»»ã€è®¡ç®—æœºå­¦é™¢ç»ˆèº«æ•™èŒç³»åˆ—åŠ©ç†æ•™æŽˆï¼‰è¿™æ˜¯ä¸‰å¤©é‡Œæˆ‘æœ€æ„Ÿå…´è¶£çš„ä¸€åœºäº†ï¼Œä¸Šå¹²è´§ï¼ èƒ¡ä¾ æ•™æŽˆè¡¨ç¤ºï¼Œæœºå™¨å­¦ä¹ è¦è¢«å„è¡Œå„ä¸šæ™®éæŽ¥å—å’Œåº”ç”¨ï¼Œå‰ææ˜¯è¦å…·æœ‰å¯è§£é‡Šæ€§ã€‚ä½†æ˜¯èµ‹äºˆæœºå™¨å­¦ä¹ å¯è§£é‡Šæ€§æ˜¯ä¸€ä¸ªéžå¸¸éš¾çš„é—®é¢˜ã€‚ç¬¬ä¸€ï¼Œå¯è§£é‡Šæ€§æ²¡æœ‰æ˜Žç¡®çš„å®šä¹‰ï¼Œå¯èƒ½æ˜¯ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ï¼Œä¹Ÿå¯èƒ½æ˜¯é¢„æµ‹ç»“æžœçš„å¯è§£é‡Šæ€§ï¼Œç”šè‡³å¯èƒ½æ˜¯ç³»ç»Ÿä¸­æŸä¸€ä¸ªéƒ¨åˆ†çš„å¯è§£é‡Šæ€§ã€‚ç¬¬äºŒï¼Œå¦‚æžœåšæ·±åº¦å­¦ä¹ çš„å¯è§£é‡Šå·¥ä½œï¼ŒçŽ°æœ‰çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿåƒåƒä¸‡ï¼Œæˆ‘ä»¬æ²¡åŠžæ³•å¯¹æ¯ä¸€ä¸ªç³»ç»Ÿéƒ½åšã€‚ç¬¬ä¸‰ï¼Œè®©æœºå™¨å­¦ä¹ ç³»ç»Ÿå…·æœ‰å¯è§£é‡Šæ€§ï¼Œå¿…é¡»å¤§é‡HCIã€Visualizationä¸“å®¶è·¨å­¦ç§‘åˆä½œï¼Œæ˜¯ä¸€é¡¹å·¨å¤§çš„æŒ‘æˆ˜ã€‚ ä¸‹é¢è¿™ä¸ªè§†é¢‘æ˜¯ä¸€ä¸ªåˆ†ç±»æ¨¡åž‹çš„å¯è§†åŒ–ç•Œé¢ï¼Œç±»ä¼¼å†³ç­–æ ‘/boostingçš„æ ·å­ï¼Œå¯ä»¥æ‰‹åŠ¨å¢žåŠ /åˆ å‡æ ‘çš„èŠ‚ç‚¹ï¼Œè¿˜æŒºå¥½çŽ©çš„ï¼š ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œèƒ¡ä¾ æ•™æŽˆæå‡ºï¼Œå°†æ€§èƒ½å¼ºå¤§ã€ä¸å¯è§£é‡Šçš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿå­¦åˆ°çš„çŸ¥è¯†ï¼Œè¿ç§»åˆ°æ€§èƒ½è¾ƒå¼±ä½†å¯è§£é‡Šçš„æµ…åº¦å­¦ä¹ ç³»ç»Ÿä¸­ã€‚æ›´å¤šæœºå™¨å­¦ä¹ çš„å¯è§£é‡Šæ€§ä¿¡æ¯é“¾æŽ¥ðŸ”— æŽ¥ä¸‹æ¥é‡ç‚¹æ•´ç†ä¸‹è‡ªåŠ¨æœºå™¨å­¦ä¹ (auto ml)çš„ç¬”è®°ï¼š é¦–å…ˆä»‹ç»ä»€ä¹ˆå«åšè‡ªåŠ¨çš„æœºå™¨å­¦ä¹ ï¼ˆAuto Machine Learningï¼‰ ç›®å‰æ— è®ºæ˜¯å¼€å‘ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡åž‹ï¼Œè¿˜æ˜¯æ‹›äººå¼€å‘ä¸€ä¸ªæ¨¡åž‹ï¼Œç»æµŽæˆæœ¬å’Œæ—¶é—´æˆæœ¬éƒ½æ˜¯ç›¸å½“å¯è§‚çš„ã€‚å› æ­¤å‡å¦‚æœ‰ä¸€ä¸ªæ¡†æž¶æä¾›è‡ªåŠ¨çš„æœºå™¨å­¦ä¹ æœåŠ¡ï¼Œæ— éœ€ä½ æœ‰è¶…çº§ä¸“ä¸šçš„å»ºæ¨¡çŸ¥è¯†ï¼Œåªéœ€è¦ä½ æŠŠæ•°æ®ä¸¢è¿›åŽ»ï¼Œå®ƒè‡ªåŠ¨å¸®ä½ åŠ å·¥æ•°æ®ï¼Œè‡ªåŠ¨éåŽ†æ¨¡åž‹å»ºæ¨¡ï¼Œç”šè‡³ä¸€å°æ—¶å†…è‡ªåŠ¨è·³å‡ºæœ€ä¼˜çš„æ¨¡åž‹ç»“è®ºï¼Œå²‚ä¸æ˜¯å¦™å“‰ï¼Ÿè¿™å°±æ˜¯ Auto ML å¹²çš„äº‹~ æ¯”å¦‚åƒä¸‹é¢è¿™æ ·ï¼šä¸‹å›¾æ˜¯æœ€åŽŸå§‹ã€æœ€ç®€å•çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿã€‚æˆ‘ä»¬æœ‰ä¸€ç»„æ•°æ®ï¼Œæƒ³çŸ¥é“å®ƒæ˜¯æ–‡æœ¬è¿˜æ˜¯æ•°å€¼ï¼Œå…·ä½“æ˜¯ç”¨Text miningã€Classificationè¿˜æ˜¯Regressionã€‚å¦‚æžœç”¨Classificationï¼Œæ•ˆæžœè¿˜ä¸é”™ï¼Œç³»ç»Ÿå°±ä¼šæŽ¨èç»™ä½ ã€‚è¿™æ˜¯æœ€åŽŸå§‹çš„çŽ°æœ‰äº§å“èƒ½å®žçŽ°çš„åŠŸèƒ½ï¼Œç»™å®šä¸€äº›æ•°æ®åŽå¯ä»¥æŽ¨èç›¸åº”çš„ç³»ç»Ÿç»™å¤§å®¶ã€‚ èƒ¡ä¾ æ•™æŽˆä»–ä»¬çš„å·¥ä½œå†…å®¹ä¸Žæˆæžœï¼š ä»–ä»¬æŒ‘é€‰äº†çº¦300ä¸ªUCIçš„æ•°æ®ï¼Œé‡æ–°é‡‡æ ·å½¢æˆäº†4000ä¸ªæ•°æ®ã€‚ç„¶åŽæŠŠèƒ½æ‰¾åˆ°çš„20å¤šä¸ªåˆ†ç±»çš„packageå…¨éƒ¨åº”ç”¨åˆ°è¿™4000ä¸ªæ•°æ®ä¸ŠåŽ»ï¼Œè§‚å¯Ÿæ•ˆæžœå¦‚ä½•ã€‚æ–°çš„æ•°æ®è¿›æ¥åŽï¼Œä»–ä»¬å°±æ‰¾å‡ºçŸ©é˜µä¸­å’Œæ–°æ•°æ®æœ€åƒçš„Datasetï¼Œå°†è¿™ä¸ªDatasetä¸ŠåŽ†å²è¡¨çŽ°æœ€å¥½çš„æ¨¡åž‹æŽ¨èç»™ç”¨æˆ·ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå°†æœºå™¨å­¦ä¹ æ•ˆæžœæå‡äº†å¾ˆå¤šã€‚ ä»–ä»¬ç›®å‰æ­£åœ¨åšçš„å·¥ä½œæ˜¯ç ”ç©¶æ€Žæ ·åšå¥½ç¥žç»ç»“æž„çš„æœç´¢ã€‚æœ‰äº†æ•°æ®åŽï¼Œç³»ç»Ÿå¯ä»¥è‡ªåŠ¨æŽ¨èä¸€ä¸ªç›¸åº”çš„æ·±åº¦å­¦ä¹ ç»“æž„ç»™è¯¥æ•°æ®ã€‚åœ¨æ²¡æœ‰èµ„æºï¼Œæ²¡æœ‰å¤§é‡æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆå’Œæ•°æ®ç§‘å­¦å®¶çš„æƒ…å†µä¸‹ï¼Œè¿™æ ·ä¸€ä¸ªç»“æž„æˆ–è®¸å¯ä»¥åˆæ­¥æ»¡è¶³åˆåˆ›å…¬å¸ã€ç¤¾ä¼šå­¦ç§‘å’ŒåŒ»ç”Ÿçš„æ•°æ®æŽ¢ç´¢éœ€æ±‚ã€‚ ç¬¬ä¸€æ­¥ï¼Œè¦æ ¹æ®ç›¸åº”æ¨¡åž‹ï¼Œé€šè¿‡é—ä¼ ç®—æ³•æˆ–è€…å¼ºåŒ–å­¦ä¹ æ¥åšã€‚éžå¸¸è€—æ—¶è€—åŠ›ã€‚ ç¬¬äºŒæ­¥ï¼Œæœ‰äº†ç»“æž„åŽï¼Œè¿˜è¦ä»Žå¤´å¼€å§‹è®­ç»ƒè¿™ä¸ªæ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Œè¿™æ ·å®ƒæ‰èƒ½åº”ç”¨åˆ°ç›¸åº”çš„å·¥ä½œä¸­åŽ»ã€‚ æœ‰äº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„åŽŸå§‹ç»“æž„åŽï¼Œè¿˜å¯ä»¥å°†å®ƒå˜å®½ã€å˜æ·±ã€åŠ é€Ÿï¼Œè®©å®ƒçš„é€Ÿåº¦æ›´å¿«ã€‚ é‡‡ç”¨äº†Bayesian Optimizationæ›¿ä»£ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å’Œé—ä¼ ç®—æ³•ï¼Œè®©è¿™ä¸€æ­¥å˜å¾—æ¯”è¾ƒå¿«ã€‚ æ‰€æœ‰çš„å­¦ä¹ éƒ½æ˜¯åŸºäºŽä¸Šä¸€æ­¥ï¼Œæ‰€ä»¥ç¬¬äºŒæ­¥ä¹Ÿèƒ½è®©é€Ÿåº¦éžå¸¸å¿«ã€‚ä»–ä»¬å¯ä»¥æŠŠæ—¶é—´ä»ŽåŽŸå§‹çš„å‡ å¤©åŽ‹ç¼©åˆ°ä¸€ä¸ªå°æ—¶å†…ã€‚ä½ ç»™å®šä¸€ä¸ªæ•°æ®ï¼Œä»–ä»¬å¾ˆå¿«å°±èƒ½æŽ¨èç›¸åº”çš„æ·±åº¦å­¦ä¹ ç»“æž„ç»™ä½ ã€‚ ä¸‹å›¾å±•ç¤ºäº†ä»–ä»¬ä¸€ä¸ªæœˆå‰å‘å¸ƒçš„packageï¼ŒAuto-Keras å…´è¶£ä½¿ç„¶ï¼Œçœ‹å®Œäº†auto-kerasçš„å…¨éƒ¨ä»£ç åŽï¼Œåƒæˆ‘ä¸€æ ·å‘çŽ°è¿˜æ˜¯ä¸å¤Ÿçœ‹çš„è¯â€¦ä¸‹é¢æ˜¯å­¦ç•Œ/ä¸šç•Œåœ¨ auto-machine-learning ä¸Šçš„è¿›å±•ï¼š googleâ€™s Cloud AutoML, Googleâ€™s Prediction API Microsoftâ€™s Custom Vision, Microsoftâ€™s Azure Machine Learning auto-sklearnï¼Œè®ºæ–‡é“¾æŽ¥ï¼šEfficient and Robust Automated Machine Learning ClimbsRocks/auto_ml ä»¥åŠå…¶ä»–æä¾›ç±»ä¼¼æœåŠ¡çš„å…¬å¸ï¼še.g.,BigML.com, Wise.io, SkyTree.com, RapidMiner.com, Dato.com, Prediction.io, DataRobot.com, and Amazon Machine Learning ä¸ªäººç›®å‰æœ€å–œæ¬¢auto-sklearnï¼Œåœ¨MLçš„æ¡†æž¶ä¸Šå¢žåŠ äº† meta-learning å’Œ build-ensemble ä¸¤ä¸ªæœ‰æ„æ€çš„æ¨¡å—å˜æˆ Auto-ML æ¡†æž¶ï¼šä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€èˆ¬çš„åˆ†ç±»æˆ–è€…å›žå½’çš„æœºå™¨å­¦ä¹ æ¨¡åž‹å³å°†æˆ–è€…å·²ç»å®žçŽ°äº†ä½Žé—¨æ§›æˆ–è€…é›¶é—¨æ§›å…è´¹çš„ç¨‹åº¦ã€‚æœºå™¨å­¦ä¹ è‡ªåŠ¨å»ºæ¨¡çš„éš¾ç‚¹è¿˜æ˜¯åœ¨æ•°æ®æ¸…æ´—å’Œç‰¹å¾è¡ç”Ÿè¿™äº›æŠ€å·§ï¼Œè‡³äºŽæ¨¡åž‹çš„é€‰æ‹©å’Œè¶…å‚æ•°è°ƒå‚å·²ç»æœ‰æ¯”è¾ƒæˆç†Ÿå¯ç”¨çš„ä»£ç äº†ã€‚æ‰€ä»¥ï¼Œå·²ç»æœ‰å¼€æºçš„ä»£ç çš„è¯ï¼Œé‚£äº›æä¾›å»ºæ¨¡æœåŠ¡å¹³å°çš„å…¬å¸ä¸ºå•¥è¿˜èƒ½æ‹‰åˆ°é‚£ä¹ˆå¤šèžèµ„ï¼Ÿ whatever, æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯ äººäººéƒ½å¯ä»¥ç”¨å¾—èµ·æœºå™¨å­¦ä¹ ç³»ç»ŸðŸ™‚ [è®¡ç®—æœºè§†è§‰ä¸“åœº] ===========================äº‘ã€ç«¯ã€èŠ¯ä¸Šçš„è§†è§‰è®¡ç®— ï¼ˆå­™å‰‘ï¼Œæ—·è§†ç§‘æŠ€é¦–å¸­ç§‘å­¦å®¶ã€ç ”ç©¶é™¢é™¢é•¿ï¼‰è›®å–œæ¬¢å­™å‰‘çš„ï¼Œå¯ä»¥ç§è‰ä¸€ä¸‹ï¼šã€è¯¦ç»†é“¾æŽ¥ðŸ”—ã€‘ï¼ŒæŽ¨èâ¤ ResNet part: å„ç§netå¤§åˆç…§ï¼š è®¡ç®—æœºè§†è§‰ç ”ç©¶ä¸­çš„æ–°æŽ¢ç´¢ ï¼ˆæž—è¾¾åŽï¼Œå•†æ±¤ç§‘æŠ€è”åˆåˆ›å§‹äººï¼Œæ¸¯ä¸­æ–‡-å•†æ±¤è”åˆå®žéªŒå®¤ä¸»ä»»ï¼‰ã€è¯¦ç»†é“¾æŽ¥ðŸ”—ã€‘ï¼ŒæŽ¨èâ¤ äººè„¸è¯†åˆ«çš„æ ·æœ¬æ•°æ®åŸºäºŽç”µå½±ç´ æï¼ŒæŒºæœ‰æ„æ€çš„ï¼Œä»Žè€Œæ€è€ƒå‡ºäººç‰©ä¹‹é—´çš„å…³ç³»çš„ä¿¡æ¯å¯¹äººè„¸è¯†åˆ«çš„ä½œç”¨ï¼š çœ‹å›¾è¯´è¯ï¼š æ—¢ç„¶AIæ¨¡åž‹èƒ½ç”Ÿæˆä¸€å¥è¯ï¼Œé‚£ä¹ˆæ˜¯ä¸æ˜¯ä¹Ÿèƒ½ç”Ÿæˆä¸€æ®µåŠ¨ä½œï¼Ÿä¸‹å›¾å±•ç¤ºäº†ä»–ä»¬çš„ä¸€é¡¹æœ€æ–°ç ”ç©¶ï¼Œå¾ˆå¤šAIå…¬å¸éƒ½åœ¨åšè¿™æ–¹é¢çš„ç ”ç©¶ï¼Œè®©AIç”Ÿæˆä¸€æ®µç”ŸåŠ¨çš„èˆžè¹ˆã€‚ä¸‹é¢æ˜¯ä¸€äº›ç®€å•çš„åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œéƒ½æ˜¯è®¡ç®—æœºè‡ªåŠ¨ç”Ÿæˆçš„ï¼Œä¸æ˜¯ç”¨ç¨‹åºæè¿°å‡ºæ¥çš„ï¼š [IoTä¸“åœº] ===========================æµ·å°”çš„ç‰©è”ç½‘ ï¼ˆèµµå³°ï¼Œæµ·å°”å®¶ç”µäº§ä¸šé›†å›¢å‰¯æ€»è£ã€CTOï¼‰æ™ºæ…§å®¶åº­ 4ä¸ªåº”ç”¨ç‰©ç†ç©ºé—´ï¼šæ™ºæ…§å®¢åŽ…ã€æ™ºæ…§åŽ¨æˆ¿ã€æ™ºæ…§æµ´å®¤ã€æ™ºæ…§å§å®¤ 7ä¸ªè§£å†³æ–¹æ¡ˆï¼šå…¨å±‹ç”¨ç”µã€å…¨å±‹ç”¨æ°´ã€å…¨å±‹æ´—æŠ¤ã€å…¨å±‹å®‰é˜²ã€å…¨å±‹å¨±ä¹ã€å…¨å±‹å¥åº·ã€å…¨å±‹ä¿¡æ¯ é£Ÿè”ç½‘ ç‰©è”æ„ŸçŸ¥ï¼šå›¾åƒè¯†åˆ«ã€é£Ÿæè¯†åˆ«ã€æ¸©åº¦ä¼ æ„Ÿã€äºŒç»´ç  äººå·¥æ™ºèƒ½ï¼šè¯­éŸ³äº¤äº’ã€é£Ÿæç®¡ç†ã€èœè°±æŽ¨èã€é£Ÿå“æº¯æº ç”Ÿæ€æœåŠ¡ï¼šç”Ÿé²œè´­ç‰©ã€æ–°é—»èµ„è®¯ã€è§†é¢‘å¨±ä¹ã€å¥åº·ç®¡ç† è¡£è”ç½‘ï¼š è¡£è”ç”Ÿæ€è”ç›Ÿ è¡£è”æ ‡å‡† APPæ˜¾ç¤º æ´—è¡£æœºå¯è¯†åˆ«å„ç±»è¡£æœ U+ æ™ºæ…§ç”Ÿæ´»å¹³å°ï¼Œ ä»¥ IoT+AI èµ‹èƒ½æ™ºæ…§å®¶åº­ ä¸šåŠ¡åœºæ™¯ï¼šåŽ¨æˆ¿ç¾Žé£Ÿã€èµ·å±…åœºæ™¯ã€æ´—æµ´å¥åº· U+ æ™ºæ…§ç”Ÿæ´»å¹³å°ï¼šUHomeOS (FULL/ Compact/ Lite) æ•°æ®ï¼Œç‰©è”ï¼Œç”Ÿæ€ï¼Œäº¤äº’ ç»ˆç«¯/ç”¨æˆ·ï¼šæœºæœºäº’è”ï¼Œç”¨æˆ·ç¤¾ç¾¤ UHomeOSï¼š è®¾å¤‡å®žæ—¶åœ¨çº¿ï¼Œå…¨æµç¨‹äº’è”äº’é€šï¼Œå®žçŽ°æ•…éšœè‡ªåé¦ˆè¯Šæ–­ã€‚ï¼ˆIoT+AIï¼‰ å¯å®šåˆ¶äº¤äº’ï¼Œåˆ†å¸ƒå¼ã€åœºæ™¯åŒ–çš„ç”¨æˆ·äº¤äº’æ–¹å¼ï¼š æ™ºèƒ½éŸ³ç®± è§†è§‰æŠ•å°„ è…•æŠ•ï¼ŒæŽŒä¸Šäº¤äº’ æ‰‹æœºAPP ç½‘å™¨å± ç”µè§†å± å…¶å®ƒä¼šåœºçš„é“¾æŽ¥ðŸ”— å†™åœ¨æœ€åŽ å»ºç«‹æ¨¡åž‹çš„ç¬¬ä¸€æ­¥å°±æ˜¯å¤§é‡çš„æ ·æœ¬æ•°æ®ï¼Œè€Œç›®å‰å°±å¥½åƒæ˜¯é‡‡é›†æ•°æ®çš„é˜¶æ®µï¼ŒåŒæ—¶æ•°æ®æ ‡æ³¨è‡ªåŠ¨åŒ–è¿™ä¸ªåŠŸèƒ½ä¹Ÿå‡¸æ˜¾çš„æ¯”è¾ƒå®žç”¨ã€‚å¾ˆå¤šå…¬å¸éƒ½æ€¥ç€æŠ¢å å¸‚åœºï¼ŒèŽ·å–æ•°æ®ï¼Œå°†å…¶ç§æœ‰åŒ–ã€‚çš„ç¡®èŽ·å–æ•°æ®æœ‰æ—¶æ˜¯ä»˜å‡ºå¾ˆå¤§ä»£ä»·çš„ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ç§‘ç ”çš„ç¬¬ä¸€æ­¥ã€‚ä½†æ˜¯å…¶ä¸­ä¸€ä½æ¼”è®²å˜‰å®¾æå‡ºä¸€ä¸ªæœ‰æ„æ€çš„è§‚ç‚¹ï¼š æ•°æ®æ˜¯äººæ°‘å¸ï¼Œä¸åˆ†äº«ä¼šè´¬å€¼ã€‚æ–°çš„dataåœ¨æºæºä¸æ–­çš„äº§ç”Ÿï¼Œä¸ç”¨/ä¸åˆ†äº«çš„dataä¼šè´¬å€¼ã€‚å¼€æºçš„ä»£ç æ—©å°±ä¸æ˜¯æ–°é²œäº‹äº†ï¼Œå¼€æºçš„æ•°æ®ä¹Ÿæ…¢æ…¢å¼€å§‹å½¢æˆæ°›å›´ï¼Œæ¯”å¦‚å›¾åƒé¢†åŸŸçš„ImageNetã€‚ å…³äºŽç®—æ³•å­˜åœ¨çš„æ„ä¹‰å¾ˆå¤šç®—æ³•ä»Žä½ å‡ºç”Ÿå‰å°±æœ‰äº†ï¼Œä½†æ˜¯åªæ˜¯ä»Žè¿™ä¸¤å¹´å¼€å§‹ç«èµ·æ¥è€Œå·²ï¼Œè¿˜æœ‰å¾ˆå¤šå¾ˆå¥½ç”¨çš„ç®—æ³•æ²¡æœ‰ç«èµ·æ¥ï¼Œé£Žæ°´è½®æµè½¬çš„è¯ï¼Œè¯´ä¸å®šå“ªå¤©å°±â€¦æ‰€ä»¥è¿™å°±è¯´åˆ°äº†ï¼ˆç®—æ³•ï¼‰å­˜åœ¨çš„æ„ä¹‰ã€‚æ²¡æœ‰å•ç‹¬çš„èƒ½æ‰“éå¤©ä¸‹çš„ç®—æ³•ï¼Œéƒ½æ˜¯ç®—æ³•ä¹‹é—´äº’ç›¸ç»“åˆåŽ»é¢å¯¹å®žé™…å›°éš¾çš„ã€‚åŒæ—¶ä¸è¦æ²‰è¿·åœ¨ç®—æ³•ä¸­ä¸èƒ½è‡ªæ‹”ï¼Œè¦æ ¹æ®çŽ°å®žé—®é¢˜æ‰¾æ–¹æ³•ï¼Œè€Œä¸æ˜¯æ ¹æ®æ–¹æ³•æ‰¾é—®é¢˜ã€‚ å…³äºŽäººå·¥æ™ºèƒ½çªƒå–éšç§çš„è¯´æ³•å¾ˆå¤šäººå¯¹äºŽåƒæŽ¨èç®—æ³•è¿™ç±»çš„åº”ç”¨æœ‰æŠ—æ‹’ï¼Œæ¯”å¦‚Facebookä¹‹å‰è¢«ç”¨æˆ·ç¾¤èµ·è€Œæ”»ä¹‹ã€‚æˆ‘å°±å¾ˆæƒ³ç«™å‡ºæ¥ä¸ºè¿™äº›ä¼˜ç§€çš„ç®—æ³•è¯´ä¸€å¥ï¼Œæ¯ä¸ªäººçš„idã€å§“åï¼Œè¿™äº›ä¿¡æ¯å¯¹äºŽç®—æ³•æ¥è¯´æ˜¯æœ€æœ€ä¸é‡è¦çš„ä¿¡æ¯ï¼Œéƒ½ä¼šåœ¨ç¬¬ä¸€æ­¥è¢«å‰”é™¤ã€‚å·¥ç¨‹å¸ˆåªæƒ³çŸ¥é“æ ·æœ¬æœ‰å“ªäº›ç‰¹å¾ï¼Œè€Œç‰¹å¾ï¼Œå°±æ˜¯åªæ˜¯ç‰¹å¾ï¼Œè€Œå·²ã€‚é‡‡é›†æ•°æ®çš„æ—¶å€™ï¼ŒåªçŸ¥é“æœ‰å–œæ¬¢æ—…è¡Œçš„è¿™æ ·ä¸€ä¸ªäººï¼Œè€Œä¸çŸ¥é“è¿™ä¸ªäººæ˜¯è°ã€‚ä¸æ˜¯å› ä¸ºä½ å«â€œèŒƒå†°å†°â€è€Œç»™ä½ æŽ¨èåŒ–å¦†å“çš„å¹¿å‘Šï¼Œè€Œæ˜¯å› ä¸ºä½ çˆ±ç¾Žï¼Œæœ‰åŒ–å¦†ä¹ æƒ¯ï¼Œæ‰ä¼šç»™ä½ æŽ¨åŒ–å¦†å“çš„å¹¿å‘Šã€‚å¦‚æžœæ˜¯ä¸€åªæœ‰è´­ä¹°èƒ½åŠ›ï¼Œæœ‰åŒ–å¦†ä¹ æƒ¯ï¼Œå¹¶ä¸”æœ‰æ‰‹æœºï¼Œæ‰‹æœºä¸Šè¿˜æœ‰ç¤¾äº¤appçš„è€é¼ ï¼Œä¹Ÿä¼šæ”¶åˆ°ç±»ä¼¼çš„å¹¿å‘Šçš„ã€‚ç§è‰æ˜¯å¾ˆèŠ±æ—¶é—´å’Œç²¾åŠ›çš„ï¼Œæˆ‘å°±å¾ˆå–œæ¬¢ä¼˜ç§€çš„ç®—æ³•æŽ¨èå‡ºæ¥çš„ï¼Œå°±åƒçŸ¥ä¹Žçš„é«˜ç¥¨å›žç­”ï¼Œæ·˜å®çš„çŒœä½ å–œæ¬¢ï¼Œè™¾ç±³çš„æ¯æ—¥30é¦–ã€‚å·¥ç¨‹å¸ˆä¸ä¼šæ‰“å¼€æ‰€æœ‰æ ·æœ¬ç”¨æˆ·çš„æ‘„åƒå¤´åŽ»çœ‹ä»–ä»¬çš„éšç§ï¼Œå˜æ€æ‰åšè¿™æ ·çš„äº‹ã€‚ å…³äºŽäººå·¥æ™ºèƒ½æ·˜æ±°äººç±»çš„è¯´æ³•å¾ˆå¤šåŸ¹è®­æœºæž„ä¸ºäº†å–è¯¾å°±å‘äººä»¬è´©å–ç„¦è™‘ï¼Œè¯´äººç±»ä¼šå› ä¸ºç§‘æŠ€è€Œå¤±ä¸šã€‚å¼ é’¹é™¢å£«æåˆ°è¯´ï¼Œé‚£äº›ç…§ç« åŠžäº‹ï¼Œä¸éœ€è¦ä»»ä½•çµæ´»æ€§çš„å·¥ä½œæ‰ä¼šå¦‚æ­¤ã€‚éš¾é“è¿™ä¸æ˜¯å¥½äº‹å—ï¼Ÿç§‘æŠ€æ˜¯åœ¨æé«˜æˆ‘ä»¬çš„ç”Ÿæ´»è´¨é‡ã€‚å¼•ç”¨ä¸€ä½æ›´ç›´æŽ¥çš„æ•™æŽˆçš„è¯å°±æ˜¯ï¼šä¼šç”¨äººå·¥æ™ºèƒ½çš„ä»£æ›¿ä¸ä¼šç”¨çš„ã€‚å…¶å®žå¾ˆå¥½ç†è§£ï¼Œæ±‚ç”Ÿæ¬²æ—ºç››çš„äººä»¬æ—©å°±ä¼šç”¨æ™ºèƒ½äº§å“ï¼Œæ¯”å¦‚æ™ºèƒ½æ‰‹æœºï¼Œ2018å¹´çŽ°åœ¨ä¸ä¼šç”¨æ‰‹æœºçš„äººå¿…å®šå¼•èµ·ç”Ÿæ´»çš„ä¸ä¾¿ã€‚æ±‚ç”Ÿæ¬²çˆ†ç‚¸æ—ºç››çš„åƒæˆ‘è¿™æ ·çš„äººï¼Œå°±åŽ»å­¦ä¹ äººå·¥æ™ºèƒ½ã€‚æ‰‹æœºï¼Œä»ŽæŸä¸ªè§’åº¦æ¥è¯´ä¹Ÿæ˜¯æœºå™¨çš„ä¸€ç§ï¼Œå½“æ‰‹æœºå‡ºæ¥çš„æ—¶å€™ä¸çŸ¥é“æ˜¯å¦å¼•èµ·å¾ˆå¤šäººç„¦è™‘ã€‚AI is helping people, not replacing them. ä¸‰å¤©çš„è¡Œç¨‹å°±åƒæ˜¯ a big picture of current AI.æé«˜äº†ä¸€ä¸‹äººç”Ÿå¹¿åº¦ðŸ˜€ï¼Œä¹Ÿé¡ºä¾¿çŸ¥é“åŒè¡Œçš„ä¼™ä¼´éƒ½èµ°åˆ°äº†ä»€ä¹ˆåœ°æ­¥~å¯¹éš”è¡Œå¦‚éš”å±±çš„äº§ä¸šæˆæžœä¹Ÿä¸€é¥±çœ¼ç¦ã€‚æé«˜å¹¿åº¦åŽï¼Œè‡³äºŽæ·±åº¦ï¼Œé‚£æ˜¯æ—¥ç§¯æœˆç´¯çš„äº‹äº†ã€‚ PS. é—¨ç¥¨å¾ˆè´µï¼Œæ­»è´µï¼Œæ˜¯æˆ‘éœ‰æ¼”å”±ä¼šçš„ç¿»å€ï¼æ‰€ä»¥éž èº¬æ„Ÿè°¢ç»¿ç±³ç»™æˆ‘è¿™ä¸ªæœºä¼š~ æ„Ÿè°¢é˜…è¯»~ Have a happy dayðŸ˜Ž]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>CCF-GAIR</tag>
        <tag>AI</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[èµŒåšå°æ¸¸æˆçš„å¼€å‘ - éª°å®]]></title>
    <url>%2F2018%2F06%2F13%2F%E8%B5%8C%E5%8D%9A%E5%B0%8F%E6%B8%B8%E6%88%8F%E7%9A%84%E5%BC%80%E5%8F%91-%E9%AA%B0%E5%AE%9D-py%2F</url>
    <content type="text"><![CDATA[åŽ»è¿‡èµŒåœºçš„æœ‹å‹åº”è¯¥è§è¿‡è¿™ä¸ª â€”- éª°å®ï¼ˆSic Boï¼‰ éª°å­çš„è‹±æ–‡æ˜¯ diceï¼Œæœ‰devilâ€™s bonesdiceçš„è¯´æ³•ï¼Œæ‰€ä»¥google diceçš„æ—¶å€™ä¼šå‡ºçŽ°å¾ˆå¤šè¿™ç§äººåž‹dice~ä¸‹é¢æ˜¯éª°å®çš„å°å­ï¼Œä¸Šé¢å‡ ä¹Žæ¶µç›–æ‰€æœ‰çš„çŽ©æ³•ï¼š çŒœæ•°å­— çŒœå•åŒ æ¯”å¤§å° é€šæ€ å…¶ä»–ä¹‹å‰å¸®æœ‹å‹åšäº†ä¸‹è¿™ä¸ªæ¸¸æˆçš„è°ƒç ”å¼€å‘ï¼Œè§‰å¾—ä¸éš¾ã€‚å‰ææ˜¯äº†è§£å…¶ä¸­çš„è§„åˆ™ï¼Œå‚è€ƒè§„åˆ™ ä¸‹é¢çš„ä»£ç é‡Œï¼Œæ˜¯ç«™åœ¨åº„å®¶çš„è§’åº¦å†™çš„ moneyä¸ºæ­£çš„æ—¶å€™ï¼Œåº„å®¶èµšé’±ï¼›moneyä¸ºè´Ÿçš„æ—¶å€™ï¼Œåº„å®¶äºé’± Talk is cheap, show me the code.ç›´æŽ¥ä¸Šä»£ç 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307import numpy as npimport matplotlib.pyplot as plt import itertoolsdef cumsum(L): #if L&apos;s length is equal to 1 if L[:-1] == []: return L ret = cumsum(L[:-1]) ret.append(ret[-1] + L[-1]) return retchips = 0 # çŽ©å®¶çš„ç­¹ç é‡åˆå§‹å€¼è®¾ç½®#spend_chips = 0money_ALL = [] # æ¯å±€çš„ç›ˆäºé‡‘é¢ = åº„å®¶çš„ç­¹ç é‡money_accum_ALL = []ROUND_ALL = []for ii in range(1,11): money = [] # æ¯å±€çš„ç›ˆäºé‡‘é¢ = åº„å®¶çš„ç­¹ç é‡ money_accum = [] ROUND = [] for i in range(1,801): # å‡è®¾æ¯è½®çš„èµ”çŽ‡éƒ½æ˜¯å›ºå®šä¸å˜çš„ è¦å˜åŒ–çš„è¯å°±è‡ªåŠ¨è®¾ç½® ROUND.append(i) #----------------------------------------------------- #çŽ©å®¶ä¹°æŸ± #----------------------------------------------------- #chips = chips + 10000 # ç´¯è®¡å¢žåŠ ç­¹ç  chips = 10000 # æ¯å±€å¼€å§‹æœ‰10000ç­¹ç  # rest_chips = chips - spend_chips # ä¹°å¤§å° is_buy = np.random.randint(0,2) chips_big_small = np.random.randint(0,chips+1) * is_buy odds_big_small = 1 # èµ”çŽ‡è®¾ç½® if chips_big_small &gt; 0: guess_big_small = np.random.randint(0,2) # 0:çŒœå° 1:çŒœå¤§ # ä¹°å•åŒ is_buy = np.random.randint(0,2) chips_odd_even = np.random.randint(0,chips-chips_big_small+1) * is_buy odds_odd_even = 1 # èµ”çŽ‡è®¾ç½® if chips_odd_even &gt; 0: guess_odd_even = np.random.randint(0,2) # 0:çŒœåŒ 1:çŒœå• #ä¹°ä¸‰ä¸ªä¸€æ ·æ•°å­—ï¼ˆä»»æ„ä¸‰ä¸ªä¸€æ ·å°±è¡Œï¼‰ is_buy = np.random.randint(0,2) chips_num = np.random.randint(0,chips-chips_big_small-chips_odd_even+1) * is_buy if chips_num &gt; 0: guess_A = np.random.randint(1,7) guess_B = guess_A guess_C = guess_A #ä¹° 1 1 1 is_buy = np.random.randint(0,2) chips_num_1 = np.random.randint(0,chips-chips_big_small-chips_odd_even-chips_num+1) * is_buy if chips_num_1 &gt; 0: guess_A = 1 guess_B = guess_A guess_C = guess_A #ä¹° 2 2 2 is_buy = np.random.randint(0,2) chips_num_2 = np.random.randint(0,chips-chips_big_small-chips_odd_even-chips_num-chips_num_1+1) * is_buy if chips_num_2 &gt; 0: guess_A = 2 guess_B = guess_A guess_C = guess_A #ä¹° 3 3 3 is_buy = np.random.randint(0,2) chips_num_3 = np.random.randint(0,chips-chips_big_small-chips_odd_even-chips_num-chips_num_1-chips_num_2+1) * is_buy if chips_num_3 &gt; 0: guess_A = 3 guess_B = guess_A guess_C = guess_A #ä¹° 4 4 4 is_buy = np.random.randint(0,2) chips_num_4 = np.random.randint(0,chips-chips_big_small-chips_odd_even-chips_num-chips_num_1-chips_num_2-chips_num_3+1) * is_buy if chips_num_4 &gt; 0: guess_A = 4 guess_B = guess_A guess_C = guess_A #ä¹° 5 5 5 is_buy = np.random.randint(0,2) chips_num_5 = np.random.randint(0,chips-chips_big_small-chips_odd_even-chips_num-chips_num_1-chips_num_2-chips_num_3-chips_num_4+1) * is_buy if chips_num_5 &gt; 0: guess_A = 5 guess_B = guess_A guess_C = guess_A #ä¹° 6 6 6 is_buy = np.random.randint(0,2) chips_num_6 = np.random.randint(0,chips-chips_big_small-chips_odd_even-chips_num-chips_num_1-chips_num_2-chips_num_3-chips_num_4-chips_num_5+1) * is_buy if chips_num_6 &gt; 0: guess_A = 6 guess_B = guess_A guess_C = guess_A # ä¹°æ€»å’Œæ•° is_buy = np.random.randint(0,2) chips_sum_num = np.random.randint(0,chips-chips_big_small-chips_odd_even-chips_num-chips_num_1-chips_num_2-chips_num_3-chips_num_4-chips_num_5-chips_num_6+1) * is_buy if chips_sum_num &gt; 0: guess_sum_num = np.random.randint(4,18) if guess_sum_num in [4,17]: # èµ”çŽ‡è®¾ç½® odds_sum_num = 50 elif guess_sum_num in [5,16]: odds_sum_num = 18 elif guess_sum_num in [6,15]: odds_sum_num = 14 elif guess_sum_num in [7,14]: odds_sum_num = 12 elif guess_sum_num in [8,13]: odds_sum_num = 8 elif guess_sum_num in [9,10,11,12]: odds_sum_num = 6 # å…¨å›´ #----------------------------------------------------- # å¼€å±€ #----------------------------------------------------- A = np.random.randint(1,7) B = np.random.randint(1,7) C = np.random.randint(1,7) #----------------------------------------------------- # æ£€éªŒç»“æžœ #----------------------------------------------------- money_sum_num = 0 money_num = 0 money_big_small = 0 money_odd_even = 0 kill_money = 0 # é€šæ€æƒ…å†µ1 if chips_sum_num &gt; 0: result_sum_num = A+B+C if A == B &amp; B == C: # é€šæ€æƒ…å†µä¸‹ if guess_sum_num == result_sum_num &amp; guess_sum_num not in [3,18]: # é—²å®¶çŒœå¯¹äº† money_sum_num = -(chips_sum_num * 24) else: # çŒœé”™äº† kill_money = chips # é€šæ€ï¼Œå®Œæ¯• # é€šæ€æƒ…å†µ2 elif chips_num &gt; 0: if A == B &amp; B == C: # é€šæ€æƒ…å†µä¸‹ if guess_A == A: # é—²å®¶çŒœä¸­ money_num = -(chips_num * 24) # else: #kill_money = chips elif chips_num_1 &gt; 0: if A == B &amp; B == C &amp; B == 1 : # é€šæ€æƒ…å†µä¸‹ if guess_A == A: # é—²å®¶çŒœä¸­ money_num = -(chips_num_1 * 150) else: kill_money = chips elif chips_num_2 &gt; 0: if A == B &amp; B == C &amp; B == 2 : # é€šæ€æƒ…å†µä¸‹ if guess_A == A: # é—²å®¶çŒœä¸­ money_num = -(chips_num_1 * 150) else: kill_money = chips elif chips_num_3 &gt; 0: if A == B &amp; B == C &amp; B == 3 : # é€šæ€æƒ…å†µä¸‹ if guess_A == A: # é—²å®¶çŒœä¸­ money_num = -(chips_num_1 * 150) else: kill_money = chips elif chips_num_4 &gt; 0: if A == B &amp; B == C &amp; B == 4 : # é€šæ€æƒ…å†µä¸‹ if guess_A == A: # é—²å®¶çŒœä¸­ money_num = -(chips_num_1 * 150) else: kill_money = chips elif chips_num_5 &gt; 0: if A == B &amp; B == C &amp; B == 5 : # é€šæ€æƒ…å†µä¸‹ if guess_A == A: # é—²å®¶çŒœä¸­ money_num = -(chips_num_1 * 150) else: kill_money = chips elif chips_num_6 &gt; 0: if A == B &amp; B == C &amp; B == 6 : # é€šæ€æƒ…å†µä¸‹ if guess_A == A: # é—²å®¶çŒœä¸­ money_num = -(chips_num_1 * 150) else: kill_money = chips # ä¹°æ€»å’Œæ•° if chips_sum_num &gt; 0: if not (A == B &amp; B == C): if guess_sum_num == result_sum_num: #å¦‚æžœçŒœä¸­äº† money_sum_num = -(chips_sum_num*odds_sum_num) else: money_sum_num = chips_sum_num # ä¹°å¤§å° if chips_big_small &gt; 0: if A+B+C in list(range(1,11)): result_big_small = 0 #ç»“æžœä¸º å° else: result_big_small = 1 if guess_big_small == result_big_small: # å¦‚æžœçŒœå¯¹äº† money_big_small = -(chips_big_small*odds_big_small) # åº„å®¶ç»™ç›¸åº”çš„é’± else: money_big_small = chips_big_small # ä¹°å•åŒ if chips_odd_even &gt; 0: if (A+B+C)%2 == 1: # ç»“æžœä¸º å• result_odd_even = 1 else: result_odd_even = 0 if guess_odd_even == result_odd_even: money_odd_even = -(chips_odd_even*odds_odd_even) else: money_odd_even = chips_odd_even #----------------------------------------------------- # æ”¶é’±/ç»™é’± #----------------------------------------------------- # spend_chips = chips_big_small + chips_odd_even + chips_sum_num if kill_money == chips: money.append(kill_money) else: money.append(money_big_small + money_odd_even + money_sum_num) money_ALL.append(money) money_accum = cumsum(money) money_accum_ALL.append(money_accum) ROUND_ALL.append(ROUND) # plt.bar(ROUND,money) # plt.show() # plt.bar(ROUND,money_accum) # plt.show() # print(money) money_ALL = list(itertools.chain.from_iterable(money_ALL))money_accum_ALL = list(itertools.chain.from_iterable(money_accum_ALL))ROUND_ALL = list(itertools.chain.from_iterable(ROUND_ALL))ROUND_ALL = list(range(1,len(ROUND_ALL)+1))plt.bar(ROUND_ALL,money_ALL) plt.show() plt.bar(ROUND_ALL,money_accum_ALL) plt.show() ç»™å‡ºç»“æžœå›¾å¦‚ä¸‹ï¼šè¿™æ˜¯æ¯å±€çš„ç›ˆäºå±•ç¤ºï¼Œå¯ä»¥çœ‹å‡ºåœ¨ç¬¬4000å±€å·¦å³çš„æ—¶å€™ï¼Œåº„å®¶å¤§äºäº†ä¸€æŠŠè¿™æ˜¯ç›ˆäºç´¯ç§¯å±•ç¤ºï¼Œåœ¨æœ€ç»ˆï¼Œ8000å±€å¤§æˆ˜ä¹‹åŽï¼Œè¿˜æ˜¯åº„å®¶èµ¢äº†15ä¸‡å·¦å³ ä¸åŒå›½å®¶ï¼Œä¸åŒåœºå­çš„çŽ©æ³•è§„åˆ™ä¼šæœ‰ä¸ä¸€æ ·ï¼Œä¸Šé¢çš„codeä¹Ÿä¸èƒ½åŒ…å«æ‰€æœ‰è§„åˆ™ å¦å¤–ï¼Œpythonç›¸å…³æ–‡ä»¶è¿˜æ˜¯å»ºè®®ä»¥è‹±æ–‡å‘½åï¼Œæ ‡é¢˜åªæ˜¯èµ·åˆ°ç¼–ç¨‹è¯­è¨€æ ‡è¯†çš„ä½œç”¨ èµŒåœºè¿˜æœ‰å¾ˆå¤šå…¶ä»–æ¸¸æˆï¼Œå¦‚æžœä½ æƒ³è´¥åå…´è‡´ï¼Œå¯ä»¥åƒæˆ‘è¿™ä¹ˆåšðŸ™ƒ]]></content>
      <tags>
        <tag>python</tag>
        <tag>game</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learningç¬”è®° - XGBOOST æ•™ç¨‹]]></title>
    <url>%2F2018%2F06%2F01%2FXGBOOST%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[èƒŒæ™¯è¯´æ˜Žï¼š XGBOOSTï¼Œå± æ¦œç¥žå™¨ï¼ å…¨ç§°ï¼šeXtreme Gradient Boosting | ç®€ç§°ï¼šXGB XGBä½œè€…ï¼šé™ˆå¤©å¥‡ï¼ˆåŽç››é¡¿å¤§å­¦ï¼‰ï¼Œmy iconâ¤ XGBå‰èº«ï¼šGBDT(Gradient Boosting Decision Tree)ï¼ŒXGBæ˜¯ç›®å‰å†³ç­–æ ‘çš„é¡¶é…ã€‚ æ³¨æ„ï¼ä¸Šå›¾å¾—å‡ºè¿™ä¸ªç»“è®ºæ—¶é—´ï¼š2016å¹´3æœˆï¼Œä¸¤å¹´å‰ï¼Œç®—æ³•å‘å¸ƒåœ¨2014å¹´ï¼ŒçŽ°åœ¨æ˜¯2018å¹´6æœˆï¼Œå®ƒä»æ˜¯ç®—æ³•å±Šçš„superstarðŸŒŸï¼ ç›®å‰ï¼Œåœ¨æ‰€æœ‰å£°åæ˜¾èµ«çš„æ•°æ®æŒ–æŽ˜èµ›åœºä¸Šï¼ˆkaggle/å¤©æ± /â€¦ï¼‰ï¼Œè¿™ä¸ªç®—æ³•æ— äººä¸çŸ¥ï¼Œslayå…¨åœºã€‚ æ³¨ï¼š é€‚ç”¨äººç¾¤ï¼šæœºå™¨å­¦ä¹ ï¼ˆæ•°æ®æŒ–æŽ˜ï¼‰å¤§èµ›é€‰æ‰‹ / (å‡†)äººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆ / ç®—æ³•æ•ˆæžœé‡åˆ°ç“¶é¢ˆçš„æœ‹å‹ / â€¦ å‡è®¾ï¼šè¯»è€…ç†è§£å›žå½’æ ‘ç®—æ³•ã€æ³°å‹’å…¬å¼ã€æ¢¯åº¦ä¸‹é™æ³•å’Œç‰›é¡¿æ³•ï¼Œç®€å•è¯´å°±æ˜¯GBDTï¼Œé¡ºä¾¿ï¼ŒAdaboostä¹Ÿå¯ä»¥äº†è§£ä¸€ä¸‹ã€‚ When learning XGBoost, be calm and be patient. å› ä¸ºXGBå¾ˆå±Œï¼Œæ‰€ä»¥æœ¬æ–‡å¾ˆé•¿ï¼Œå¯ä»¥æ…¢æ…¢çœ‹ï¼Œæˆ–è€…ä¸€æ¬¡çœ‹ä¸€éƒ¨åˆ†ï¼Œitâ€™s ok~ é“¾æŽ¥ðŸ”—ï¼š XGBoost: A Scalable Tree Boosting Systemã€XGBçš„åŽŸè‘—è®ºæ–‡ã€‘ Introduction to Boosted Treesã€å¤©å¥‡å¤§ç¥žçš„pptã€‘ æ­£æ–‡ï¼š [1] ç®—æ³•åŽŸç†ç®€è¿°ï¼ˆåŸºäºŽä¸Šé¢é™ˆå¤©å¥‡çš„PPTï¼‰ï¼š (1) Review of key concepts of supervised learning | ç›‘ç£å­¦ä¹ çš„ä¸»è¦å…ƒç´  Yå€¼ï¼ˆlabelæ ‡ç­¾ï¼‰ ç›®æ ‡å‡½æ•°ï¼ˆObjective Functionï¼‰= æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰+ æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰ æŸå¤±å‡½æ•°è¡¨ç¤ºæ¨¡åž‹å¯¹è®­ç»ƒæ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ï¼Œlossè¶Šå°ï¼Œä»£è¡¨æ¨¡åž‹é¢„æµ‹çš„è¶Šå‡†ã€‚ æ­£åˆ™åŒ–é¡¹è¡¡é‡æ¨¡åž‹çš„å¤æ‚åº¦ï¼Œregularizationè¶Šå°ï¼Œä»£è¡¨æ¨¡åž‹æ¨¡åž‹çš„å¤æ‚åº¦è¶Šä½Žã€‚ ç›®æ ‡å‡½æ•°è¶Šå°ï¼Œä»£è¡¨æ¨¡åž‹è¶Šå¥½ã€‚ (2) Regression Tree and Ensemble | å½“ä½ è°ˆå†³ç­–æ ‘æ—¶ä½ åœ¨è°ˆä»€ä¹ˆTree Ensemble methodsçš„å¥½å¤„ï¼š Very widely used.Almost half of data mining competition are won by using some variants of tree ensemble methods.è¢«å¤§è§„æ¨¡çš„ä½¿ç”¨ï¼Œå‡ ä¹Žä¸€åŠçš„æ•°æ®æŒ–æŽ˜æ¯”èµ›å† å†›é˜Ÿéƒ½åœ¨ç”¨é›†åˆæ ‘æ¨¡åž‹ Invariant to scaling of inputs, so you do not need to do careful features normalization.ä¸Žè¾“å…¥æ•°æ®çš„å–å€¼èŒƒå›´æ— å…³ï¼Œæ‰€ä»¥æ— éœ€åšå¾ˆç»†è‡´çš„ç‰¹å¾å½’ä¸€åŒ– Learn higher order interaction between features.èƒ½å¤Ÿå­¦ä¹ åˆ°ç‰¹å¾é—´çš„é«˜ç»´ç›¸å…³æ€§ Can be scalable, and are used in Industry.å·¥ä¸šä½¿ç”¨ï¼Œæ‰©å±•æ€§å¥½ åœ¨è¿™é¡µï¼Œæ¨¡åž‹å¤æ‚åº¦ï¼ˆfunction spaceï¼‰æ˜¯ç”±æ‰€æœ‰çš„å›žå½’æ ‘å†³å®šçš„ã€‚ å­¦ä¹ çš„æ˜¯fkï¼ˆæ ‘ï¼‰ï¼Œè€Œä¸æ˜¯æƒé‡wâ€”â€”ä½“çŽ°gradientçš„æ€æƒ³ã€‚ ä¿¡æ¯å¢žç›Šï¼ˆInformation Gainï¼‰ï¼šå†³å®šåˆ†è£‚èŠ‚ç‚¹ï¼Œä¸»è¦æ˜¯ä¸ºäº†å‡å°‘æŸå¤±loss æ ‘çš„å‰ªæžï¼šä¸»è¦ä¸ºäº†å‡å°‘æ¨¡åž‹å¤æ‚åº¦ï¼Œè€Œå¤æ‚åº¦è¢«â€˜æ ‘æžçš„æ•°é‡â€™å½±å“ æœ€å¤§æ·±åº¦ï¼šä¼šå½±å“æ¨¡åž‹å¤æ‚åº¦ å¹³æ»‘å¶å­çš„å€¼ï¼šå¯¹å¶å­çš„æƒé‡è¿›è¡ŒL2æ­£åˆ™åŒ–ï¼Œä¸ºäº†å‡å°‘æ¨¡åž‹å¤æ‚åº¦ï¼Œæé«˜æ¨¡åž‹çš„ç¨³å®šæ€§ å›žå½’æ ‘ä¸æ­¢ç”¨äºŽåšå›žå½’ï¼Œè¿˜å¯ä»¥åšåˆ†ç±»ã€æŽ’åºç­‰ï¼Œä¸»è¦ä¾èµ–äºŽç›®æ ‡å‡½æ•°çš„å®šä¹‰ (3) Gradient Boosting (How do we Learn) Bias-variance tradeoff is everywhereåå·®ä¸Žæ–¹å·®çš„æƒè¡¡æ— å¤„ä¸åœ¨ The loss + regularization objective pattern applies for regression tree learning (function learning)æŸå¤±+æ­£åˆ™çš„æ¨¡å¼é€‚ç”¨äºŽå›žå½’æ ‘å­¦ä¹  We want predictive and simple functionsé¢„æµ‹æ¨¡åž‹çš„å‡ºè·¯åœ¨å“ªé‡Œï¼Œç»“æžœå¦‚ä¸‹ï¼š ä½¿ç”¨äºŒé˜¶æ³°å‹’å±•å¼€å¼æ¥è¿‘ä¼¼Lossï¼š ç®­å¤´æ‰€æŒ‡çš„å°±æ˜¯XGBçš„ç›®æ ‡å‡½æ•°è¡¨è¾¾å¼ï¼ŒObjç›®æ ‡å‡½æ•° = æŸå¤±å‡½æ•° + æ­£åˆ™é¡¹ + å¸¸æ•°é¡¹ï¼Œæ˜¯ä¸ªä¼˜ç§€çš„è¡¨è¾¾å¼ï¼ŒåŽé¢ä¼šè§£é‡Š æœ¬ç¯‡åªæ˜¯æäº†äº›åŸºæœ¬çš„æ¦‚å¿µï¼Œå…¶å®ƒsliceè§£è¯»è¯·å‚é˜…å®˜æ–¹ä»‹ç»æˆ–è€…é™ˆå¤©å¥‡slideå­¦ä¹ ç¬”è®°æˆ–è€…XGBoostç®—æ³•åŽŸç† [2] å‚æ•°è¯´æ˜Žï¼šXGBçš„å‚æ•°æ˜¯ç›®å‰è§è¿‡çš„æ¨¡åž‹é‡Œæœ€å¤šçš„ï¼Œé¢è¯•è¢«é—®åˆ°å°±çžŽäº†ï¼Œå¦‚æžœä½ æ˜¯ç¬¬ä¸€æ¬¡çœ‹æ‰€æœ‰çš„å‚æ•°ï¼Œè¯·åšå¥½å¿ƒç†å‡†å¤‡~ä¸‹é¢åªåˆ—ä¸¾éƒ¨åˆ†å¸¸ç”¨å‚æ•°ï¼Œæ‰€æœ‰å‚æ•°çš„å®˜æ–¹è¯´æ˜Žæ–‡æ¡£ï¼Œè¯·ç‚¹å‡»XGBoost Parameters (1) General parameters (2) Booster parameters Parameters for Tree Booster Additional parameters for Dart Booster Parameters for Linear Booster and Tweedie Regression(3) Learning Task parameters(4) Command line parameters XGBçš„ä¸‹è½½æ•™ç¨‹ï¼Œå¦‚æžœä¸æˆåŠŸï¼Œå¤šgoogle [3] ä»£ç å®žçŽ°ï¼šRè¯­è¨€ç‰ˆæœ¬(1) å¯¼å…¥æ•°æ®data0.RData ä¸‹è½½ï¼Œä»…ä½œXGBæµç¨‹å±•ç¤ºï¼Œä¸åšæ•°æ®æ¸…æ´—å¦‚æžœå¯¹æ•°æ®æ¸…æ´—æ„Ÿå…´è¶£ï¼Œè¯·ç‚¹å‡»åŸºäºŽRçš„æ•°æ®æ¸…æ´—ï¼ˆ1ï¼‰æ ·æœ¬æ•°æ®æ˜¯RDataæ ¼å¼çš„ï¼Œæ˜¯Rä¸“æœ‰çš„æ•°æ®å­˜å‚¨æ ¼å¼ï¼Œå¥½ç”¨åˆä¸å åœ°æ–¹~1234567891011121314# å¯¼å…¥åŒ…packages&lt;-c(&quot;data.table&quot;,&quot;xgboost&quot;,&quot;ggplot2&quot;,&quot;dplyr&quot;)UsePackages&lt;-function(p)&#123; if (!is.element(p,installed.packages()[,1]))&#123; install.packages(p)&#125; require(p,character.only = TRUE)&#125;for(p in packages)&#123; UsePackages(p)&#125;library(data.table)library(xgboost)library(ggplot2)library(dplyr) å¯¼å…¥æ•°æ®ï¼š12setwd(&quot;D:/Zhang&quot;) # Ræ–‡ä»¶è®¾ç½®è·¯å¾„load(&quot;data/data0.RData&quot;) # å¯¼å…¥æ•°æ® æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè½¬æ¢æ•°æ®æ ¼å¼ï¼š12345678910111213#----------------------------------------------------------# train &amp; test select randomly#----------------------------------------------------------a = round(nrow(data0)*0.8)b = sample(nrow(data0), a, replace = FALSE, prob = NULL)train= data0[b,] # è®­ç»ƒé›†80%test = data0[-b,] # æµ‹è¯•é›†20%# å°†dataframeæ ¼å¼è½¬æ¢æˆxgb.DMatrixæ ¼å¼# Yå€¼çš„åˆ—å: &apos;bad&apos;dtrain &lt;- xgb.DMatrix(data=select(train,-bad)%&gt;%as.matrix,label= train$bad%&gt;%as.matrix) æ³¨ï¼šYå€¼çš„ç‰¹å¾åæ˜¯â€˜badâ€™ (2) åˆ©ç”¨ xgb.cv è°ƒå‚12345678910111213141516171819202122232425262728293031323334353637383940best_param = list()best_seednumber = 1234best_logloss = Infbest_logloss_index = 0# è‡ªå®šä¹‰è°ƒå‚ç»„åˆfor (iter in 1:50) &#123; param &lt;- list(objective = &quot;binary:logistic&quot;, # ç›®æ ‡å‡½æ•°ï¼šlogisticçš„äºŒåˆ†ç±»æ¨¡åž‹ï¼Œå› ä¸ºYå€¼æ˜¯äºŒå…ƒçš„ eval_metric = c(&quot;logloss&quot;), # è¯„ä¼°æŒ‡æ ‡ï¼šlogloss max_depth = sample(6:10, 1), # æœ€å¤§æ·±åº¦çš„è°ƒèŠ‚èŒƒå›´ï¼š1ä¸ª 6-10 åŒºé—´çš„æ•° eta = runif(1, .01, .3), # etaæ”¶ç¼©æ­¥é•¿è°ƒèŠ‚èŒƒå›´ï¼š1ä¸ª 0.01-0.3åŒºé—´çš„æ•° gamma = runif(1, 0.0, 0.2), # gammaæœ€å°æŸå¤±è°ƒèŠ‚èŒƒå›´ï¼š1ä¸ª 0-0.2åŒºé—´çš„æ•° subsample = runif(1, .6, .9), colsample_bytree = runif(1, .5, .8), min_child_weight = sample(1:40, 1), max_delta_step = sample(1:10, 1) ) cv.nround = 50 # è¿­ä»£æ¬¡æ•°ï¼š50 cv.nfold = 5 # 5æŠ˜äº¤å‰éªŒè¯ seed.number = sample.int(10000, 1)[[1]] set.seed(seed.number) mdcv &lt;- xgb.cv(data=dtrain, params = param, nthread=6, metrics=c(&quot;auc&quot;,&quot;rmse&quot;,&quot;error&quot;), nfold=cv.nfold, nrounds=cv.nround, watchlist = list(), verbose = F, early_stop_round=8, maximize=FALSE) min_logloss = min(mdcv$evaluation_log[,test_logloss_mean]) min_logloss_index = which.min(mdcv$evaluation_log[,test_logloss_mean]) if (min_logloss &lt; best_logloss) &#123; best_logloss = min_logloss best_logloss_index = min_logloss_index best_seednumber = seed.number best_param = param &#125;&#125;(nround = best_logloss_index)set.seed(best_seednumber)best_seednumber(best_param) # æ˜¾ç¤ºæœ€ä½³å‚æ•°ç»„åˆï¼Œåˆ°åŽé¢çœŸæ­£çš„æ¨¡åž‹è¦ç”¨ å¾—åˆ°æœ€ä½³å‚æ•°ç»„åˆï¼š (3) ç»˜åˆ¶ auc | rmse | error æ›²çº¿ 123456789101112131415161718192021222324252627282930313233343536373839404142434445#mdcv$evaluation_logxgb_plot=function(input,output)&#123; history=input train_history=history[,1:8]%&gt;%mutate(id=row.names(history),class=&quot;train&quot;) test_history=history[,9:16]%&gt;%mutate(id=row.names(history),class=&quot;test&quot;) colnames(train_history)=c(&quot;logloss.mean&quot;,&quot;logloss.std&quot;,&quot;auc.mean&quot;,&quot;auc.std&quot;,&quot;rmse.mean&quot;,&quot;rmse.std&quot;,&quot;error.mean&quot;,&quot;error.std&quot;,&quot;id&quot;,&quot;class&quot;) colnames(test_history)=c(&quot;logloss.mean&quot;,&quot;logloss.std&quot;,&quot;auc.mean&quot;,&quot;auc.std&quot;,&quot;rmse.mean&quot;,&quot;rmse.std&quot;,&quot;error.mean&quot;,&quot;error.std&quot;,&quot;id&quot;,&quot;class&quot;) his=rbind(train_history,test_history) his$id=his$id%&gt;%as.numeric his$class=his$class%&gt;%factor if(output==&quot;auc&quot;)&#123; auc=ggplot(data=his,aes(x=id, y=auc.mean,ymin=auc.mean-auc.std,ymax=auc.mean+auc.std,fill=class),linetype=class)+ geom_line()+ geom_ribbon(alpha=0.5)+ labs(x=&quot;nround&quot;,y=NULL,title = &quot;XGB Cross Validation AUC&quot;)+ theme(title=element_text(size=15))+ theme_bw() return(auc) &#125; if(output==&quot;rmse&quot;)&#123; rmse=ggplot(data=his,aes(x=id, y=rmse.mean,ymin=rmse.mean-rmse.std,ymax=rmse.mean+rmse.std,fill=class),linetype=class)+ geom_line()+ geom_ribbon(alpha=0.5)+ labs(x=&quot;nround&quot;,y=NULL,title = &quot;XGB Cross Validation RMSE&quot;)+ theme(title=element_text(size=15))+ theme_bw() return(rmse) &#125; if(output==&quot;error&quot;)&#123; error=ggplot(data=his,aes(x=id,y=error.mean,ymin=error.mean-error.std,ymax=error.mean+error.std,fill=class),linetype=class)+ geom_line()+ geom_ribbon(alpha=0.5)+ labs(x=&quot;nround&quot;,y=NULL,title = &quot;XGB Cross Validation ERROR&quot;)+ theme(title=element_text(size=15))+ theme_bw() return(error) &#125;&#125; auc1xgb_plot(mdcv$evaluation_log[,-1]%&gt;%data.frame,&quot;auc&quot;) è®­ç»ƒé›†ä¸Žæµ‹è¯•é›†çš„è¡¨çŽ°å·®è·æœ‰ç‚¹å¤§ï¼Œå¯èƒ½å‡ºçŽ°è¿‡æ‹Ÿåˆ rmse1xgb_plot(mdcv$evaluation_log[,-1]%&gt;%data.frame,&quot;rmse&quot;) è®­ç»ƒé›†ä¸Žæµ‹è¯•é›†çš„è¡¨çŽ°è¾ƒç»Ÿä¸€ï¼Œä½†æ˜¯è¿™ä¸ªæ•°å€¼è¿˜æ˜¯åé«˜ error1xgb_plot(mdcv$evaluation_log[,-1]%&gt;%data.frame,&quot;error&quot;) æµ‹è¯•é›†çš„è¡¨çŽ°éžå¸¸ä¸ç¨³å®šï¼Œerrorå€¼åé«˜ æ€»çš„æ¥è¯´æ¨¡åž‹éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ï¼Œä½†æ˜¯ä½œä¸ºXGBåŠŸèƒ½ä»¥åŠæµç¨‹çš„å±•ç¤ºï¼Œæœ¬ç¯‡ä¸åšç»†è‡´è°ƒæ•´ï¼Œç»§ç»­ä¸‹ä¸€æ­¥ï¼ (4) å»ºç«‹æ¨¡åž‹æ ¹æ®è½¬æ¢åŽçš„æ•°æ®æ ¼å¼dtrainï¼Œè°ƒå‚ç»“æžœçš„æœ€ä½³å‚æ•°ç»„åˆbest_paramï¼Œæœ€ä½³è¿­ä»£æ¬¡æ•°nroundæ¥å»ºæ¨¡1model &lt;- xgb.train(data=dtrain, params=best_param, nrounds=nround, nthread=6, watchlist = list()) (5) ç»˜åˆ¶ImportanceæŽ’åºå›¾123456789101112131415importanceRaw &lt;- xgb.importance(feature_names=colnames(dtrain), model = model)xgb.ggplot.importance(importanceRaw) # importance å°±æ˜¯ ä¿¡æ¯å¢žç›Š# #--------------------------------------------------------------------------------------# # feature selection # è¿™é‡Œå¯ä»¥æ ¹æ®importanceè®¾ç½®é˜ˆå€¼ï¼Œè¿›è¡Œç‰¹å¾ç­›é€‰ï¼Œè¿™æ˜¯ç‰¹å¾ç­›é€‰çš„æ–¹å¼ä¹‹ä¸€# cum_impt=data.frame(names=importanceRaw$Feature,impt=cumsum(importanceRaw$Importance))# cum_impt=filter(cum_impt,cum_impt$impt&lt;0.9)# selected_feature&lt;-cum_impt$names# # train=select(train,selected_feature)# dtrain&lt;- xgb.DMatrix(data=select(train,-bad)%&gt;%as.matrix,label= train$bad%&gt;%as.matrix)# # model &lt;- xgb.train(data=dtrain, params=best_param, nrounds=nround, nthread=6, watchlist = list())# #-------------------------------------------------------------------------------------- ä¸Šå›¾ä»£è¡¨ç‰¹å¾çš„é‡è¦æ€§æŽ’åºï¼Œå¯ä»¥è®¾ç½®é‡è¦æ€§é˜ˆå€¼ï¼Œè¿›è¡Œç‰¹å¾ç­›é€‰ã€‚ (6) è¿›è¡Œé¢„æµ‹12dtest=select(test,-bad) # &apos;bad&apos;æ˜¯Yå€¼yhat=predict(model,as.matrix(dtest),missing=NA) (7) ä¿å­˜æ¨¡åž‹æ–‡ä»¶1save(model, file = &quot;model/model_xgb.rda&quot;) ä¸‹æ¬¡ä½¿ç”¨æ—¶ï¼Œèƒ½ç›´æŽ¥å¯¼å…¥è®­ç»ƒå¥½çš„æ¨¡åž‹ï¼Œè¿›è¡Œé¢„æµ‹ã€‚ [4] ä»£ç å®žçŽ°ï¼šPythonç‰ˆæœ¬xgbçš„æ›´æ–°è¿­ä»£ç‰¹åˆ«å¿«ï¼Œç›®å‰åœ¨Windowsä¸Šçš„å®‰è£…å°±å¾ˆçƒ§è„‘ï¼Œå¸Œæœ›ä½›ç³»å®‰è£…ä¸€ä¸‹ä¸æä¾›æºæ•°æ®ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥åŽ»æ‰¾åˆ†ç±»çš„æ•°æ®è¯•ç€è·‘ä¸€ä¸‹ (1) æ‹†åˆ†æ•°æ®é›†ä»»ä½•æŠ¥é”™no moduleçš„åŒ…éƒ½è¯·è‡ªè¡Œpipå®‰è£…ä¸‹æ¥123456789101112131415161718192021222324# å¯¼å…¥åŒ…import os os.chdir(&quot;C:/Users/Yi/Desktop/abc&quot;) # è®¾ç½®æ–‡ä»¶è·¯å¾„import randomimport pandas as pdimport matplotlib.pyplot as pltimport numpy as npimport xgboost as xgbfrom numpy import sortfrom xgboost import plot_importance,XGBClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_selection import SelectFromModelfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,mean_squared_errorfrom ggplot import *from sklearn.externals import joblib# split data into X and YX = tmp_df # ç‰¹å¾é›†ï¼Œæ•°æ®è¯·è‡ªè¡Œæä¾›Y = label_Y # æ ‡ç­¾é›†ï¼Œæ•°æ®è¯·è‡ªè¡Œæä¾›# split data into train and test sets # æ‹†åˆ†æ•°æ®é›†X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=7) (2) APIæŽ¥å£è¯´æ˜Žæˆªè‡³ 2018/6 ï¼Œxgb model æœ‰ä¸¤ä¸ªæŽ¥å£ï¼Œç‚¹å‡»æŽ¥å£æ–‡ä»¶æŽ¥å£æ–‡ä»¶å€¼å¾—åå¤é˜…è¯»ç†Ÿæ‚‰ä¸€ä¸‹ï¼Œä¸Žå‚æ•°è¯´æ˜Žä¸€èµ·é£Ÿç”¨æ›´ä½³~ XGB Learning API ( import xgboost ) Scikit-Learn API ( from xgboost import XGBClassifier ) (3) XGBè°ƒå‚ æ–¹æ³•ä¸€ï¼š ç›´æŽ¥è°ƒå‚ï¼Œè°ƒç”¨ xgbooståŒ… çš„ XGBClassifier()å¯ä»¥å¯¹å…¶å‚æ•°è¿›è¡Œæ‰‹åŠ¨ä¿®æ”¹ï¼Œdefaultå‚æ•°å¦‚ä¸‹ æ–¹æ³•äºŒï¼š éšæœºè°ƒå‚ï¼Œä½¿ç”¨ xgb.cv 123456789101112131415161718192021222324252627282930313233343536373839404142best_param = list()best_seednumber = 123best_logloss = np.Infbest_logloss_index = 0dtrain = xgb.DMatrix(X_train, y_train, feature_names = list(X_train))# è‡ªå®šä¹‰è°ƒå‚ç»„åˆ------------------------------------for iter in range(50): param = &#123;&apos;objective&apos; : &quot;binary:logistic&quot;, # ç›®æ ‡å‡½æ•°ï¼šlogisticçš„äºŒåˆ†ç±»æ¨¡åž‹ï¼Œå› ä¸ºYå€¼æ˜¯äºŒå…ƒçš„ &apos;max_depth&apos; : np.random.randint(6,11), # æœ€å¤§æ·±åº¦çš„è°ƒèŠ‚èŒƒå›´ &apos;eta&apos; : np.random.uniform(.01, .3), # etaæ”¶ç¼©æ­¥é•¿è°ƒèŠ‚èŒƒå›´ &apos;gamma&apos; : np.random.uniform(0.0, 0.2), # gammaæœ€å°æŸå¤±è°ƒèŠ‚èŒƒå›´ &apos;subsample&apos; : np.random.uniform(.6, .9), &apos;colsample_bytree&apos; : np.random.uniform(.5, .8), &apos;min_child_weight&apos; : np.random.randint(1,41), &apos;max_delta_step&apos; : np.random.randint(1,11)&#125; cv_nround = 50 # è¿­ä»£æ¬¡æ•°ï¼š50 cv_nfold = 5 # 5æŠ˜äº¤å‰éªŒè¯ seed_number = np.random.randint(0ï¼Œ100) random.seed(seed_number) mdcv &lt;- xgb.cv(params = param, dtrain=dtrain, metrics=[&quot;auc&quot;,&quot;rmse&quot;,&quot;error&quot;,&quot;logloss&quot;], nfold=cv_nfold, num_boost_round=cv_nround, verbose_eval = None, early_stopping_rounds=8, maximize=False) min_logloss = min(mdcv[&apos;test-logloss-mean&apos;]) min_logloss_index = mdcv.index[mdcv[test-logloss-mean] == min(mdcv[test-logloss-mean])][0] if min_logloss &lt; best_logloss: best_logloss = min_logloss best_logloss_index = min_logloss_index best_seednumber = seed_number best_param = paramrandom.seed(best_seednumber)nround = best_logloss_indexprint(&apos;best_round = %d, best_seednumber = %d&apos; %(nround,best_seednumber))print(&apos;best_param : ------------------------------&apos;)print(best_param) # æ˜¾ç¤ºæœ€ä½³å‚æ•°ç»„åˆï¼Œåˆ°åŽé¢çœŸæ­£çš„æ¨¡åž‹è¦ç”¨ æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ gridsearch å’Œ cross validationå‚è€ƒ Complete Guide to Parameter Tuning in XGBoost (4) ç»˜åˆ¶ train/test çš„ auc/rmse/errorå®šä¹‰å‡½æ•°1234567891011121314151617181920212223242526272829303132333435363738394041424344def xgb_plot(input,output): history=input train_history=history.iloc[:,8:16].assign(id=[i+1 for i in history.index]) train_history[&apos;Class&apos;] = &apos;train&apos; test_history=history.iloc[:,0:8].assign(id=[i+1 for i in history.index]) test_history[&apos;Class&apos;] = &apos;test&apos; train_history.columns = [&quot;auc_mean&quot;,&quot;auc_std&quot;,&quot;error_mean&quot;,&quot;error_std&quot;,&quot;logloss_mean&quot;,&quot;logloss_std&quot;,&quot;rmse_mean&quot;,&quot;rmse_std&quot;,&quot;id&quot;,&quot;Class&quot;] test_history.columns = [&quot;auc_mean&quot;,&quot;auc_std&quot;,&quot;error_mean&quot;,&quot;error_std&quot;,&quot;logloss_mean&quot;,&quot;logloss_std&quot;,&quot;rmse_mean&quot;,&quot;rmse_std&quot;,&quot;id&quot;,&quot;Class&quot;] his=pd.concat([train_history,test_history]) if output==&quot;auc&quot;: his[&apos;y_min_auc&apos;] = his[&apos;auc_mean&apos;]-his[&apos;auc_std&apos;] his[&apos;y_man_auc&apos;] = his[&apos;auc_mean&apos;]+his[&apos;auc_std&apos;] auc=ggplot(his,aes(x=&apos;id&apos;, y=&apos;auc.mean&apos;, ymin=&apos;y_min_auc&apos;, ymax=&apos;y_man_auc&apos;,fill=Class)+\ geom_line()+\ geom_ribbon(alpha=0.5)+\ labs(x=&quot;nround&quot;,y=&apos;&apos;,title = &quot;XGB Cross Validation AUC&quot;) return(auc) if output==&quot;rmse&quot;: his[&apos;y_min_rmse&apos;] = his[&apos;rmse_mean&apos;]-his[&apos;rmse_std&apos;] his[&apos;y_man_rmse&apos;] = his[&apos;rmse_mean&apos;]+his[&apos;rmse_std&apos;] rmse=ggplot(his,aes(x=&apos;id&apos;, y=&apos;rmse.mean&apos;,ymin=&apos;y_min_rmse&apos;,ymax=&apos;y_man_rmse&apos;,fill=Class))+\ geom_line()+\ geom_ribbon(alpha=0.5)+\ labs(x=&quot;nround&quot;,y=&apos;&apos;,title = &quot;XGB Cross Validation RMSE&quot;) return(rmse) if output==&quot;error&quot;: his[&apos;y_min_error&apos;] = his[&apos;error_mean&apos;]-his[&apos;error_std&apos;] his[&apos;y_man_error&apos;] = his[&apos;error_mean&apos;]+his[&apos;error_std&apos;] error=ggplot(his,aes(x=&apos;id&apos;,y=&apos;error.mean&apos;,ymin=&apos;y_min_error&apos;,ymax=&apos;y_man_error&apos;,fill=Class))+\ geom_line()+\ geom_ribbon(alpha=0.5)+\ labs(x=&quot;nround&quot;,y=&apos;&apos;,title = &quot;XGB Cross Validation ERROR&quot;) return(error) æ¨ªåæ ‡æ˜¯è¿­ä»£æ¬¡æ•°ï¼Œå¯ä»¥è§‚å¯Ÿè¿­ä»£æ—¶æ˜¯å¦è¿‡æ‹Ÿåˆ trainæ›²çº¿å’Œtestæ›²çº¿çš„ç›¸å·®ç¨‹åº¦ï¼Œå¯ä»¥ä¾§é¢åæ˜ æ¨¡åž‹å¤æ‚åº¦ï¼Œæ£€éªŒæ˜¯å¦è¿‡æ‹Ÿåˆ1xgb_plot(mdcv,&apos;auc&apos;) 1xgb_plot(mdcv,&apos;rmse&apos;) 1xgb_plot(mdcv,&apos;error&apos;) (5) å»ºæ¨¡ï¼Œè¿›è¡Œé¢„æµ‹ï¼Œæ‰“å°è¯„ä¼°æŒ‡æ ‡ æ–¹æ³•ä¸€ï¼š ä½¿ç”¨ xgboost.train 1234567891011121314151617# åˆ©ç”¨ä¸Šé¢è°ƒå‚ç»“æžœï¼š best_parammd_1 = xgb.train(best_param, dtrain, num_boost_round=nround)# é¢„æµ‹dtest = xgb.DMatrix(X_test, feature_names=list(X_test))preds = md_1.predict(dtest)print(mean_square_error(y_test, preds))predictions = [round(value) for value in preds]accuracy = accuracy_score(y_test, predictions)f1_score = f1_score(y_test,predictions)print(&quot;Accuracy: %.2f%%&quot; %(accuracy * 100.0))print(&quot;F1 Score: %.2f%%&quot; %(f1_score * 100.0))# save modelmd_1.save_model(&apos;xgb.model&apos;) æ–¹æ³•äºŒï¼š ä½¿ç”¨ XGBClassifier() 1234567891011121314151617# ç”±äºŽ xgb.train ä¸Ž XGBClassifier() æœ‰éƒ¨åˆ†å‚æ•°çš„åå­—ç¨æœ‰å‡ºå…¥ï¼Œå…·ä½“å‚è€ƒAPIæŽ¥å£æ–‡æ¡£best_param[&apos;learning_rate&apos;] = best_param.pop(&apos;eta&apos;) # ä¿®æ”¹å‚æ•°å­—å…¸çš„æŸä¸ªkeyåå­—best_param.update(&#123;&apos;colsample_bytree&apos;: 1&#125;) # å–æ¶ˆåˆ—æŠ½æ ·ï¼Œä¿®æ”¹å‚æ•°å­—å…¸çš„æŸä¸ªvaluemd_2 = XGBClassifier(**best_param) # 2ä¸ª*å·ï¼Œå…è®¸ç›´æŽ¥å¡«å…¥å­—å…¸æ ¼å¼çš„parammd_2.fit(X_train, y_train) ypred = md_2.predict(X_test)predictions = [round(value) for value in ypred]# æ‰“å°è¯„ä¼°æŒ‡æ ‡MSE = mean_squared_error(y_test, predictions)print(&quot;MSE: %.2f%%&quot; % (MSE * 100.0)) accuracy = accuracy_score(y_test, predictions)print(&quot;Accuracy: %.2f%%&quot; % (accuracy * 100.0))f1_score = f1_score(y_test, predictions)print(&quot;F1 Score: %.2f%%&quot; % (f1_score * 100.0)) (6) ç»˜åˆ¶ImportanceæŽ’åºå›¾1234ax = xgb.plot_importance(md_2, height=0.5)fig = ax.figurefig.set_size_inches(25,20) # å¯è°ƒèŠ‚å›¾ç‰‡å°ºå¯¸å’Œç´§å¯†ç¨‹åº¦plt.show() (7) æ ¹æ®Importanceè¿›è¡Œç‰¹å¾ç­›é€‰12345678910111213141516171819202122232425262728293031323334353637383940414243# sorted(list(selection_model.booster().get_score(importance_type=&apos;weight&apos;).values()),reverse = True)importance_plot = pd.DataFrame(&#123;&apos;feature&apos;:list(X_train.columns),&apos;importance&apos;:md_2.feature_importances_&#125;)importance_plot = importance_plot.sort_values(by=&apos;importance&apos;)importance_plot = importance_plot.reset.index(drop=True)thresholds = importance_plot.importancethresholds_valid = np.unique(thresholds[thresholds != 0])for thresh in thresholds_valid: # select features using threshold selection = SelectFromModel(md_2, threshold=thresh, prefit=True) select_X_train = selection.transform(X_train) # train model selection_model = XGBClassifier(**best_param) selection_model.fit(select_X_train, y_train) # eval model select_X_test = selection.transform(X_test) y_pred = selection_model.predict(select_X_test) predictions = [round(value) for value in y_pred] accuracy = accuracy_score(y_test, predictions) print(&quot;Thresh=%.4f, n=%d, Accuracy: %.2f%%&quot; % (thresh, select_X_train.shape[1], accuracy*100.0))thresh = 0.034selected_features = list(importance_plot[importance_plot.importance &gt; thresh][&apos;feature&apos;])print(&apos;selected features are :\n %s&apos;%selected_features)select_X_train = X_train[selected_features] # ç­›é€‰Importanceç¬¦åˆé˜ˆå€¼çš„ç‰¹å¾é›†n_features = selected_X_train.shape[1]print(&apos;total: %d features are selected&apos; %n_features)selection_model = XGBClassifier(**best_param) selection_model.fit(select_X_train, y_train)select_X_test = X_test[selected_features]y_pred = selection_model.predict(select_X_test)predictions = [round(value) for value in y_pred]accuracy = accuracy_score(y_test, predictions)f1_score = f1_score(y_test, predictions)print(&quot;Accuracy: %.2f%%&quot; % (accuracy * 100.0))print(&quot;F1 Score: %.2f%%&quot; % (f1_score * 100.0)) è‡³äºŽæ˜¯å…ˆè°ƒå‚ï¼Œå†åšå˜é‡ç­›é€‰ï¼Œè¿˜æ˜¯å…ˆç­›é€‰åŽè°ƒå‚ï¼Œæˆ–æ˜¯åå¤è°ƒå‚åå¤ç­›é€‰ï¼Œçº¯å‡­ä¸ªäººå–œå·ã€‚ (8) ç»˜åˆ¶å†³ç­–æ ‘ å…ˆä¸‹è½½graphviz çš„ graphviz-2.38.zipï¼Œæˆ‘çš„æ˜¯windowsï¼Œå…¶ä»–ç³»ç»Ÿè¯·è‡ªç”±é€‰æ‹© é…ç½®çŽ¯å¢ƒå˜é‡123# graphvizæ–‡ä»¶å­˜åœ¨çš„è·¯å¾„é…ç½®os.environ[&quot;PATH&quot;] += os.pathsep + &apos;C:/Users/Yi/Anaconda3/envs/release/bin/&apos; # åœ¨å¼•å·&apos;&apos;è¿™é‡Œæ›¿æ¢ä½ çš„dot.exeè·¯å¾„xgb.to_graphviz(md_2, num_trees=0, rankdir=&apos;LR&apos;) # num_treesçš„å€¼æ˜¯ç¬¬å‡ æ£µæ ‘ï¼Œ0ä¸ºç¬¬ä¸€æ£µï¼Œrankdiræ˜¯æ ‘çš„æ–¹å‘ï¼Œdefaultæ˜¯ä»Žä¸Šåˆ°ä¸‹ (9) ä¿å­˜æ¨¡åž‹æ–‡ä»¶ï¼Œå¯¼å…¥æ¨¡åž‹æ–‡ä»¶12345# save modeljoblib.dump(selection_model,&apos;xgb.model&apos;)# load modelloaded_model = joblib.load(&apos;xgb.model&apos;) [5] XGBçš„ä¼˜ç‚¹æ•²æ¡Œå­ï¼é‡ç‚¹è€ƒç‚¹ï¼åŒæ„ä¹‰é—®é¢˜ï¼šXGB ä¸Ž GBDTçš„åŒºåˆ« æŸå¤±å‡½æ•°ï¼šGBDTæ˜¯ä¸€é˜¶ï¼ŒXGBæ˜¯äºŒé˜¶æ³°å‹’å±•å¼€ XGBçš„æŸå¤±å‡½æ•°å¯ä»¥è‡ªå®šä¹‰ï¼Œå…·ä½“å‚è€ƒ objective è¿™ä¸ªå‚æ•° XGBçš„ç›®æ ‡å‡½æ•°è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæœ‰æ­£åˆ™é¡¹ï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼ŒæŽ§åˆ¶æ¨¡åž‹å¤æ‚åº¦ é¢„å‰ªæžï¼šé¢„é˜²è¿‡æ‹Ÿåˆ GBDTï¼šåˆ†è£‚åˆ°è´ŸæŸå¤±ï¼Œåˆ†è£‚åœæ­¢ XGBï¼šä¸€ç›´åˆ†è£‚åˆ°æŒ‡å®šçš„æœ€å¤§æ·±åº¦ï¼ˆmax_depthï¼‰ï¼Œç„¶åŽå›žè¿‡å¤´å‰ªæžã€‚å¦‚æŸä¸ªç‚¹ä¹‹åŽä¸å†æ­£å€¼ï¼ŒåŽ»é™¤è¿™ä¸ªåˆ†è£‚ã€‚ä¼˜ç‚¹æ˜¯ï¼Œå½“ä¸€ä¸ªè´ŸæŸå¤±(-2)åŽå­˜åœ¨ä¸€ä¸ªæ­£æŸå¤±(+10)ï¼Œ(-2+10=8&gt;0)æ±‚å’Œä¸ºæ­£ï¼Œä¿ç•™è¿™ä¸ªåˆ†è£‚ã€‚ XGBæœ‰åˆ—æŠ½æ ·/column sampleï¼Œå€Ÿé‰´éšæœºæ£®æž—ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ ç¼ºå¤±å€¼å¤„ç†ï¼šXGBå†…ç½®ç¼ºå¤±å€¼å¤„ç†è§„åˆ™ï¼Œç”¨æˆ·æä¾›ä¸€ä¸ªå’Œå…¶å®ƒæ ·æœ¬ä¸åŒçš„å€¼ï¼Œä½œä¸ºä¸€ä¸ªå‚æ•°ä¼ è¿›åŽ»ï¼Œä½œä¸ºç¼ºå¤±å€¼å–å€¼ã€‚XGBåœ¨ä¸åŒèŠ‚ç‚¹é‡åˆ°ç¼ºå¤±å€¼é‡‡å–ä¸åŒå¤„ç†æ–¹æ³•ï¼Œå¹¶ä¸”å­¦ä¹ æœªæ¥é‡åˆ°ç¼ºå¤±å€¼çš„æƒ…å†µã€‚ XGBå†…ç½®äº¤å‰æ£€éªŒï¼ˆCVï¼‰ï¼Œå…è®¸æ¯è½®boostingè¿­ä»£ä¸­ç”¨äº¤å‰æ£€éªŒï¼Œä»¥ä¾¿èŽ·å–æœ€ä¼˜ Boosting_n_round è¿­ä»£æ¬¡æ•°ï¼Œå¯åˆ©ç”¨ç½‘æ ¼æœç´¢grid searchå’Œäº¤å‰æ£€éªŒcross validationè¿›è¡Œè°ƒå‚ã€‚GBDTä½¿ç”¨ç½‘æ ¼æœç´¢ã€‚ XGBè¿è¡Œé€Ÿåº¦å¿«ï¼šdataäº‹å…ˆå®‰æŽ’å¥½ä»¥blockå½¢å¼å­˜å‚¨ï¼Œåˆ©äºŽå¹¶è¡Œè®¡ç®—ã€‚åœ¨è®­ç»ƒå‰ï¼Œå¯¹æ•°æ®æŽ’åºï¼ŒåŽé¢è¿­ä»£ä¸­åå¤ä½¿ç”¨blockç»“æž„ã€‚å…³äºŽå¹¶è¡Œï¼Œä¸æ˜¯åœ¨treeç²’åº¦ä¸Šçš„å¹¶è¡Œï¼Œå¹¶è¡Œåœ¨ç‰¹å¾ç²’åº¦ä¸Šï¼Œå¯¹ç‰¹å¾è¿›è¡ŒImportanceè®¡ç®—æŽ’åºï¼Œä¹Ÿæ˜¯ä¿¡æ¯å¢žç›Šè®¡ç®—ï¼Œæ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹ã€‚ çµæ´»æ€§ï¼šXGBå¯ä»¥æ·±åº¦å®šåˆ¶æ¯ä¸€ä¸ªå­åˆ†ç±»å™¨ æ˜“ç”¨æ€§ï¼šXGBæœ‰å„ç§è¯­è¨€å°è£… æ‰©å±•æ€§ï¼šXGBæä¾›äº†åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒHadoopå®žçŽ° å…±åŒä¼˜ç‚¹ï¼š å½“æ•°æ®æœ‰å™ªéŸ³çš„æ—¶å€™ï¼Œæ ‘Treeçš„ç®—æ³•æŠ—å™ªèƒ½åŠ›æ›´å¼º æ ‘å®¹æ˜“å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¤„ç† æ ‘å¯¹åˆ†ç±»å˜é‡Categorical featureæ›´å‹å¥½ XGBå®žåœ¨å¤ªå¼ºå¤§ å®žæ—¶åœ¨æ›´æ–°ï¼Œç›®å‰çš„æ€»ç»“åªæ˜¯åˆ©ç”¨ç›®å‰çš„èµ„æº æœªæ¥ä¼šå‘å±•æˆä»€ä¹ˆæ ·ï¼Œè°ä¹ŸçŒœä¸åˆ°ã€‚ å‚è€ƒï¼š å®˜æ–¹ä½¿ç”¨æ‰‹å†Œ Awesome XGBoost R+pythonï¸±XGBoost]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>python</tag>
        <tag>xgboost</tag>
        <tag>algorithm</tag>
        <tag>boosting</tag>
        <tag>gradient</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learningç¬”è®° - åŸºäºŽRçš„æ•°æ®æ¸…æ´—ï¼ˆ1ï¼‰]]></title>
    <url>%2F2018%2F05%2F25%2F%E5%9F%BA%E4%BA%8ER%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[å½“ç®—æ³•é€æ¸æ¡†æž¶åŒ–ï¼Œå˜æˆè°ƒå‚çš„æŠŠæˆï¼Œæ•°æ®æ¸…æ´— å°±æˆä¸ºäº†æ‰€è°“çš„æ•°æ®æŒ–æŽ˜çš„ç²¾é«“ï¼Œæ˜¯ä½ ä¸Žåˆ«äººçš„æ¨¡åž‹æ‹‰å¼€å·®è·çš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯ä½ å»ºæ¨¡åŠŸåŠ›çš„æœ€ä½³å±•ç¤ºã€‚å¦‚ä½•æ´—æ•°æ®ï¼Ÿå½“ç„¶ä¸æ˜¯ç«‹ç™½æ´—è¡£æ¶²ï¼Œé›•ç‰Œæ´—è¡£çš‚ä¹‹ç±»çš„ã€‚ æ•°æ®æ¸…æ´—æ˜¯ä¸€ä¸ªæ¼«é•¿ã€è€—æ—¶ã€ç»éªŒç§¯ç´¯çš„è¿‡ç¨‹ï¼Œæœ¬æ–‡åŒ…æ‹¬ï¼š å¯¼å…¥æ•°æ® ç†è§£ç‰¹å¾ã€æ•°æ®ç±»åž‹ ç¼ºå¤±å€¼å¤„ç† æ•°æ®ç±»åž‹çš„ç»Ÿä¸€æ€§å¤„ç† åˆ†ç±»å˜é‡çš„å¤„ç†(unary | binary | nomial | categorical | ordinal) è¿žç»­å˜é‡çš„å¤„ç†(interval) æŽ’åºä¸åˆ†å…ˆåŽï¼Œæœ‰äº›æ­¥éª¤ä¼šé‡å¤å‡ºçŽ°æ¯”å¦‚è§‚å¯Ÿç¼ºå¤±å€¼ï¼Œç¼ºå¤±å€¼å¡«å……ç­‰ 1. å¯¼å…¥æ•°æ®train.csv ä¸‹è½½æœ¬æ–‡çš„æ ·æœ¬æ•°æ®ä¸ºkaggleå…¥é—¨èµ›Titanicçš„æ•°æ®ï¼Œåœ°çƒäººéƒ½æ‡‚çš„â€¦â€¦å§ï¼Ÿ 12345678910111213# å¯¼å…¥åŒ…packages&lt;-c(&quot;ggplot2&quot;,&quot;dplyr&quot;,&quot;varhandle&quot;)UsePackages&lt;-function(p)&#123; if (!is.element(p,installed.packages()[,1]))&#123; install.packages(p)&#125; require(p,character.only = TRUE)&#125;for(p in packages)&#123; UsePackages(p)&#125;library(ggplot2)library(dplyr)library(varhandle) ä»¥ä¸‹çš„æ•°æ®æ¸…æ´—æ–¹æ³•ã€æ€è·¯åŒ…å«å´ä¸ä»…é™äºŽè¿™ä¸ªæ•°æ®æ ·æœ¬ï¼ŒæŒ‰ç…§æ•´ä½“åˆ†æžæ¡†æž¶ä¼šæ‹“å±•åˆ°å…¶ä»–å¸¸ç”¨çš„ç‰¹å¾1234# æˆ‘å°†ä¸‹è½½çš„æ•°æ®æ”¾åœ¨&quot;D:/1_kaggle/titanic/data&quot;çš„è·¯å¾„ä¸‹setwd(&quot;D:/1_kaggle/titanic&quot;) # è®¾ç½®è·¯å¾„train = read.csv(&apos;data/train.csv&apos;) # å¯¼å…¥csvæ ¼å¼çš„æ•°æ® 2. ç†è§£ç‰¹å¾ã€æ•°æ®ç±»åž‹123456789101112131415161718192021222324ncol(train) # å­—æ®µæ•°é‡ åˆ—æ•°# 12nrow(train) # æ ·æœ¬æ•°é‡ è¡Œæ•°# 891colnames(train) # å­—æ®µå åˆ—å# &quot;PassengerId&quot; &quot;Survived&quot; &quot;Pclass&quot; &quot;Name&quot;# &quot;Sex&quot; &quot;Age&quot; &quot;SibSp&quot; &quot;Parch&quot; # &quot;Ticket&quot; &quot;Fare&quot; &quot;Cabin&quot; &quot;Embarked&quot; cbind(apply(train,2,function(x)length(unique(x))),sapply(train,class)) # èŽ·å–æ¯åˆ—çš„æ•°æ®ç§ç±»æ•°ï¼Œå’Œ æ•°æ®ç±»åž‹# PassengerId &quot;891&quot; &quot;integer&quot;# Survived &quot;2&quot; &quot;integer&quot;# Pclass &quot;3&quot; &quot;integer&quot;# Name &quot;891&quot; &quot;factor&quot; # Sex &quot;2&quot; &quot;factor&quot; # Age &quot;89&quot; &quot;numeric&quot;# SibSp &quot;7&quot; &quot;integer&quot;# Parch &quot;7&quot; &quot;integer&quot;# Ticket &quot;681&quot; &quot;factor&quot; # Fare &quot;248&quot; &quot;numeric&quot;# Cabin &quot;148&quot; &quot;factor&quot; # Embarked &quot;4&quot; &quot;factor&quot; è®­ç»ƒé›†ä¸­ä¹˜å®¢çš„ç‰¹å¾æœ‰ï¼šPassengerIdã€Pclassã€Nameã€Sexã€Ageã€SibSpã€Parchã€Ticketã€Fareã€Cabinã€Embarked 12ä¸ªå­—æ®µ/ç‰¹å¾ï¼Œ12åˆ—ï¼Œ891è¡Œ integer | numeric ä¸ºè¿žç»­å˜é‡/æ•°å€¼åž‹ç‰¹å¾ factor ä¸ºåˆ†ç±»å˜é‡/ç¦»æ•£åž‹çš„ç‰¹å¾ï¼Œä¸€èˆ¬éœ€è¦é¢å¤–å¤„ç†æ‰èƒ½è®­ç»ƒ 3. ç¼ºå¤±å€¼å¤„ç†ï¼ˆ1ï¼‰è§‚å¯Ÿç¼ºå¤±å€¼æ¯”ä¾‹123456789101112131415161718192021# å»ºç«‹è§‚å¯Ÿç¼ºå¤±å€¼æ¯”ä¾‹çš„å‡½æ•° na.plotna.plot=function(data)&#123; missing1=sapply(data,function(x)sum(x == &apos;&apos;)/nrow(data)) missing2=sapply(data,function(x)sum(sum(is.null(x)), sum(is.na(x)))/nrow(data)) if(sum(is.na(missing1))&gt;0)&#123; missing1[is.na(missing1)] = 0 &#125; missing = missing1 + missing2 print(missing) missing=missing[order(missing,decreasing = T)] nadata=missing[missing&gt;0] na_df=data.frame(var=names(nadata),na=nadata,row.names = NULL) ggplot(na_df)+ geom_bar(aes(x=reorder(var,na),y=na),stat=&apos;identity&apos;, fill=&apos;red&apos;)+ labs(y=&apos;% Missing&apos;,x=NULL,title=&apos;Percent of Missing Data by Feature&apos;) + coord_flip(ylim = c(0,1)) &#125;# è°ƒç”¨å‡½æ•°na.plot(train) å­—æ®µ Cabin/ Age/ Embarked æœ‰ç¼ºå¤±å€¼ ç¼ºå¤±å€¼ç¼ºå°‘è¶Šå¥½ï¼Œæ²¡æœ‰ç¼ºå¤±å€¼æœ€å¥½ ç¼ºå¤±å€¼å­˜åœ¨äºŽDataFrameçš„å½¢å¼ï¼š NA NULL â€˜â€™ 123456789101112# ä»¥Cabinä¸ºä¾‹train$Cabin[is.na(train$Cabin)] # ç­›é€‰å‡ºCabinä¸ºNAçš„ç¼ºå¤±å€¼Cabin# train$Cabin[!is.na(train$Cabin)] # åé€»è¾‘train$Cabin[is.null(train$Cabin)] # ç­›é€‰å‡ºCabinä¸ºNULLçš„ç¼ºå¤±å€¼Cabin # train$Cabin[!is.null(train$Cabin)]# åé€»è¾‘train$Cabin[train$Cabin == &apos;&apos;] # ç­›é€‰å‡ºCabinä¸º&apos;&apos;çš„ç¼ºå¤±å€¼Cabin# train$Cabin[train$Cabin != &apos;&apos;] # åé€»è¾‘filter(train,!is.na(Cabin)&amp;!is.null(Cabin)&amp;(Cabin != &apos;&apos;))$Cabin # ç­›é€‰å‡ºéžç¼ºå¤±å€¼çš„Cabinå€¼ è¿™æ ·åšæ•°æ®æ¸…æ´—ä¸å…æœ‰äº›ç¹çï¼Œå¦‚æžœé€»è¾‘åˆç†ï¼Œå¯ä»¥æŠŠNULLå€¼å’Œâ€™â€™å€¼éƒ½å¤„ç†æˆNAå€¼123456# ä¸‰ç§ç¼ºå¤±å€¼çš„åŒºåˆ«åœ¨äºŽlengthé•¿åº¦length(NA) == 1length(NULL) == 0length(&apos;&apos;) == 1 ï¼ˆ2ï¼‰å½“é«˜ç¼ºå¤±å€¼å æ¯”å‡ºçŽ°æ—¶ï¼ˆä¾‹ï¼šmissing proportion &gt; 95%ï¼‰ï¼Œä¸€èˆ¬è€ƒè™‘åˆ é™¤è¯¥ç‰¹å¾1234567891011121314151617181920# å»ºç«‹åˆ é™¤ç¼ºå¤±å€¼å æ¯”é«˜äºŽæŸæ¯”ä¾‹çš„ç‰¹å¾çš„å‡½æ•°# ä¸‹é¢çš„å‡½æ•°è®¾ç½®çš„é˜ˆå€¼æ˜¯ 0.75na.drop=function(data)&#123; missing1=sapply(data,function(x)sum(x == &apos;&apos;)/nrow(data)) missing2=sapply(data,function(x)sum(sum(is.null(x)), sum(is.na(x)))/nrow(data)) if(sum(is.na(missing1))&gt;0)&#123; missing1[is.na(missing1)] = 0 &#125; missing = missing1 + missing2 missing=cbind(colnames(data),as.numeric(missing)[!is.na(as.numeric(missing))])%&gt;%as.matrix() print(missing) valid=missing%&gt;%as.data.frame%&gt;%filter(missing[,2]&lt;0.75) # ç¼ºå¤±å€¼å æ¯”é˜ˆå€¼è®¾ç½® valid$V1&lt;-unfactor(valid$V1) data=data%&gt;%select(valid$V1)%&gt;%as.data.frame()&#125;train=na.drop(train) # å®žæ–½åˆ é™¤é«˜ç¼ºå¤±å€¼ç‰¹å¾na.plot(train) # è§‚å¯Ÿæ˜¯å¦è¢«åˆ é™¤ Cabinç¼ºå¤±77%ï¼Œå°†è¢«åˆ é™¤ ï¼ˆ3ï¼‰é‡è¦ç‰¹å¾ &amp; é«˜ç¼ºå¤±å€¼å æ¯”ï¼šå°†è¯¥ç‰¹å¾è½¬æ¢æˆbinaryç‰¹å¾ï¼Œæœ‰æ•°å€¼ä¸º1ï¼Œç¼ºå¤±å€¼ä¸º01234567891011121314151617181920212223train = read.csv(&apos;data/train.csv&apos;) #æ¢å¤åŽŸdf# å»ºç«‹å‡½æ•°ï¼šç­›é€‰å‡ºé«˜ç¼ºå¤±å€¼å æ¯”çš„ç‰¹å¾na.toBinary=function(data)&#123; missing1=sapply(data,function(x)sum(x == &apos;&apos;)/nrow(data)) missing2=sapply(data,function(x)sum(sum(is.null(x)), sum(is.na(x)))/nrow(data)) if(sum(is.na(missing1))&gt;0)&#123; missing1[is.na(missing1)] = 0 &#125; missing = missing1 + missing2 missing=cbind(colnames(data),as.numeric(missing)[!is.na(as.numeric(missing))])%&gt;%as.matrix() print(missing) toBinary=missing%&gt;%as.data.frame()%&gt;%filter(missing[,2]&gt;0.75 &amp; missing[,2]&lt;1) # ç¼ºå¤±å€¼å æ¯”é˜ˆå€¼è®¾ç½®,è¿™é‡Œæ˜¯é€‰å–ç¼ºå¤±å€¼æ¯”ä¾‹å¤§äºŽ0.75ï¼Œå°äºŽ1çš„å­—æ®µï¼Œä»…ä»…ä½œä¸ºç­›é€‰å‡ºCabinå­—æ®µ toBinary$V1&lt;-unfactor(toBinary$V1) print(toBinary$V1)&#125;na.toBinary(train)# &quot;Cabin&quot;# å‡è®¾Cabinä¸ºé‡è¦ä¿¡æ¯ï¼Œå¯¹ç¼ºå¤±å€¼è¿›è¡ŒäºŒå€¼åŒ–train$Cabin = as.numeric(!is.na(train$Cabin)&amp;!is.null(train$Cabin)&amp;(train$Cabin != &apos;&apos;)) è¿™é‡Œåªæ˜¯æ‹¿Cabinåšä¸ªä¾‹å­ï¼Œå®žé™…ä¸Šå¹¶ä¸ä¼šæŠŠè¿™ä¸ªä¿¡æ¯äºŒå€¼åŒ– å½“å‡ºçŽ°é‡è¦ç‰¹å¾ &amp; é«˜ç¼ºå¤±å€¼å æ¯”æ—¶ï¼Œæ‰è¿›è¡Œè¿™ä¸ªå¤„ç† ï¼ˆ4ï¼‰ç¼ºå¤±å€¼å¡«å…… ç¼ºå¤±å€¼å¡«å……æœ‰å¾ˆå¤šæ–¹æ³•ï¼š æ•°å€¼åž‹ç‰¹å¾ï¼šå¯ä»¥ç”¨ å‡å€¼/æœ€å¤§å€¼/æœ€å°å€¼/ä¼—æ•° å¡«å…… æ—¶é—´åºåˆ—ç‰¹å¾ï¼šä¾‹å¦‚è‹¹æžœä»Šå¤©çš„ä»·æ ¼ç¼ºå¤±ï¼Œå¯ä»¥ç”¨æ˜¨æ—¥çš„ä»·æ ¼å¡«å…… å°†ç¼ºå¤±å€¼å½’ä¸ºä¸€ç±»ï¼šå¯ä»¥ç”¨ä¸æ›¾å‡ºçŽ°ä¹Ÿä¸ä¼šå‡ºçŽ°çš„å€¼ï¼Œæ¯”å¦‚ï¼šç¼ºå¤±å¹´é¾„ç”¨ 999 å¡«å……ï¼Œç¼ºå¤±ä½“é‡ç”¨ -1 å¡«å…… ç¼ºå¤±æ¯”ä¾‹å°çš„è¿žç»­å˜é‡ï¼šå¯ç”¨æœ‰æ•°å€¼çš„æ•°æ®è¿›è¡Œå¯¹ç¼ºå¤±å€¼çš„å›žå½’é¢„æµ‹å¡«å……ï¼Œä¾‹ï¼šç­ä¸ŠæŸåŒå­¦çš„èº«é«˜ é‡è¦ç‰¹å¾ &amp; é«˜ç¼ºå¤±å€¼å æ¯”ï¼šå°†ç¼ºå¤±å€¼äºŒå€¼åŒ–ï¼Œåœ¨ä¸Šé¢å·²ç»ä¸¾ä¾‹è¯´æ˜Ž å…¶ä»–ï¼šå¯ä»¥ç”¨ä»»ä½•åˆç†çš„é€»è¾‘è¿›è¡Œå¡«å…… ï¼ˆæ•°å€¼åž‹ç‰¹å¾ï¼‰å‡å€¼/æœ€å¤§å€¼/æœ€å°å€¼/ä¼—æ•°çš„èŽ·å–123456789101112131415161718192021222324# ä»¥ Age ä¸ºä¾‹tmp_Age = filter(train,!is.na(Age)&amp;!is.null(Age)&amp;(Age != &apos;&apos;))$Age # ç­›é€‰å‡ºéžç©ºå¹´é¾„å€¼mean(tmp_Age)# 29.69912max(tmp_Age)# 80min(tmp_Age)# 0.42# ä¼—æ•°æ²¡æœ‰ç›´æŽ¥å¯ä»¥è°ƒç”¨çš„functionï¼Œéœ€è¦è‡ªå·±å†™# create getmode functiongetmode&lt;-function(v)&#123; uniqv&lt;-unique(v) uniqv[which.max(tabulate(match(v,uniqv)))]&#125;getmode(tmp_Age)# 24# å°†Ageçš„ç¼ºå¤±å€¼ä»¥ä¼—æ•°å¡«å……train$Age[is.na(train$Age)] = getmode(tmp_Age) 4. æ•°æ®ç±»åž‹çš„ç»Ÿä¸€æ€§å¤„ç†æ•°æ®ç±»åž‹æœ‰ä¸¤ç§å±žæ€§ï¼š åœ¨æ•°æ®é‡Œå±•ç¤ºçš„ç±»åž‹ åœ¨å®žé™…æ„ä¹‰ä¸­çš„ç±»åž‹ å½“ä¸¤ç§ç±»åž‹ä¸ç»Ÿä¸€æ—¶ï¼Œéœ€è¦å°†å…¶ç»Ÿä¸€ï¼Œæˆ–è€…åšç›¸åº”æ ‡è¯† ä¾‹ï¼š Pclassï¼Œèˆ±ä½ç­‰çº§è¿™ä¸ªç‰¹å¾åœ¨æ•°æ®é›†ä¸­ä»¥ 1/2/3 çš„æ•´æ•°å±•ç¤ºï¼Œæ˜¾ç¤ºç±»åž‹ä¸ºâ€integerâ€ï¼Œä¸ºè¿žç»­å˜é‡ï¼Œä½†æŒ‰å­—é¢è§£é‡Šè¿™ä¸ªç‰¹å¾å®žé™…ä¸Šå±žäºŽcategoricalåˆ†ç±»å˜é‡ï¼Œç­‰çº§1/ç­‰çº§2/ç­‰çº§3ï¼Œæ„ä¹‰ä¸Šç±»åž‹åº”è¯¥æ˜¯â€factorâ€ï¼Œä¸¤ç§å±žæ€§ä¸ç»Ÿä¸€ã€‚ SibSpç‰¹å¾æè¿°ä¹˜å®¢çš„å…„å¼Ÿå§å¦¹æˆ–è€…é…å¶æ•°é‡ï¼Œæ˜¾ç¤ºç±»åž‹ä¸ºâ€integerâ€ï¼Œå­—é¢ç±»åž‹ä¹Ÿæ˜¯æ•°å€¼åž‹ï¼Œè¿žç»­å˜é‡ï¼Œä¸¤ç§ç±»åž‹ç»Ÿä¸€ã€‚ æ•°æ®ç±»åž‹åº”è¯¥ç»Ÿä¸€å¦‚ä¸‹ï¼š è¿žç»­å˜é‡ï¼šAgeã€SibSpã€Parchã€Fareï¼ˆå·²ç»å…¨éƒ¨ä¸ºæ•°å€¼åž‹å˜é‡ï¼‰ åˆ†ç±»å˜é‡ï¼šPassengerIdã€Pclassã€Nameã€Sexã€Ticketã€Cabinã€Embarkedï¼ˆæœ‰æ•°å€¼åž‹æœ‰ç¦»æ•£åž‹ï¼‰ 5. åˆ†ç±»å˜é‡çš„å¤„ç†åˆ†ç±»å˜é‡ä¼šå‡ºçŽ°å¤šç§æƒ…å†µï¼šï¼ˆ1ï¼‰æƒ…å†µï¼šåˆ†ç±»ä¿¡æ¯éœ€è¦æå–åœ¨Nameä¸­ï¼Œæœ‰Mr/Mrs/Missç­‰ç§°å‘¼ä¿¡æ¯å¯ä»¥åˆ¤åˆ«ä¹˜å®¢çš„æ€§åˆ«ï¼Œè™½ç„¶åœ¨è¿™é‡ŒSexæ€§åˆ«çš„ä¿¡æ¯æœªç¼ºå¤±ï¼Œä½†æ˜¯åœ¨ç»“æžœå‡ºæ¥å‰æ‰€æœ‰çš„ä¿¡æ¯éƒ½ä¸èƒ½è½»æ˜“ä¸¢å¼ƒè€Œä¸”ï¼ŒæŒ‰ç…§Miss/Mrså¯ä»¥å¤§æ¦‚é¢„æµ‹ç¼ºå¤±çš„å¹´é¾„ï¼Œå¯¹äºŽåå­—ä¸­æœ‰Mrsçš„ä¹˜å®¢çš„ç¼ºå¤±å¹´é¾„å¡«å……å°±ä¸ä¼šå‡ºçŽ°10å²ä¹‹ç±»çš„123456789101112131415161718192021train = read.csv(&apos;data/train.csv&apos;)which(grepl(&apos;Mrs.&apos;, train$Name)) # ç­›é€‰å‡ºMrsä¹˜å®¢çš„Indextrain$Name[which(grepl(&apos;Mrs.&apos;, train$Name))] # ç­›é€‰å‡ºMrsä¹˜å®¢çš„åå­—ï¼Œè¿™éƒ¨åˆ†äººçš„æ€§åˆ«å¯ä»¥æ ‡è¯†ä¸º å¥³æ€§ï¼Œåœ¨è¿™é‡Œæš‚ä¸åšæ”¹åŠ¨which(grepl(&apos;Mrs.&apos;, train$Name[is.na(train$Age)])) # ç­›é€‰å‡ºå¹´é¾„ä¸ºç¼ºå¤±å€¼çš„Mrsä¹˜å®¢çš„Index# ç­›é€‰å‡ºå¹´é¾„ä¸ºç¼ºå¤±å€¼çš„Mrsä¹˜å®¢çš„å¹´é¾„ï¼Œä¸‹ä¸€æ­¥è¿›è¡Œèµ‹å€¼train$Age[is.na(train$Age)][which(grepl(&apos;Mrs.&apos;, train$Name[is.na(train$Age)]))] # ç”¨æ— ç¼ºå¤±å€¼çš„Mrsç¾¤ä½“çš„å¹´é¾„çš„å‡å€¼ æ¥å¡«å…… Mrsç¾¤ä½“çš„ç¼ºå¤±å¹´é¾„# è¿™æ ·æ¯”ç›´æŽ¥ç”¨å…¨ä½“æ ·æœ¬çš„å¹´é¾„å‡å€¼æ¥å¡«å……ç¼ºå¤±çš„æ ·æœ¬å¹´é¾„è¦åˆç†ä¸€äº›train$Age[is.na(train$Age)][which(grepl(&apos;Mrs.&apos;, train$Name[is.na(train$Age)]))] = mean(train$Age[!is.na(train$Age)][which(grepl(&apos;Mrs.&apos;, train$Name[!is.na(train$Age)]))])# åŒç†å¯ä»¥å¯¹ Missç¾¤ä½“å’Œ Mrç¾¤ä½“è¿›è¡Œå¹´é¾„å¡«å……train$Age[is.na(train$Age)][which(grepl(&apos;Miss.&apos;, train$Name[is.na(train$Age)]))] = mean(train$Age[!is.na(train$Age)][which(grepl(&apos;Miss.&apos;, train$Name[!is.na(train$Age)]))])train$Age[is.na(train$Age)][which(grepl(&apos;Mr.&apos;, train$Name[is.na(train$Age)]))] = mean(train$Age[!is.na(train$Age)])train$Age[is.na(train$Age)] = mean(train$Age[!is.na(train$Age)])# å°†å¹´é¾„æ•´æ•°åŒ–train$Age = as.integer(train$Age) ï¼ˆ2ï¼‰æƒ…å†µï¼šåˆ†ç±»çš„ç±»æ•°levelå¤ªå¤š IDç±»åž‹ï¼š PassengerIdï¼šä¸€ä¸ªæ ·æœ¬ä¸ºä¸€ç±»ï¼Œæ¯ä¸ªæ ·æœ¬çš„IDä¸ä¸€æ ·ï¼Œæ— æ•ˆä¿¡æ¯ç›´æŽ¥åˆ é™¤ èº«ä»½è¯IDï¼šä¸Žå•çº¯çš„Indexå­—æ®µä¸ä¸€æ ·ï¼Œä¸èƒ½ç›´æŽ¥åˆ é™¤ï¼Œèƒ½ä»Žé‡Œé¢æå–å‡ºç”Ÿå¹´æœˆæ—¥ï¼Œå¹´é¾„ï¼Œæ€§åˆ«ï¼Œæˆ·ç±çœä»½ç­‰ï¼Œä¿¡æ¯æå–å®Œæ¯•åŽå¯ä»¥åˆ é™¤ userIDï¼šæœ‰äº›ç”¨æˆ·IDé‡Œä¹ŸåŒ…å«ç”¨æˆ·çš„æ³¨å†Œä¿¡æ¯ï¼Œæ¯”å¦‚æ³¨å†Œå¹´æœˆæ—¥ç­‰ï¼Œéœ€è¦å¯¹æ•°æ®æ•æ„Ÿ å¤šç±»åˆ«å°‘æ ·æœ¬ ç±»åž‹ï¼š åˆå¹¶ç±»åˆ«ï¼šåŒåä¸€è´­ç‰©è€…çš„çœä»½ä¿¡æ¯ï¼Œåœ¨æ±Ÿæµ™æ²ªæœ‰çˆ†ç‚¸æ€§çš„æ”¶ä»¶æ•°é‡ï¼Œè€Œåœ¨è¥¿è—ã€æ–°ç–†è¿‘10ä¸ªåŒºåŸŸç­‰åœ°çš„æ”¶ä»¶æ•°é‡å¾ˆå°‘ï¼Œå¯ä»¥æŠŠè¿™10ä¸ªåœ°åŒºåˆå¹¶ä¸ºä¸€ç±»ï¼šåè¿œåœ°åŒº ï¼ˆ3ï¼‰æƒ…å†µï¼šåˆ†ç±»çš„ç±»æ•°levelå¤ªå°‘ å•ç±»åˆ«ï¼šæ‰€æœ‰æ ·æœ¬éƒ½å±žäºŽä¸€ä¸ªç±»åˆ«ï¼Œæ— æ•ˆç±»åˆ«ä¿¡æ¯ï¼Œåˆ é™¤ç‰¹å¾ ï¼ˆ4ï¼‰æƒ…å†µï¼šè¿žç»­å˜é‡ç¦»æ•£åŒ–/åˆ†ç®± æ¯”å¦‚Ageï¼šå¯ä»¥æŠŠè¿žç»­çš„å¹´é¾„æ•°å€¼åˆ’åˆ†ä¸ºï¼šå°‘å¹´/é’å¹´/ä¸­å¹´/è€å¹´ï¼Œåœ¨å†³ç­–æ ‘ä¸­è¿™ç§æ“ä½œä¼šæ¯”è¾ƒå¸¸è§ å¤„ç†æ–¹æ³•ï¼šï¼ˆ1ï¼‰onehot ç‹¬çƒ­ç¼–ç å¤„ç†one-hot encodingå°±æ˜¯ç”¨hä¸ªå˜é‡æ¥ä»£è¡¨è¿™hä¸ªlevelï¼Œæ¯”å¦‚3ä¸ªlevelçš„å˜é‡å°±è¡¨ç¤ºæˆ100ï¼Œ010ï¼Œ001 è¿™é‡Œä»¥Cabinã€Embarkedä¸ºä¾‹ï¼Œè¿›è¡Œonehotå¤„ç†123456789101112131415161718192021222324252627282930313233train = read.csv(&apos;data/train.csv&apos;)# Cabinæœ¬èº«æ˜¯ç”±å­—æ¯ä¸Žæ•°å­—ç»„æˆçš„# å­—æ¯æ›´èƒ½ä»£è¡¨èˆ±ä½çš„ä¸€å®šä¿¡æ¯ï¼Œå› æ­¤å¯¹Cabinåšå¦‚ä¸‹å¤„ç†# å¯¹äºŽæœ‰å€¼çš„Cabinï¼šä»…ä¿ç•™å­—æ¯ä¿¡æ¯# å¯¹äºŽç¼ºå¤±å€¼çš„Cabinï¼šç”¨ NA ä½œä¸ºä¸€ç±»å¡«å……train$Cabin = ifelse(train$Cabin != &quot;&quot;,substr(gsub(&apos;[0-9]&apos;, &apos;&apos;, train$Cabin), 1, 1), &apos;NA&apos;)unique(train$Cabin) # å¤„ç†åŽCabinçš„æ•°å€¼ç§ç±»# NA C E G D A B F T # ä¿ç•™Embarkçš„å­—æ¯ä¿¡æ¯ï¼Œç¼ºå¤±å€¼ç”¨NAå¡«å……train$Embarked = ifelse(train$Embarked != &quot;&quot;, substr(train$Embarked, 1, 1), &apos;NA&apos;)unique(train$Embarked) # å¤„ç†åŽEmbarkçš„æ•°å€¼ç§ç±»# S C Q NA# åªæœ‰classç±»åž‹ä¸ºfactorçš„ç‰¹å¾æ‰èƒ½åšmodel.matrixå¤„ç†ï¼Œéœ€è¦æå‰æŠŠcharacterç±»åž‹è½¬æ¢ä¸ºfactorç±»åž‹features = c(&apos;Cabin&apos;,&apos;Embarked&apos;) #éœ€è¦åšè½¬æ¢çš„ç‰¹å¾åç§°for (f in features)&#123; if( (class(train[[f]]) == &quot;character&quot;) || (class(train[[f]]) == &quot;factor&quot;)) &#123; levels = unique(train[[f]]) train[[f]] = factor(train[[f]], level = levels) &#125;&#125;# onehotå¤„ç†trainMatrix &lt;- model.matrix(~ Cabin + Embarked, data=train, contrasts.arg = lapply(train[,c(&apos;Cabin&apos;,&apos;Embarked&apos;)], contrasts, contrasts=FALSE))trainMatrix &lt;- trainMatrix[,-1] Cabinå˜æˆlength(unique(train$Cabin)) = 9 ä¸ªonehotç‰¹å¾ Embarkå˜æˆlength(unique(train$Embarked)) = 4 ä¸ªonehotç‰¹å¾ ï¼ˆ2ï¼‰dummy è™šæ‹Ÿå˜é‡ç¼–ç å¤„ç†dummy encodingå°±æ˜¯æŠŠä¸€ä¸ªæœ‰hä¸ªlevelçš„å˜é‡å˜æˆh-1ä¸ªå˜é‡ï¼Œæ¯”å¦‚3ä¸ªlevelçš„å˜é‡å°±è¡¨ç¤ºæˆæˆ10ï¼Œ01ï¼Œæˆ–00 6. è¿žç»­å˜é‡çš„å¤„ç†å½“åˆ†ç±»å˜é‡éƒ½å˜æˆ 0/1 çš„æ•°å­—ï¼Œè€Œè¿žç»­å˜é‡ï¼ˆæ¯”å¦‚å¹´é¾„ï¼Œæ˜¯0~100çš„åˆ†å¸ƒï¼‰çš„å€¼åŸŸè¿œè¿œå¤§äºŽ0~1ï¼Œéš¾å…æ˜¾å¾—ä¸å…¬å¹³ã€‚è¿™ä¸ªè¯´æ³•æ˜¯æœ‰ç§‘å­¦ä¾æ®çš„ï¼Œæœ‰å…´è¶£å¯ä»¥æ‰‹åŠ¨æŽ¨å¯¼ä¸€ä¸‹ã€‚å› æ­¤ä¸€èˆ¬ä¼šå¯¹è¿žç»­å˜é‡è¿›è¡Œ å½’ä¸€åŒ–(Regularization) | æ ‡å‡†åŒ–(Standardization) | åŽ»ä¸­å¿ƒåŒ– | â€¦(åŸºæœ¬ä¸Šæ˜¯åŒä¸€ç±»æ€æƒ³ï¼Œæš‚ä¸”éƒ½ç§°ä¸ºæ ‡å‡†åŒ–) æ ‡å‡†åŒ–æœ‰ä»¥ä¸‹å‡ ç§å¤„ç†æ–¹å¼ï¼šemmmâ€¦.æš‚æ—¶æ‡’å¾—å†æ‰“ä¸€éï¼Œç›´æŽ¥ä¸Šå›¾ï¼Œå¸Œæœ›èƒ½çœ‹çš„æ¸…æ¥š è¿™é‡Œä»¥Fareä¸ºä¾‹ï¼Œè¿›è¡Œç¬¬äºŒç§ï¼šæœ€å¤§-æœ€å°åŒ– | max-min å¤„ç†1train$Fare = (train$Fare-min(train$Fare))/(max(train$Fare)-min(train$Fare)) è¿™æ ·ç¥¨ä»·Fareçš„å€¼åŸŸå°±å¤„äºŽ0~1ä¹‹é—´äº† æ¸…æ´—è¿™ä¸ªdataframeä¸æ˜¯ç›®çš„æœ¬æ–‡çš„ç›®çš„æ˜¯ï¼Œå½“é‡åˆ°ç±»ä¼¼çš„è„æ•°æ®æ—¶ï¼ŒçŸ¥é“æ€Žä¹ˆåŽ»æ¸…æ´— æœ¬æ–‡åªæ˜¯å±•ç¤ºäº†æ•°æ®æ¸…æ´—çš„ä¸€ä¸ªå¤§æ¦‚æ¡†æž¶ï¼Œå…·ä½“çš„æ¸…æ´—æ–¹æ³•ä¼šæ ¹æ®ä¸åŒçš„è„æ•°æ®åšä¸åŒçš„å¤„ç†æ¯”å¦‚ï¼Œå¦‚ä½•ä»Žèº«ä»½è¯IDèŽ·å–ç”Ÿæ—¥/æ€§åˆ«ç­‰ä¿¡æ¯ åœ¨ã€åŸºäºŽRçš„æ•°æ®æ¸…æ´—ï¼ˆ2ï¼‰ã€‘ä¸­ï¼Œå°†ä¼šæ¶‰åŠä¸€äº›æœ‰ä»£è¡¨æ€§çš„ç‰¹å¾çš„å¤„ç†ï¼Œä¸ºç‰¹å¾å·¥ç¨‹æ¨¡å—åšé“ºåž« ä½œä¸šï¼šRMarkdownçš„å®‰è£…ä¸Žä½¿ç”¨]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>R</tag>
        <tag>data cleaning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNNç¬”è®° - LeNet5ç»“æž„è¯¦è§£]]></title>
    <url>%2F2018%2F05%2F02%2FCNN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-LeNet5%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[å­¦ä¹ å·ç§¯ç¥žç»ç½‘ç»œ(CNN)æ˜¯å­¦ä¹ TensorFlowç»•ä¸å¼€çš„ä¸€é“åŽCNNçš„å­¦ä¹ è®¡åˆ’æ˜¯ï¼šã€ç®—æ³•ç»“æž„è¯¦è§£ + Tensorflowä»£ç å®žçŽ°ã€‘ LeNet5 AlexNet VGG GoogleNet ResNet ====== æ¦‚å¿µé“ºåž« ======CNNæœ‰å‡ ä¸ªé‡è¦çš„ç‚¹ï¼šå±€éƒ¨æ„ŸçŸ¥ã€å‚æ•°å…±äº«ã€æ± åŒ–ã€‚ å±€éƒ¨æ„ŸçŸ¥æ¯ä¸ªç¥žç»å…ƒå…¶å®žæ²¡æœ‰å¿…è¦å¯¹å…¨å±€å›¾åƒè¿›è¡Œæ„ŸçŸ¥ï¼Œåªéœ€è¦å¯¹å±€éƒ¨è¿›è¡Œæ„ŸçŸ¥ï¼Œç„¶åŽåœ¨æ›´é«˜å±‚å°†å±€éƒ¨çš„ä¿¡æ¯ç»¼åˆèµ·æ¥å°±å¾—åˆ°äº†å…¨å±€çš„ä¿¡æ¯ã€‚ å‚æ•°å…±äº«å‚æ•°å…±äº«æ˜¯å‡å°‘å‚æ•°çš„æœ‰æ•ˆåŠžæ³•ã€‚å…·ä½“åšæ³•æ˜¯ï¼Œåœ¨å±€éƒ¨è¿žæŽ¥ä¸­éšè—å±‚çš„æ¯ä¸€ä¸ªç¥žç»å…ƒè¿žæŽ¥çš„æ˜¯ä¸€ä¸ª 10Ã—10 çš„å±€éƒ¨å›¾åƒï¼Œå› æ­¤æœ‰10Ã—10 ä¸ªæƒå€¼å‚æ•°ï¼Œå°†è¿™ 10Ã—10 ä¸ªæƒå€¼å‚æ•°å…±äº«ç»™å‰©ä¸‹çš„ç¥žç»å…ƒã€‚ å•æ ¸å•é€šé“å·ç§¯å•ä¸ªå·ç§¯æ ¸ç¤ºæ„ï¼š æ­¥é•¿ Stride = 15Ã—5 Image â€”â€”&gt; 3Ã—3 kernel â€”â€”&gt; (5-3+1)Ã—(5-3+1) convolved feature map å¤šæ ¸å•é€šé“å·ç§¯ä¸€ä¸ªå·ç§¯æ ¸æå–å¾—åˆ°çš„ç‰¹å¾æ˜¯ä¸å……åˆ†çš„ã€‚å‡è®¾æœ‰kä¸ªå·ç§¯æ ¸ï¼Œé‚£ä¹ˆå¯è®­ç»ƒçš„å‚æ•°çš„ä¸ªæ•°å°±å˜ä¸ºäº†kÃ—10Ã—10ã€‚æ³¨æ„æ²¡æœ‰åŒ…å«åç½®å‚æ•°ã€‚æ¯ä¸ªå·ç§¯æ ¸å¾—åˆ°ä¸€ä¸ªFeature Mapã€‚å·ç§¯çš„è¿‡ç¨‹ä¸ºç‰¹å¾æå–çš„è¿‡ç¨‹ï¼Œå¤šæ ¸å·ç§¯ä¸­ï¼Œéšå±‚çš„èŠ‚ç‚¹æ•°é‡ä¸ºï¼š kÃ—(1000-100+1)Ã—(1000-100+1) ï¼Œå¯¹äºŽä¸‹å›¾çš„æ‰‹å†™æ•°å­—ç°åº¦å›¾ï¼Œåšå•é€šé“å·åŠæ“ä½œï¼š å¤šæ ¸å¤šé€šé“å·ç§¯å½“å›¾åƒä¸ºRGBæˆ–ARGBï¼ˆAä»£è¡¨é€æ˜Žåº¦ï¼‰æ—¶ï¼Œå¯ä»¥åœ¨å¤šé€šé“è¿›è¡Œå·ç§¯æ“ä½œï¼Œæˆ–è€…å¯¹äºŽå †å å·ç§¯å±‚æ¥è¯´ï¼Œ pooling å±‚ä¹‹åŽå¯ä»¥ç»§ç»­æŽ¥ä¸‹ä¸€ä¸ª å·ç§¯å±‚ï¼Œå¯¹ pooling å±‚å¤šä¸ª Feature Map çš„æ“ä½œå³ä¸ºå¤šé€šé“å·ç§¯ï¼Œä¸‹å›¾ä¸º ä¸¤ä¸ªå·ç§¯æ ¸åœ¨ARGBå››é€šé“ä¸Šè¿›è¡Œå·ç§¯æ“ä½œï¼Œåœ¨ç”Ÿæˆ å¯¹åº”çš„ Feature Map æ—¶ï¼Œ è¿™ä¸ªå·ç§¯æ ¸å¯¹åº”4ä¸ªå·ç§¯æ¨¡æ¿ï¼ˆè¿™ä¸€ä¸ªå·ç§¯æ ¸å¯¹åº”çš„å››ä¸ªæ¨¡æ¿éƒ½ä¸ä¸€æ ·ï¼‰ï¼Œåˆ†åˆ«ç”¨4ç§ä¸åŒçš„é¢œè‰²è¡¨ç¤ºï¼ŒFeature Map å¯¹åº”çš„ä½ç½®çš„å€¼æ˜¯ç”±å››æ ¸å·ç§¯æ¨¡æ¿åˆ†åˆ«ä½œç”¨åœ¨4ä¸ªé€šé“çš„å¯¹åº”ä½ç½®å¤„çš„å·ç§¯ç»“æžœç›¸åŠ ç„¶åŽå–æ¿€æ´»å‡½æ•°å¾—åˆ°çš„ï¼Œæ‰€ä»¥åœ¨å››é€šé“å¾—åˆ°2é€šé“çš„è¿‡ç¨‹ä¸­ï¼Œå‚æ•°æ•°ç›®ä¸º 4Ã—2Ã—2Ã—2ä¸ªï¼Œå…¶ä¸­4è¡¨ç¤º4ä¸ªé€šé“ï¼Œç¬¬ä¸€ä¸ª2è¡¨ç¤ºç”Ÿæˆ2ä¸ªå·ç§¯æ ¸ï¼Œæœ€åŽçš„2Ã—2è¡¨ç¤ºå·ç§¯æ ¸å¤§å°ã€‚è§ä¸‹å›¾ï¼š æ± åŒ– poolingæ± åŒ–è¿‡ç¨‹ç¤ºæ„ï¼š æ­£æ–‡ï¼šLeNet5ä¸»è¦ç”¨äºŽæ‰‹å†™æ•°å­—çš„è‡ªåŠ¨è¯†åˆ«outputè¾“å‡ºç±»åˆ«å…±10ä¸ªï¼Œä¸ºæ•°å­—0,1,2,â€¦,9 LeNetçš„8å±‚ç»“æž„ï¼šè¾“å…¥å±‚-C1å·ç§¯å±‚-S2æ± åŒ–å±‚-C3å·ç§¯å±‚-S4æ± åŒ–å±‚-C5å·ç§¯å±‚-F6å…¨è¿žæŽ¥å±‚-è¾“å‡ºå±‚ LeNetçš„8å±‚å…¨è¿‡ç¨‹å›¾ï¼š ä¸‹å›¾ä¸ºå¯¹åº”æ¯å±‚çš„å‚æ•°ä¸ªæ•°parameterså’Œè¿žæŽ¥ç‚¹ä¸ªæ•°connectionsè®¡ç®—æµç¨‹å›¾ï¼Œæ¯ä¸ªfeature mapä¸‹æœ‰è¯¥å±‚çš„è®¡ç®—å…¬å¼ï¼š == è¾“å…¥å±‚ == 1å¼ 32Ã—32çš„å›¾ç‰‡ == C1å·ç§¯å±‚ == å•é€šé“ï¼Œ6ä¸ªå·ç§¯æ ¸ï¼Œå¾—åˆ°6ä¸ªfeature maps kernal size: 5Ã—5 feature map size: (32-5+1)Ã—(32-5+1)= 28Ã—28 parameters: 6Ã—(5Ã—5+1) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5Ã—5ä¸ºå·ç§¯æ¨¡æ¿å‚æ•°ï¼Œ1ä¸ºåç½®å‚æ•° connections: 6Ã—(5Ã—5+1) Ã—28Ã—28 == S2æ± åŒ–å±‚ == 6ä¸ªæ± åŒ–æ ¸ï¼Œå¾—åˆ°6ä¸ªfeature maps kernal sizeï¼š2Ã—2 feature map size: (28/2)Ã—(28/2)= 14Ã—14 parameters: 6Ã—(1+1) parametersè®¡ç®—è¿‡ç¨‹ï¼š2Ã—2 å•å…ƒé‡Œçš„å€¼ç›¸åŠ ç„¶åŽå†ä¹˜ä»¥è®­ç»ƒå‚æ•°wï¼Œå†åŠ ä¸Šä¸€ä¸ªåç½®å‚æ•°b(æ¯ä¸€ä¸ªfeature mapå…±äº«ç›¸åŒwå’Œb)ï¼ˆè¿™ä¸ªåœ°æ–¹å’Œä¹‹å‰è‡ªå·±æƒ³çš„ä¸å¤ªä¸€æ ·ï¼‰ connections: 6Ã—(2Ã—2+1) Ã—14Ã—14 ä¸‹å›¾ä¸ºå·ç§¯æ“ä½œä¸Žæ± åŒ–çš„ç¤ºæ„å›¾ï¼š == C3å·ç§¯å±‚ == å¤šé€šé“ï¼š14ä¸ªé€šé“ï¼Œ16ä¸ªå·ç§¯æ ¸ï¼Œå¾—åˆ°16ä¸ªfeature maps kernal sizeï¼š5Ã—5 feature map size: (14-5+1)Ã—(14-5+1ï¼‰= 10Ã—10 parameters: 6Ã—(3Ã—5Ã—5+1) + 6Ã—(4Ã—5Ã—5+1) + 3Ã—(4Ã—5Ã—5+1) + 1Ã—(6Ã—5Ã—5+1) æ³¨æ„æ­¤å¤„C3å¹¶ä¸æ˜¯ä¸ŽS2å…¨è¿žæŽ¥è€Œæ˜¯éƒ¨åˆ†è¿žæŽ¥ï¼Œè§ä¸‹å›¾ connections: 6Ã—(3Ã—5Ã—5+1) + 6Ã—(4Ã—5Ã—5+1) + 3Ã—(4Ã—5Ã—5+1) + 1Ã—(6Ã—5Ã—5+1) Ã—10Ã—10 ä¸ºä»€ä¹ˆé‡‡ç”¨éƒ¨åˆ†è¿žæŽ¥ï¼Ÿ Dropoutçš„ç”±æ¥é¦–å…ˆéƒ¨åˆ†è¿žæŽ¥ï¼Œå¯è®¡ç®—çš„å‚æ•°å°±ä¼šæ¯”è¾ƒå°‘å…¶æ¬¡æ›´é‡è¦çš„æ˜¯å®ƒèƒ½æ‰“ç ´å¯¹ç§°æ€§ï¼Œè¿™æ ·å°±èƒ½å¾—åˆ°è¾“å…¥çš„ä¸åŒç‰¹å¾é›†åˆä»¥ç¬¬0ä¸ªfeature mapæè¿°è®¡ç®—è¿‡ç¨‹ï¼šç”¨1ä¸ªå·ç§¯æ ¸(å¯¹åº”3ä¸ªå·ç§¯æ¨¡æ¿ï¼Œä½†ä»ç§°ä¸ºä¸€ä¸ªå·ç§¯æ ¸ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯ä¸‰ç»´å·ç§¯æ ¸)åˆ†åˆ«ä¸ŽS2å±‚çš„3ä¸ªfeature mapsè¿›è¡Œå·ç§¯ï¼Œç„¶åŽå°†å·ç§¯çš„ç»“æžœç›¸åŠ ï¼Œå†åŠ ä¸Šä¸€ä¸ªåç½®ï¼Œå†æžä¸ªæ¿€æ´»å‡½æ•°å°±å¯ä»¥å¾—å‡ºå¯¹åº”çš„feature mapäº† == S4æ± åŒ–å±‚ == 16ä¸ªæ± åŒ–æ ¸ï¼Œå¾—åˆ°16ä¸ªfeature maps kernal sizeï¼š2Ã—2 feature map size: (10/2)Ã—(10/2)= 5Ã—5 parameters: 16Ã—(1+1) connections: 16Ã—(2Ã—2+1) Ã—5Ã—5 == C5å·ç§¯å±‚ == 120ä¸ªå·ç§¯æ ¸ï¼Œå¾—åˆ°120ä¸ªfeature maps kernal sizeï¼š5Ã—5 æ¯ä¸ªfeature mapçš„å¤§å°éƒ½ä¸Žä¸Šä¸€å±‚S4çš„æ‰€æœ‰feature mapsè¿›è¡Œè¿žæŽ¥ï¼Œè¿™æ ·ä¸€ä¸ªå·ç§¯æ ¸å°±æœ‰16ä¸ªå·ç§¯æ¨¡æ¿ feature map size: (5-5+1)Ã—(5-5+1ï¼‰= 1Ã—1 &nbsp;&nbsp;&nbsp;&nbsp;è¿™æ ·åˆšå¥½å˜æˆäº†å…¨è¿žæŽ¥ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸æŠŠå®ƒå†™æˆF5ï¼Œå› ä¸ºè¿™åªæ˜¯å·§åˆ parameters: 120Ã—(16Ã—5Ã—5+1) connections: 120Ã—(16Ã—5Ã—5+1) Ã—1Ã—1 == F6å…¨è¿žæŽ¥å±‚ == Full Connection parameters: 84Ã—(120Ã—1Ã—1+1) connections: 84Ã—(120Ã—1Ã—1+1) Ã—1Ã—1 == è¾“å‡ºå±‚ == å¾—åˆ°åˆ†ç±»ç»“æžœï¼Œä¾‹ï¼šç»“æžœä¸ºæ•°å­— 5 ä»£ç å®žçŽ°ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import osprint(os.getcwd()) #æ˜¾ç¤ºå½“å‰è·¯å¾„os.chdir(&quot;D:/test/&quot;) #åœ¨å¼•å·å†…å¡«å…¥ä½ æƒ³æ”¾ä»£ç å’Œæ•°æ®çš„è·¯å¾„import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;,one_hot = True) #è¯»å–æ•°æ®sess = tf.InteractiveSession() def weight_variable(shape): # æƒå€¼åˆå§‹åŒ–è®¾ç½® initial = tf.truncated_normal(shape,stddev=0.1) return tf.Variable(initial) def bias_variable(shape): # åç½®biasåˆå§‹åŒ–è®¾ç½® initial = tf.constant(0.1,shape = shape) return tf.Variable(initial) def conv2d(x,W): return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=&apos;SAME&apos;) # å·ç§¯strides=[é¦–ä½é»˜è®¤ä¸º1,å¹³è¡Œæ­¥é•¿=1,ç«–ç›´æ­¥é•¿=1,å°¾ä½é»˜è®¤ä¸º1] def max_pool_2x2(x): return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=&apos;SAME&apos;) # ksize=[1,2,2,1] æ± åŒ–æ ¸size: 2Ã—2 # æ± åŒ–strides=[é¦–ä½é»˜è®¤ä¸º1,å¹³è¡Œæ­¥é•¿=2,ç«–ç›´æ­¥é•¿=2,å°¾ä½é»˜è®¤ä¸º1]# placeholderï¼šç­‰å¾…è¾“å…¥æ•°æ®ï¼Œxä¸ºå ä½ç¬¦ï¼ŒæŽ¥å—åž‹å·ä¸ºfloat32çš„æ•°æ®ï¼Œè¾“å…¥æ ¼å¼ä¸ºçŸ©é˜µ[None,784]x = tf.placeholder(tf.float32,[None,784]) #784 = 28Ã—28 åªå¯¹è¾“å…¥çŸ©é˜µçš„åˆ—æ•°æœ‰è¦æ±‚y_ = tf.placeholder(tf.float32,[None,10]) x_image = tf.reshape(x,[-1,28,28,1]) # [batch=-1, height=28, width=28, in_channels=1]# Conv1 Layer å·ç§¯å±‚ W_conv1 = weight_variable([5,5,1,32]) # [5Ã—5å·ç§¯æ ¸, in_channels=1, out_channels=32=å·ç§¯æ ¸ä¸ªæ•°]b_conv1 = bias_variable([32]) # [32=å·ç§¯æ ¸ä¸ªæ•°=biasä¸ªæ•°]h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1) # æ¿€æ´»å‡½æ•°ï¼šreluh_pool1 = max_pool_2x2(h_conv1) # æ± åŒ–æ–¹å¼ï¼šmax pool å–æœ€å¤§å€¼ # Conv2 Layer W_conv2 = weight_variable([5,5,32,64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2) # æ¿€æ´»å‡½æ•°ï¼šreluh_pool2 = max_pool_2x2(h_conv2) W_fc1 = weight_variable([7*7*64,1024]) b_fc1 = bias_variable([1024]) h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1) keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob) W_fc2 = weight_variable([1024,10]) b_fc2 = bias_variable([10]) y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2) # æŸå¤±å‡½æ•°loss function: äº¤å‰ç†µcross_entropycross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv),reduction_indices=[1])) train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # ä¼˜åŒ–æ–¹æ³•ï¼šAdamOptimizer| å­¦ä¹ é€ŸçŽ‡ï¼š(1e-4)| äº¤å‰ç†µï¼šæœ€å°åŒ– correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1)) # tf.equalè¿”å›žå¸ƒå°”å€¼ | tf.argmax(y_,1)ï¼šæ•°å­—1ä»£è¡¨æœ€å¤§å€¼accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) tf.global_variables_initializer().run() for i in range(20000): batch = mnist.train.next_batch(50) # å–‚å…¥è®­ç»ƒé›†çš„æ•°æ® if i % 1000 == 0: # æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ŒæŠŠ1000æ”¹æˆ1ï¼šéšæœºæ¢¯åº¦ä¸‹é™ train_accuracy = accuracy.eval(feed_dict=&#123;x:batch[0],y_:batch[1],keep_prob:1.0&#125;) # train accuracy: accuracy.eval print(&quot;step %d, training accuracy %g&quot;%(i,train_accuracy)) train_step.run(feed_dict=&#123;x:batch[0],y_:batch[1],keep_prob:0.5&#125;) # test accuracy: accuracy.eval print(&quot;test accuracy %g&quot;%accuracy.eval(feed_dict=&#123;x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0&#125;)) å‚è€ƒï¼š ä¸€ä¸ªå¥½çŽ©çš„åšæ•°å­—åˆ†ç±»çš„ç½‘ç«™ å¦ä¸€ä¸ªå¥½çŽ©çš„å·ç§¯ç½‘ç«™ LeNet5ä»‹ç» åˆ«äººçš„ç¬”è®°çœ‹çš„å†å¤šä¹Ÿä¸å¦‚äº²è‡ªçœ‹ä¸€éåŽŸè‘—è®ºæ–‡å‘€~â¤ LeNetè®ºæ–‡é“¾æŽ¥]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>LeNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anacondaå¤šçŽ¯å¢ƒå¤šç‰ˆæœ¬pythoné…ç½®]]></title>
    <url>%2F2018%2F04%2F25%2FAnaconda%E5%A4%9A%E7%8E%AF%E5%A2%83%E5%A4%9A%E7%89%88%E6%9C%ACpython%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[å°è¯•è¿‡ pycharm / jupyter notebook / spyderåœ¨pythonçš„å­¦ä¹ åˆæœŸï¼Œå¯»å¯»è§…è§…åŽé’Ÿæƒ…äºŽAnacondaçš„spyderé€‚åˆåšæ•°æ®åˆ†æžï¼Œå¯ä»¥é€‰ä¸­è¿è¡Œã€‚å…ˆå­¦çš„Rè¯­è¨€æŽ¥è§¦çš„Rstudioï¼Œçœ‹è§spyderå°±æ˜¯æ ¼å¤–äº²åˆ‡ï¼Œä¸Žmatlabçš„çŽ¯å¢ƒéƒ½æ˜¯åŒä¸€é£Žæ ¼çš„ã€‚ ä¸ºä»€ä¹ˆéƒ½æŽ¨èAnacondaåšpythonçš„å­¦ä¹ çŽ¯å¢ƒï¼Ÿå¯è‡ªå®šä¹‰è®¾ç½®Ræˆ–pythonçš„ç‰ˆæœ¬çŽ¯å¢ƒï¼Œä¸”å¯ä»¥éšæ„åˆ‡æ¢ã€‚ pythonå­¦ä¹ ä¸­å…ä¸äº†é‡åˆ°ä¸€ä¼šè¿™ä¸ªæ“ä½œåªèƒ½åœ¨2.7ä¸‹æ–½å±•ï¼Œä¸€ä¼šé‚£ä¸ªæ¡†æž¶åªèƒ½åœ¨3.5ä¸‹æ­å»ºçš„æƒ…å½¢ï¼Œæ‰€ä»¥åœ¨æœºå­ä¸Šæ‹¥æœ‰å¤šçŽ¯å¢ƒç‰ˆæœ¬çš„pythonæ˜¯å¿…å¤‡æŠ€èƒ½äº†ã€‚ ä»¥ä¸‹æ˜¯Anacondaå¤šçŽ¯å¢ƒé…ç½®æ­£æ–‡ 1. ä¸‹è½½AnacondaðŸ‘‰ä¼ é€é—¨å‚»ç“œå¼å®‰è£…æ­¥éª¤ï¼Œåœ¨è·¯å¾„è®¾ç½®æ—¶é€‰ä¸­çŽ¯å¢ƒå˜é‡é…ç½®ï¼Œæˆ–è€…å®‰è£…å®Œè‡ªå·±é…ç½®ã€‚æˆ‘é€‰çš„3.6winç‰ˆæœ¬ã€‚ 2. æ‰“å¼€Anaconda Navigatorå·¦è¾¹ä¸€åˆ—æœ‰ Home/ Environments/ Projects/ Learning/ Communityé€‰ä¸­Environments defaultçŽ¯å¢ƒï¼šbase(root) å¦å¤–å¢žåŠ çš„3.5çŽ¯å¢ƒï¼špython35 å‡è®¾éœ€æ±‚æ˜¯æ­å»º3.5çŽ¯å¢ƒ ç‚¹å‡»æœ€ä¸‹ä¸€æŽ’çš„create å‡ºçŽ°create new environmentå¯¹è¯æ¡† python35 ä½œä¸ºåå­—æ ‡è¯†ï¼ŒNameä¸èƒ½ä»¥æ•°å­—å¼€å¤´ å¯é€‰pythonæˆ–è€…Rï¼Œè¿™é‡Œé€‰ä¸­python åœ¨ç‰ˆæœ¬ä¸‹æ‹‰æ¡†é€‰æ‹©3.5 ç‚¹å‡» createï¼Œç­‰å¾…ä¸€ä¼šå°±åœ¨Environmentså‡ºçŽ°å¦‚å›¾python35çŽ¯å¢ƒ æ³¨ï¼šåŠ å¯†çŽ¯å¢ƒä¼šå¯¼è‡´ä¸èƒ½ç”¨Anaconda Navigatoræ³¨ï¼šå¢žåŠ çŽ¯å¢ƒé…ç½®æ­¥éª¤ä¹Ÿå¯åœ¨Anaconda Promptå®žçŽ°12345æ‰“å¼€Anaconda Prompt ä½¿ç”¨å‘½ä»¤conda info -eæŸ¥çœ‹å½“å‰ç³»ç»Ÿä¸‹çš„çŽ¯å¢ƒåç§° ä½¿ç”¨å‘½ä»¤conda create --name python35 python=3.5åˆ›å»ºä¸€ä¸ªåä¸ºpython35çš„æ–°çŽ¯å¢ƒï¼Œå¹¶æŒ‡å®špythonç‰ˆæœ¬ä¸º3.5ï¼Œå¦‚æžœä»…ç”¨python=3ï¼Œåˆ™ä¼šå®‰è£…æœ€æ–°çš„3.xç‰ˆæœ¬ å®‰è£…å®ŒæˆåŽé€šè¿‡activate python35æ¿€æ´»æ–°çŽ¯å¢ƒ æ­¤æ—¶æŸ¥çœ‹pythonç‰ˆæœ¬python --versionå³ä¸ºpython3çš„ç‰ˆæœ¬ 3. åœ¨æ–°çŽ¯å¢ƒé…ç½® spyder/ jupyetr/â€¦ æ‰“å¼€promptæ¿€æ´»æ–°çŽ¯å¢ƒ è¾“å…¥ activate name(çŽ¯å¢ƒå)è¿™é‡Œè¾“å…¥activate python35ä¸ç”¨é…ç½®ä»»ä½•çŽ¯å¢ƒå˜é‡ï¼Œåœ¨æœ€å‰æ–¹çš„æ‹¬å·é‡Œ(python35)æ˜¾ç¤ºå·²ç»åœ¨3.5çŽ¯å¢ƒçš„è·¯å¾„ä¸‹ é…ç½®spyderè¾“å…¥ conda install spyderä¹Ÿå¯ä¸‹è½½jupyter notebookæˆ–è€…å…¶ä»–ç­‰å¾…å®‰è£…å®Œæˆï¼Œåœ¨ä½ çš„èœå•ä¸‹ä¹Ÿä¼šå‡ºçŽ°spyder(python35)ï¼Œå¦‚ä¸‹å›¾è¿™æ ·å¯ä»¥ç›´æŽ¥åœ¨è¿™ä¸ªçŽ¯å¢ƒä¸‹æ•²ä»£ç ðŸ‘ install packageséœ€è¦çš„åŒ…å¯ä»¥åœ¨navigatorä¸Šæœç´¢ä¸‹è½½ï¼Œæˆ–è€…åœ¨promptä¸Špip installâ€¦ é€€å‡ºå½“å‰pythonçŽ¯å¢ƒå½“å‰å·¥ä½œçŽ¯å¢ƒçš„è·¯å¾„åˆ‡æ¢åˆ°ç³»ç»Ÿæ ¹ç›®å½•ï¼špromptè¾“å…¥ï¼šdeactivate åˆ°æ­¤çŽ¯å¢ƒå°±æ­å®Œå•¦~ å¯ä»¥å®‰å¿ƒçš„åœ¨pythonä¸Šå¤§å±•èº«æ‰‹å•¦ï¼ å‚è€ƒï¼šAnacondaå¤šçŽ¯å¢ƒå¤šç‰ˆæœ¬pythoné…ç½®æŒ‡å¯¼å¦‚ä½•åœ¨å¤šç‰ˆæœ¬anaconda pythonçŽ¯å¢ƒä¸‹è½¬æ¢spyderï¼Ÿ]]></content>
      <tags>
        <tag>python</tag>
        <tag>anaconda</tag>
        <tag>spyder</tag>
        <tag>environment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¦‚ä½•æ­å»ºè‡ªå·±çš„ä¸ªäººç½‘ç«™ï¼ˆä¸‹ï¼‰]]></title>
    <url>%2F2018%2F04%2F20%2F%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[æœ¬ç¯‡æ¶‰åŠHexoä¸ªäººç½‘ç«™çš„åŸŸåç»‘å®šã€æ·»åŠ è¯„è®ºåŠŸèƒ½ã€è®¿é—®æ¬¡æ•°ç»Ÿè®¡è®¾ç½®ç­‰æ•™ç¨‹ã€‚é€‚å®œäººç¾¤ï¼šæœ‰è¿½æ±‚çš„åšä¸»ä»¬ï¼ˆä¸»é¢˜Themeæœªä½¿ç”¨Nextçš„åšä¸»ä»¬ä¹Ÿè¯·è¿›ï¼‰ åŸŸåç»‘å®šæ ¹æ®ä¸Šç¯‡æ•™ç¨‹ï¼Œç›®å‰é»˜è®¤çš„åŸŸåè¿˜æ˜¯username.github.ioï¼Œä½†å°è±¡ä¸­ä¸ªäººç½‘ç«™å¥½åƒéƒ½æ˜¯www.name.comæ ¼å¼çš„ï¼Ÿæ€Žä¹ˆæ ·æƒ³æ¢å—ï¼Ÿé¦–å…ˆâœ‹ï¼Œä½ å¾—æœ‰___ï¼Ÿ å½“ç„¶æ˜¯åŸŸåå•¦ï¼ ä¹°åŸŸåä½ å¾—ä¹°ä¸€ä¸ªåŸŸåã€‚xxäº‘éƒ½èƒ½ä¹°ï¼Œæˆ‘åœ¨è…¾è®¯äº‘ä¹°çš„ã€‚ï¼ˆè¯·ç»™æˆ‘å¹¿å‘Šè´¹ï¼@è…¾è®¯äº‘ï¼‰æˆ‘å®žåœ¨å¤ªå–œæ¬¢Karlie Klossï¼Œå¥¹çš„ä¸€ä¸ªç½‘ç«™kodewithklossy.comé‚£ä¹ˆæˆ‘çš„è‚¯å®šå°±æ˜¯codewithzhangyi.comðŸ˜œ å®žåè®¤è¯å®¡æ ¸ä¹°å¥½åŸŸåä¹‹åŽï¼Œæ‰“å¼€åŸŸåæœåŠ¡ï¼Œå¯ä»¥çœ‹è§ä½ åˆšä¹°çš„åŸŸåè®°å½•ã€‚æ­¤æ—¶ä½ éœ€è¦åšå®žåè®¤è¯ï¼Œå¾ˆé‡è¦ï¼Œè¶…è¿‡æœ‰æ•ˆæœŸï¼ˆå¤§æ¦‚3-5å¤©ï¼‰æœªè®¤è¯åŸŸåå°†è¢«é”å®šã€‚ å¤‡æ¡ˆå¦‚æžœæœåŠ¡å™¨æ˜¯å†…åœ°çš„å°±éœ€è¦å¤‡æ¡ˆã€‚Hexoç½‘ç«™çš„æœåŠ¡å™¨æ˜¯æµ·å¤–çš„å› æ­¤å¯ä»¥è·³è¿‡è¿™æ­¥ã€‚ æ·»åŠ è§£æžè®°å½•è®¤è¯æˆåŠŸåŽçš„åŸŸåæ‰èƒ½è¢«åšè§£æžã€‚è®¤è¯éœ€è¦3-5ä¸ªå·¥ä½œæ—¥ã€‚æˆ‘çš„å¤§æ¦‚åŠå¤©å°±OKäº†ã€‚å½“è§£æžçŠ¶æ€å˜æˆã€æ­£å¸¸è§£æžã€‘æ—¶ï¼Œç‚¹å‡»åŸŸåè®°å½•æœ€å³ä¾§æ“ä½œçš„ã€è§£æžã€‘æ·»åŠ è§£æžï¼šè§£æžè®°å½•è®¾ç½®ä¸¤ä¸ªï¼šwwwå’Œ@ï¼Œçº¿è·¯é»˜è®¤å°±ok(1) @:è®°å½•ç±»åž‹é€‰Aï¼ŒAè®°å½•çš„å°±æ˜¯ipåœ°å€ï¼Œgithub(å®˜æ–¹æ–‡æ¡£)æä¾›äº†ä¸¤ä¸ªIPåœ°å€ï¼Œ192.30.252.153å’Œ192.30.252.154ï¼Œè¿™ä¸¤ä¸ªIPåœ°å€ä¸ºgithubçš„æœåŠ¡å™¨åœ°å€ï¼Œä¸¤ä¸ªéƒ½è¦å¡«ä¸Š(2) www:è®°å½•ç±»åž‹é€‰CNAMEï¼ŒCNAMEè®°å½•å€¼å¡«ä½ çš„githubåšå®¢ç½‘å€ï¼Œå¦‚æˆ‘çš„æ˜¯yzhang1270.github.io æœ¬åœ°è®¾ç½®ä¿®æ”¹è¿™äº›å…¨éƒ¨è®¾ç½®å®ŒæˆåŽï¼Œæ­¤æ—¶ä½ å¹¶ä¸èƒ½æ ¹æ®ç”³è¯·çš„åŸŸåè®¿é—®ä½ çš„åšå®¢ã€‚æŽ¥ç€ä½ éœ€è¦åšçš„æ˜¯åœ¨hexoæ ¹ç›®å½•çš„sourceæ–‡ä»¶å¤¹é‡Œåˆ›å»ºCNAMEæ–‡ä»¶ï¼Œä¸å¸¦ä»»ä½•åŽç¼€ï¼Œé‡Œé¢æ·»åŠ ä½ çš„åŸŸåä¿¡æ¯ï¼Œå¦‚ï¼šcodewithzhangyi.comã€‚å®žè·µè¯æ˜Žå¦‚æžœæ­¤æ—¶ä½ å¡«å†™çš„æ˜¯www.codewithzhangyi.comé‚£ä¹ˆä»¥åŽä½ åªèƒ½ç”¨www.codewithzhangyi.comè®¿é—®ï¼Œè€Œå¦‚æžœä½ å¡«å†™çš„æ˜¯codewithzhangyi.comï¼Œé‚£ä¹ˆç”¨www.codewithzhangyi.comå’Œcodewithzhangyi.comè®¿é—®éƒ½æ˜¯å¯ä»¥çš„ã€‚é‡æ–°hexo g,å¹¶å‘å¸ƒå³å¯ç”¨æ–°çš„åŸŸåè®¿é—®ã€‚ è®¿é—®å‡ºçŽ°404çš„åŽŸå› å¯èƒ½æ˜¯ï¼š(1)ç»‘å®šäº†ä¸ªäººåŸŸåï¼Œä½†æ˜¯åŸŸåè§£æžé”™è¯¯ã€‚(2)åŸŸåè§£æžæ­£ç¡®ä½†ä½ çš„åŸŸåæ˜¯é€šè¿‡å›½å†…æ³¨å†Œå•†æ³¨å†Œçš„ï¼Œä½ çš„åŸŸåå› æ²¡æœ‰å®žååˆ¶è€Œæ— æ³•è®¿é—®ã€‚(3)é…ç½®æ²¡é—®é¢˜çš„æƒ…å†µä¸‹ï¼Œæ¢ä¸ªæµè§ˆå™¨è¯•è¯•ã€‚(4)ä¸‹è½½çš„hexoæœ‰é—®é¢˜ï¼Œé‡æ–°ä¸‹è½½ã€‚ æ·»åŠ è¯„è®ºæ¨¡å— åœ¨æ·»åŠ è¯„è®ºè¿™ä¸ªè®¾ç½®ä¸Šè´¹äº†ç‚¹æ—¶é—´ï¼Œå› ä¸ºæ•´é¡¿ï¼Œè¯„è®ºæœåŠ¡æŒ‚äº†ä¸€å¤§ç‰‡ï¼Œè¯·å„ä½å¯»æ‰¾æ•™ç¨‹çš„æ—¶å€™é‡ç‚¹çœ‹æ—¶é—´ï¼Œ2017å¹´åŠä¹‹å‰çš„æ–‡ç« å°±æ²¡æœ‰å¤šå¤§çš„å€Ÿé‰´æ„ä¹‰ï¼ŒåŒ…æ‹¬è¿™ç¯‡æ•™ç¨‹ä¹Ÿæ˜¯æœ‰æ—¶é™æ€§çš„ï¼Œè°èƒ½è·Ÿçš„ä¸Šå˜åŒ–å‘¢ã€‚ (1) å¤šè¯´ - æœ€å¤šç”¨æˆ·ä½¿ç”¨çš„è¯„è®ºï¼Œä½†é—æ†¾2017å¹´6æœˆå°†æš‚å®šæœåŠ¡ï¼›ä¸å»ºè®®æ–°ç”¨æˆ·ä½¿ç”¨ï¼Œä½†ä¸ºæ—§ç”¨æˆ·ä¿ç•™ï¼Œä¹Ÿæ„Ÿè°¢å¤šè¯´ä¸€è·¯çš„é™ªä¼´ï¼›(2) ç½‘æ˜“äº‘è·Ÿå¸– - ç½‘æ˜“æä¾›çš„è¯„è®ºç»„ä»¶ï¼ŒåŠŸèƒ½æ¯”è¾ƒç®€å•ï¼Œæ€§èƒ½ä¼˜ç§€ï¼›ç®¡ç†åŽå°åœ¨æŸ¥è¯¢ä¸Šè¿˜ä¸ç®—ç‰¹åˆ«æ™ºèƒ½ï¼Œä½†è¶³å¤Ÿæ™®é€šç”¨æˆ·ä½¿ç”¨ï¼›(3) ç•…è¨€ - æœç‹æä¾›çš„è¯„è®ºç»„ä»¶ï¼ŒåŠŸèƒ½ä¸°å¯Œï¼Œä½“éªŒä¼˜å¼‚ï¼›ä½†å¿…é¡»è¿›è¡ŒåŸŸåå¤‡æ¡ˆã€‚åªè¦åŸŸåå¤‡è¿‡æ¡ˆå°±å¯ä»¥é€šè¿‡å®¡æ ¸ã€‚(4) Disqus - å›½å¤–ä½¿ç”¨è¾ƒå¤šçš„è¯„è®ºç»„ä»¶ã€‚ä¸‡é‡Œé•¿åŸŽæ°¸ä¸å€’ï¼Œä¸€æžçº¢æå‡ºå¢™æ¥ï¼Œä½ æ‡‚çš„ã€‚ä»¥ä¸Šè¯„è®ºæ¨¡å—åº”è¯¥å¤§å®¶éƒ½çŸ¥é“ï¼Œå¤šè¯´å’Œç½‘æ˜“äº‘è·Ÿå¸–æ²¡æœ‰äº†ï¼Œç•…è¨€è¦å¤‡æ¡ˆï¼Œå¯¹äºŽå¯¹äºŽæŒ‚é åœ¨GitHubçš„åšå®¢éžå¸¸çš„ä¸å‹å¥½ï¼Œæ”¾å¼ƒï¼Disqusï¼Œä¸å¸Œæœ›è‡ªå·±çš„åšå®¢ï¼Œå¯ä»¥ä¸åˆ†å›½ç•Œï¼ä¹Ÿæ”¾å¼ƒï¼ è¸©å‘æ€»ç»“ï¼šä½¿ç”¨Gitmentè¯„è®ºæœåŠ¡Gitment æ˜¯ä½œè€…imsunå®žçŽ°çš„ä¸€æ¬¾åŸºäºŽ GitHub Issues çš„è¯„è®ºç³»ç»Ÿã€‚æ”¯æŒåœ¨å‰ç«¯ç›´æŽ¥å¼•å…¥ï¼Œä¸éœ€è¦ä»»ä½•åŽç«¯ä»£ç ã€‚å¯ä»¥åœ¨é¡µé¢è¿›è¡Œç™»å½•ã€æŸ¥çœ‹ã€è¯„è®ºã€ç‚¹èµžç­‰æ“ä½œï¼ŒåŒæ—¶æœ‰å®Œæ•´çš„ Markdown / GFM å’Œä»£ç é«˜äº®æ”¯æŒã€‚å°¤ä¸ºé€‚åˆå„ç§åŸºäºŽ GitHub Pages çš„é™æ€åšå®¢æˆ–é¡¹ç›®é¡µé¢ã€‚ æ³¨å†Œ OAuth Applicationæ³¨å†Œä¸€ä¸ªæ–°çš„OAuth Applicationï¼Œå…¶ä»–å†…å®¹å¯ä»¥éšæ„å¡«å†™ï¼Œä½†è¦ç¡®ä¿å¡«å…¥æ­£ç¡®çš„ callback URLï¼ˆå¦‚ https:// yzhang1270.github.ioï¼‰è¿™ä¸ªçœŸçš„å¾ˆé‡è¦ï¼ï¼ï¼åˆ›å»ºæˆåŠŸåŽï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ª client ID å’Œä¸€ä¸ª client secretï¼Œè¿™ä¸ªå°†è¢«ç”¨äºŽä¹‹åŽçš„ç”¨æˆ·ç™»å½•ã€‚ å¼•å…¥ Gitmentå°†ä¸‹é¢çš„ä»£ç æ·»åŠ åˆ°ä½ çš„D:\username.github.io\themes\typing\layout_partial\after-footer.ejsï¼šã€æç¤ºã€‘ï¼šæˆ‘è¿™é‡Œçš„ä¸»é¢˜æ˜¯typingï¼Œåœ¨typingé‡Œæ˜¯è‡ªå¸¦gitmentçš„ã€‚ 123456789101112131415&lt;div id=&quot;container&quot;&gt;&lt;/div&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;https://imsun.github.io/gitment/style/default.css&quot;&gt;&lt;script src=&quot;https://imsun.github.io/gitment/dist/gitment.browser.js&quot;&gt;&lt;/script&gt;&lt;script&gt;var gitment = new Gitment(&#123; id: &apos;é¡µé¢ ID&apos;, // å¯é€‰ã€‚é»˜è®¤ä¸º location.href è¿™é‡Œæœ‰æœºå…³ï¼ŒåŽé¢ä¼šå†è®²åˆ°ï¼ðŸ˜Ž owner: &apos;ä½ çš„ GitHub Name&apos;, //æ¯”å¦‚æˆ‘çš„å«YZHANG1270 repo: &apos;å­˜å‚¨è¯„è®ºçš„ repo&apos;, //æ¯”å¦‚æˆ‘çš„å«YZHANG1270.github.io oauth: &#123; client_id: &apos;ä½ çš„ client ID&apos;, //æ¯”å¦‚æˆ‘çš„328*********** client_secret: &apos;ä½ çš„ client secret&apos;, //æ¯”å¦‚æˆ‘çš„49ce*********************** &#125;,&#125;)gitment.render(&apos;container&apos;)&lt;/script&gt; å¯é€‰ï¼šåœ¨ä¸»é¢˜çš„_config.ymlä¸­é…ç½®å¥½å…¨å±€å‚æ•°ï¼šåŒæ—¶ä¹Ÿè¦åœ¨è„šæœ¬ä¿®æ”¹æŒ‡å®šåœ°å€ã€‚ åˆå§‹åŒ–è¯„è®ºé¡µé¢å‘å¸ƒåŽï¼Œä½ å‘çŽ°è¯„è®ºæœ‰ä¸ªerror:æ­¤æ—¶ï¼Œä½ ç‚¹å‡»Loginç”¨è‡ªå·±è´¦æˆ·ç™»é™†ï¼Œå†åˆ·æ–°é¡µé¢å°±æœ‰åˆå§‹åŒ–æŒ‰é”®ï¼Œç‚¹å‡»åˆå§‹åŒ–å³å¯ï¼šæ­£å¸¸æƒ…å†µä¸‹ï¼Œåªè¦ç”¨GitHubè´¦æˆ·ç™»é™†å³å¯å‘å¸ƒè¯„è®ºå•¦ï¼š åœ¨è¯„è®ºåŠŸèƒ½è®¾ç½®è¿™é‡Œè¸©äº†å‡ ä¸ªå‘ï¼Œè¿›è¡Œæ€»ç»“ä¸€ä¸‹ðŸ˜ Gitmentè¯„è®ºåŠŸèƒ½è¸©å‘æ€»ç»“ owner: â€˜Your GitHub IDâ€™1owner: &apos;ä½ çš„ GitHub ID&apos; å¯ä»¥æ˜¯ä½ çš„GitHubç”¨æˆ·åï¼Œä¹Ÿå¯ä»¥æ˜¯GitHub idï¼Œå»ºè®®ç›´æŽ¥ç”¨GitHubç”¨æˆ·åå°±å¯ä»¥ã€‚èŽ·å–GitHub idçš„æ–¹æ³•ï¼š https:// api.github.com/users/ä½ çš„è´¦æˆ·å Error: Not Foundowneræˆ–è€…repoé…ç½®é”™è¯¯äº†ï¼Œæ³¨æ„åå­—å’Œä»“åº“åå­—çš„å¤§å°å†™ã€‚ Error: Comments Not Initialized(1)åœ¨æ³¨å†ŒOAuth Applicationè¿™ä¸ªæ­¥éª¤ä¸­ï¼Œç»™Authorization callback URLæŒ‡å®šçš„åœ°å€é”™äº†(2)è¿˜æ²¡æœ‰åœ¨è¯¥é¡µé¢çš„Gitmentè¯„è®ºåŒºç™»é™†GitHubè´¦å·(3)https://github.com/imsun/gitment/issues/95 Errorï¼švalidation failedè¿™ä¸ªçœŸçš„æŠ˜è…¾æˆ‘ä¸€ä¸‹åˆï¼ï¼(å’¬ç‰™åˆ‡é½¿.jpg)issueçš„æ ‡ç­¾labelæœ‰é•¿åº¦é™åˆ¶ï¼labelsçš„æœ€å¤§é•¿åº¦é™åˆ¶æ˜¯50ä¸ªå­—ç¬¦ã€‚ 1id: &apos;é¡µé¢ ID&apos;, // å¯é€‰ã€‚é»˜è®¤ä¸º location.href è¿™ä¸ªæ˜¯ä¹‹å‰é…ç½®çš„æ—¶å€™æåˆ°çš„æœºå…³ã€‚idçš„ä½œç”¨ï¼Œå°±æ˜¯é’ˆå¯¹ä¸€ä¸ªæ–‡ç« æœ‰å”¯ä¸€çš„æ ‡è¯†æ¥åˆ¤æ–­è¿™ç¯‡æœ¬ç« ã€‚ åœ¨issuesé‡Œé¢ï¼Œå¯ä»¥å‘çŽ°æ˜¯æ ¹æ®ç½‘é¡µæ ‡é¢˜æ¥æ–°å»ºissuesçš„ï¼Œç„¶åŽæ¯ä¸ªissuesæœ‰ä¸¤ä¸ªlabelsï¼ˆæ ‡ç­¾ï¼‰ï¼Œä¸€ä¸ªæ˜¯gitmentï¼Œå¦ä¸€ä¸ªå°±æ˜¯idã€‚æ‰€ä»¥æ˜Žç™½äº†åŽŸç†åŽï¼Œå°±æ˜¯å› ä¸ºidå¤ªé•¿ï¼Œå¯¼è‡´åˆå§‹åŒ–å¤±è´¥ï¼ŒçŽ°åœ¨å°±æ˜¯è¦è®©idä¿è¯åœ¨50ä¸ªå­—ç¬¦å†…ã€‚å¯¹åº”é…ç½®çš„idä¸ºï¼š1id: &apos;&lt;%= page.title %&gt;&apos; å½©è›‹ðŸŽŠï¼š å¦‚æžœç”¨ç½‘é¡µæ ‡é¢˜ä¹Ÿä¸èƒ½ä¿è¯åœ¨50ä¸ªå­—ç¬¦ï¼æœ€åŽï¼Œæˆ‘ç”¨æ–‡ç« çš„æ—¶é—´ï¼Œè¿™æ ·é•¿åº¦æ˜¯ä¿è¯åœ¨50ä¸ªå­—ç¬¦å†…ï¼Œå®Œç¾Žè§£å†³ï¼ï¼ˆé¿å…äº†æ–‡ç« æ¯æ¬¡æ›´æ–°æ ‡é¢˜æˆ–è·¯å¾„æ—¶ï¼Œä¼šé‡æ–°åˆ›å»ºä¸€ä¸ªissueè¯„è®ºçš„é—®é¢˜ã€‚ï¼‰1id: &apos;&lt;%= page.date %&gt;&apos; å¦‚æžœä½ åŽŸæ¥æ²¡æœ‰è®¾ç½®idè¿™ä¸€è¡Œï¼Œè®°å¾—åœ¨è¿™è¡ŒåŽé¢åŠ é€—å·ï¼Œæˆ‘å°±æ ½äº†å‚»äº†ã€‚ Gitmentçš„æ±‰åŒ–åªéœ€åˆ°æ¨¡æ¿é‡Œå°†åŽŸæ¥å®šä¹‰CSSå’ŒJSçš„é‚£ä¸¤è¡Œæ”¹æˆï¼š 12&lt;link rel=&quot;stylesheet&quot; href=&quot;https://billts.site/extra_css/gitment.css&quot;&gt;&lt;script src=&quot;https://billts.site/js/gitment.js&quot;&gt;&lt;/script&gt; æ‰€æœ‰æ–‡ç« ä¸€é”®åˆå§‹åŒ–è¯„è®ºåˆ°æœ¬æ–‡ç¼–å†™æ—¶ï¼Œè¿˜æ²¡æœ‰ä¸€ä¸ªå®Œå–„çš„è§£å†³æ–¹æ³•ï¼Œå°±æ˜¯ç”¨è„šæœ¬æ¥æ‰§è¡Œè‡ªåŠ¨åŒ–ï¼Œæœ‰éœ€è¦çš„å¯ä»¥è¯¦ç»†äº†è§£ï¼šhttps://github.com/imsun/gitment/issues/5 å‚è€ƒæ–‡ç« ï¼šGitmentè¯„è®ºåŠŸèƒ½æŽ¥å…¥è¸©å‘æ•™ç¨‹ æ–‡ç« /ç½‘ç«™è®¿é—®æ¬¡æ•°ç»Ÿè®¡è®¿é—®æ¬¡æ•°ç»Ÿè®¡ä¹Ÿæœ‰å¾ˆå¤šæ–¹æ³•ï¼Œè¿™é‡Œåªç®€å•ä»‹ç»ä¸è’œå­è®¡æ•°æ–¹æ³•ã€‚å¾ˆç®€å•ï¼Œå°±ä¸‰æ­¥ï¼ å®‰è£…è„šæœ¬æ‰“å¼€D:\username.github.io\themes\typing\layout_partial\header.ejsåœ¨æœ€åŽåŠ å…¥ä¸‹é¢ä»£ç ï¼š 12&lt;script async src=&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; æ˜¾ç¤ºç«™ç‚¹æ€»è®¿é—®é‡è¦æ˜¾ç¤ºç«™ç‚¹æ€»è®¿é—®é‡ï¼Œå¤åˆ¶ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°ä½ éœ€è¦æ˜¾ç¤ºçš„ä½ç½®ã€‚æœ‰ä¸¤ç§ç®—æ³•å¯é€‰ï¼š(1)ç®—æ³•aï¼špvçš„æ–¹å¼ï¼Œå•ä¸ªç”¨æˆ·è¿žç»­ç‚¹å‡»nç¯‡æ–‡ç« ï¼Œè®°å½•næ¬¡è®¿é—®é‡ã€‚ 123&lt;span id=&quot;busuanzi_container_site_pv&quot;&gt; æœ¬ç«™æ€»è®¿é—®é‡&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;æ¬¡&lt;/span&gt; (2)ç®—æ³•bï¼šuvçš„æ–¹å¼ï¼Œå•ä¸ªç”¨æˆ·è¿žç»­ç‚¹å‡»nç¯‡æ–‡ç« ï¼Œåªè®°å½•1æ¬¡è®¿å®¢æ•°ã€‚123&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt; æœ¬ç«™è®¿å®¢æ•°&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;äººæ¬¡&lt;/span&gt; æ‰“å¼€themes/ä½ çš„ä¸»é¢˜/layout/_partial/ä½ å¯ä»¥é€‰æ‹©æ˜¾ç¤ºåœ¨ç½‘é¡µçš„å¤´éƒ¨header.ejsæ–‡ä»¶é‡Œï¼Œæ–‡ç« çš„article.ejsæ–‡ä»¶é‡Œï¼Œæˆ–è€…ç½‘é¡µçš„å°¾éƒ¨after-footer.ejsæ–‡ä»¶é‡Œï¼Œç­‰ç­‰ã€‚ æ˜¾ç¤ºå•é¡µé¢è®¿é—®é‡è¦æ˜¾ç¤ºæ¯ç¯‡æ–‡ç« çš„è®¿é—®é‡ï¼Œå¤åˆ¶ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°ä½ éœ€è¦æ˜¾ç¤ºçš„ä½ç½®ã€‚ä¸Žä¸Šé¢åŒç†ã€‚ç®—æ³•ï¼špvçš„æ–¹å¼ï¼Œå•ä¸ªç”¨æˆ·ç‚¹å‡»1ç¯‡æ–‡ç« ï¼Œæœ¬ç¯‡æ–‡ç« è®°å½•1æ¬¡é˜…è¯»é‡ã€‚123&lt;span id=&quot;busuanzi_container_page_pv&quot;&gt; æœ¬æ–‡æ€»é˜…è¯»é‡&lt;span id=&quot;busuanzi_value_page_pv&quot;&gt;&lt;/span&gt;æ¬¡&lt;/span&gt; ä»£ç ä¸­æ–‡å­—æ˜¯å¯ä»¥ä¿®æ”¹çš„ï¼Œåªè¦ä¿ç•™idæ­£ç¡®å³å¯ã€‚ ã€æç¤ºã€‘ï¼šä¿®æ”¹æ ‡é¢˜çš„æ–‡ç« ã€éš”å¤©å†ä¿®æ”¹å†…å®¹çš„æ–‡ç« ï¼Œgitä¼šæ ¹æ®æ—¥æœŸåšç‰ˆæœ¬æŽ§åˆ¶ã€‚æ¯ç¯‡æ–‡ç« çš„è®¿é—®åœ°å€ä¼šå› æ­¤æ›´æ”¹ã€‚æ‰€ä»¥ä¸ºäº†è®¿é—®æ•°å»ºè®®ä¸€æ¬¡æ€§å†™å®Œä¸è¦åšæ”¹åŠ¨äº†ã€‚ å¦‚æžœä½ çœ‹åˆ°è¿™é‡Œï¼Œæ­å–œä½ ï¼Œæ•™ç¨‹å·²ç»åˆ°æ­¤ç»“æŸå•¦~å¿«åŽ»è¯•è¯•å§ï¼ å¦å¤–ï¼Œæˆ‘æ˜¯å‰ç«¯é›¶åŸºç¡€å°ç™½ï¼Œæˆ‘çš„ä¸“ä¸šæ˜¯äººå·¥æ™ºèƒ½æœºå™¨å­¦ä¹ ç±»çš„ï¼Œè¿™ç½‘é¡µä¹Ÿåªæ˜¯æˆ‘çš„éšæ„å°è¯•ï¼Œè¿™äº›ä¸œè¥¿æœ‰æ—¶å€™è¿˜æ˜¯ä¼šå‡ºé”™ï¼Œæ¯•ç«Ÿåˆ†äº«å†…å®¹æ‰æ˜¯æˆ‘çš„åˆè¡·ï¼Œå…¶ä»–è¿˜æœ‰å¾ˆå¤šå¯ä»¥æ”¹è¿›çš„åœ°æ–¹ã€‚ä¹Ÿæ¬¢è¿Žå¤§å®¶çš„æ„è§å’ŒæŒ‡å¯¼åœ¨ä¸‹é¢æˆ–è€…åœ¨å¾®åšç»™æˆ‘ç•™è¨€ã€‚æˆ‘å‘çŽ°ä¸€ä¸ªå­¦ä¹ çš„å°çªé—¨å°±æ˜¯ï¼Œåœ¨ä½ å–œæ¬¢çš„ç½‘é¡µå³é”®é€‰æ‹©ã€æŸ¥çœ‹ç½‘é¡µæºä»£ç ã€‘ï¼Œå°±èƒ½å·å­¦äººå®¶çš„ä»£ç å•¦~å˜¿å˜¿ðŸ˜€ æœ‰ä¸€ä¸ªæˆ‘å¾ˆå–œæ¬¢çš„æ—…æ¸¸åšä¸»ï¼Œå¾®åšï¼šåŒ—äº¬å°é£Žå­ï¼Œå¥¹æåˆ°è¿‡å¿ƒç†å­¦ä¸Šæœ‰ä¸ªè¯å« positive reinforcementï¼ˆæ­£åŠ å¼ºï¼‰ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä¸æ–­å†™åˆ†äº«çš„åˆè¡·ã€‚å› ä¸ºå–œæ¬¢ï¼Œæ‰èƒ½åšæŒã€‚å¸Œæœ›å†™çš„ä¸œè¥¿å¯¹ä½ æœ‰å¸®åŠ©ï¼Œä¹ŸæœŸå¾…ä½ ä»¬çš„é¼“åŠ±æœŸå¾…ä½ ä»¬çš„æ‰“èµ~ æ„¿å¤§å®¶éƒ½çŽ©çš„å¼€å¿ƒâ¤]]></content>
      <tags>
        <tag>github</tag>
        <tag>hexo</tag>
        <tag>personal website</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¦‚ä½•æ­å»ºè‡ªå·±çš„ä¸ªäººç½‘ç«™ï¼ˆä¸Šï¼‰]]></title>
    <url>%2F2018%2F04%2F19%2F%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[æœ¬ç¯‡çš„ä¸ªäººç½‘ç«™æ­å»ºæ•™ç¨‹åŸºäºŽ GitHub + Hexoã€‚é€‚å®œäººç¾¤ï¼šæƒ³è¦æœ‰è‡ªå·±è¯´è¯çš„åœ°æ–¹ï¼Œæ²¡äººå¹²æ¶‰ã€‚ç³»ç»ŸçŽ¯å¢ƒï¼šwin10 æ­å»ºæ­£æ–‡ï¼š 1. å‡†å¤‡è½¯ä»¶çš„å®‰è£… Node.js Git 2. æ³¨å†Œgithub ç‚¹å‡»ðŸ‘‰https://github.comå³ä¸Šè§’sign upä¸ªäººç½‘ç«™çš„ç½‘å€æ˜¯å›ºå®šæ ¼å¼ï¼š username.github.ioè¿™ä¸ªusernameå°±æ˜¯ä½ çš„githubç”¨æˆ·åã€‚å½“ç„¶ä¹Ÿå¯ä»¥è‡ªå·±ä¹°åŸŸåå•¦ã€‚ æˆ‘çš„GitHubè´¦å·ï¼šYZHANG1270ä¸ªäººç½‘ç«™ï¼šyzhang1270.github.ioä½†æ˜¯æˆ‘ç»‘å®šåŸŸåå•¦~æœ‰ä¸ªæ›´é…·ç‚«çš„ç½‘å€ï¼šcodewithzhangyi.comï¼Œå…·ä½“å¦‚ä½•ç»‘å®šå°†åœ¨åŽç»­ç¯‡ä»‹ç»ã€‚ 3. åˆ›å»ºRepositoryç™»é™†GitHubï¼Œç‚¹å‡»å³ä¸Šè§’çš„ +å·ï¼Œé€‰æ‹©New repository åˆ›å»ºä¸€ä¸ªä¸Žä½ çš„åšå®¢ç›¸å…³çš„Repositoryé¡¹ç›®è¿›è¡Œç®¡ç†ï¼Œä¹‹åŽæ‰€æœ‰ä½ åšå®¢çš„åŠ¨æ€éƒ½ä¼šåœ¨è¿™Repositoryæ›´æ–°ã€‚Repositoryçš„åå­—æ˜¯username.github.ioï¼Œæ¯”å¦‚æˆ‘çš„yzhang1270.github.ioå·²ç»åˆ›å»ºã€‚å…¶ä½™å¯ä»¥å…ˆä¸å¡«ï¼Œç‚¹å‡»Create repository 4. é…ç½®å’Œä½¿ç”¨Githubå¼€å§‹â€”æ‰€æœ‰åº”ç”¨â€”æ‰¾åˆ°git bash 5. é…ç½®SSH KeysSSH Keysç”¨æ¥ä½¿æœ¬åœ°gité¡¹ç›®ä¸ŽGitHubè”ç³»ï¼Œè¿™æ ·èƒ½åœ¨GitHubä¸Šçš„åšå®¢é¡¹ç›®æ˜¯æœ€æ–°æ›´æ–°çš„ã€‚ æ£€æŸ¥SSH Keysçš„è®¾ç½®é¦–å…ˆæ£€æŸ¥è‡ªå·±ç”µè„‘ä¸ŠçŽ°æœ‰çš„SSH Keyï¼š 1$ cd ~/.ssh å¦‚æžœæ˜¾ç¤º No such file or directoryï¼Œè¯´æ˜Žè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡ç”¨git ç”Ÿæˆæ–°çš„SSH Key: 123$ ssh-keygen -t rsa -C &quot;é‚®ä»¶åœ°å€@youremail.com&quot;Generating public/private rsa key pair.Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa):&lt;å›žè½¦å°±å¥½&gt; è¿™é‡Œçš„é‚®ç®±åœ°å€ï¼Œè¾“å…¥æ³¨å†Œ Github çš„é‚®ç®±åœ°å€ç„¶åŽç³»ç»Ÿä¼šè¦ä½ è¾“å…¥å¯†ç ï¼š12Enter passphrase (empty for no passphrase):&lt;è®¾ç½®å¯†ç &gt;Enter same passphrase again:&lt;å†æ¬¡è¾“å…¥å¯†ç &gt; å†å›žè½¦ï¼Œè¿™é‡Œä¼šæç¤ºä½ è¾“å…¥ä¸€ä¸ªå¯†ç ï¼Œä½œä¸ºä½ æäº¤é¡¹ç›®æ—¶ä½¿ç”¨ã€‚è¿™ä¸ªå¯†ç çš„ä½œç”¨å°±æ˜¯åœ¨ä¸ªäººç½‘ç«™é‡Œæ‰€æœ‰çš„æ”¹åŠ¨åªèƒ½ç»è¿‡ä½ çš„æ‰‹ï¼Œä¹Ÿå¯ä»¥ä¸è®¾ç½®å¯†ç ï¼Œç›´æŽ¥ä¸ºç©ºã€‚æ³¨æ„ï¼šè¾“å…¥å¯†ç çš„æ—¶å€™æ²¡æœ‰è¾“å…¥ç—•è¿¹çš„ï¼Œä¸è¦ä»¥ä¸ºä»€ä¹ˆä¹Ÿæ²¡æœ‰è¾“å…¥ã€‚æœ€åŽçœ‹åˆ°è¿™æ ·çš„ç•Œé¢ï¼Œå°±æˆåŠŸè®¾ç½®ssh keyäº†ï¼š æ·»åŠ SSH Keyåˆ°GitHubä¸Šåœ¨æœ¬åœ°æ–‡ä»¶å¤¹æ‰¾åˆ°id_rsa.pubæ–‡ä»¶ï¼Œçœ‹ä¸Šé¢çš„å›¾ç‰‡ç¬¬å››è¡Œçš„ä½ç½®å‘Šè¯‰ä½ å­˜åœ¨å“ªé‡Œäº†æ²¡æ‰¾åˆ°çš„å‹¾é€‰ä¸€ä¸‹æ–‡ä»¶æ‰©å±•å éšè—çš„é¡¹ç›®.sshæ–‡ä»¶å¤¹é‡Œè®°äº‹æœ¬æ‰“å¼€è¿™ä¸ªæ–‡ä»¶å¤åˆ¶å…¨éƒ¨å†…å®¹åˆ°githubç›¸åº”ä½ç½®å›žåˆ°ä½ çš„GitHubä¸»é¡µï¼Œå³ä¸Šè§’ç‚¹å‡»å¤´åƒé€‰ä¸­Settingç»§ç»­é€‰ä¸­å·¦è¾¹èœå•æ çš„SSH and GPG keysTitleæœ€å¥½å†™ï¼Œéšä¾¿å†™ã€‚ç½‘ä¸Šæœ‰è¯´ä¸å†™titleä¹Ÿæœ‰å¯èƒ½åŽæœŸå‡ºçŽ°ä¹±ä¸ƒå…«ç³Ÿçš„é”™è¯¯Keyéƒ¨åˆ†å°±æ˜¯æ”¾åˆšæ‰å¤åˆ¶çš„å†…å®¹äº†ï¼Œç‚¹å‡»Add SSH key 6. æµ‹è¯•å›žåˆ°git bash æ¡†é‡Œè¾“å…¥ä»¥ä¸‹ä»£ç ï¼Œä¸è¦æ”¹ä»»ä½•ä¸€ä¸ªå­—ã€‚1$ ssh -T git@github.com å›žè½¦ï¼Œçœ‹åˆ°å¦‚ä¸‹ï¼š123The authenticity of host &apos;GitHub.com (207.97.227.239)&apos; can&apos;t be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no) è¾“å…¥yeså›žè½¦1Enter passphrase for key &apos;/c/Users/Yi/.ssh/id_rsa&apos;: è¾“å…¥åˆšæ‰è®¾ç½®çš„å¯†ç å›žè½¦ï¼Œçœ‹åˆ°â€œYouâ€™ve successfully authenticatedâ€¦â€æˆåŠŸï¼ä¸‹ä¸€æ­¥ï¼ 7. è®¾ç½®ç”¨æˆ·ä¿¡æ¯çŽ°åœ¨å·²ç»å¯ä»¥é€šè¿‡ SSH é“¾æŽ¥åˆ° GitHub å•¦!å½“ç„¶è¿˜éœ€è¦å®Œå–„ä¸€äº›ä¸ªäººä¿¡æ¯:12$ git config --global user.name &quot;yzhang1270&quot; //è¾“å…¥æ³¨å†Œæ—¶çš„username$ git config --global user.email &quot;yzhang1270@gmail.com&quot; //å¡«å†™æ³¨å†Œé‚®ç®± GitHub ä¹Ÿæ˜¯ç”¨è¿™äº›ä¿¡æ¯æ¥åšæƒé™çš„å¤„ç†ï¼Œè¾“å…¥ä¸‹é¢çš„ä»£ç è¿›è¡Œä¸ªäººä¿¡æ¯çš„è®¾ç½®ï¼ŒæŠŠåç§°å’Œé‚®ç®±æ›¿æ¢æˆä½ è‡ªå·±çš„ã€‚åˆ°æ­¤ï¼ŒSSH Keyé…ç½®æˆåŠŸå•¦ï¼ðŸ˜€æœ¬æœºæœ¬æœºå·²æˆåŠŸè¿žæŽ¥åˆ° githubã€‚ å¦‚æœ‰é—®é¢˜ï¼Œè¯·é‡æ–°è®¾ç½®ã€‚å¸¸è§é”™è¯¯è¯·å‚è€ƒï¼šConnecting to GitHub with SSHError: Permission denied 9. æ­å»ºHexoåšå®¢åˆ©ç”¨npmå‘½ä»¤å®‰è£…hexo12$ cd$ npm install -g hexo åˆ›å»ºç‹¬ç«‹åšå®¢é¡¹ç›®æ–‡ä»¶å¤¹å®‰è£…å®ŒæˆåŽï¼Œå…³æŽ‰ä¹‹å‰çš„Git Bashçª—å£ã€‚åœ¨æœ¬åœ°åˆ›å»ºä¸€ä¸ªä¸Ž Repositoryä¸­åšå®¢é¡¹ç›®åŒåçš„æ–‡ä»¶å¤¹username.github.io(å¦‚D:/yzhang1270.github.io)åœ¨æ–‡ä»¶å¤¹ä¸Šç‚¹å‡»é¼ æ ‡å³é”®ï¼Œé€‰æ‹© Git bash here(æžçš„æˆ‘çŽ°åœ¨æ¯æ¬¡è¦å†™æ–‡ç« çš„æ—¶å€™è„‘å­é‡Œå†’å‡ºçš„ç¬¬ä¸€å¥è¯æ°¸è¿œæ˜¯Bash Here!) ã€æç¤ºã€‘åœ¨è¿›è¡Œåšå®¢æ­å»ºå·¥ä½œæ—¶ï¼Œæ¯æ¬¡ä½¿ç”¨å‘½ä»¤éƒ½è¦åœ¨D:/yzhang1270.github.ioç›®å½•ä¸‹ã€‚ æ‰§è¡Œä¸‹é¢çš„æŒ‡ä»¤ï¼ŒHexo å°±ä¼šè‡ªåŠ¨åœ¨ D:/yzhang1270.github.io æ–‡ä»¶å¤¹å»ºç«‹ç‹¬ç«‹åšå®¢æ‰€éœ€è¦çš„æ‰€æœ‰æ–‡ä»¶å•¦ï¼1$ hexo init å®‰è£…ä¾èµ–åŒ… 1$ npm install ç¡®ä¿gitéƒ¨ç½² 1$ npm install hexo-deployer-git --save æœ¬åœ°æŸ¥çœ‹æ­å–œä½ ï¼ðŸ‘çŽ°åœ¨å·²ç»æ­å»ºå¥½æœ¬åœ°çš„ Hexo åšå®¢äº†ï¼Œæ‰§è¡Œå®Œä¸‹é¢çš„å‘½ä»¤å°±å¯ä»¥åˆ°æµè§ˆå™¨è¾“å…¥ localhost:4000 æŸ¥çœ‹åˆ°å•¦ï¼ 12$ hexo g$ hexo s hexo g æ¯æ¬¡è¿›è¡Œç›¸åº”æ”¹åŠ¨éƒ½è¦hexo g ç”Ÿæˆä¸€ä¸‹hexo s å¯åŠ¨æœåŠ¡é¢„è§ˆ ç”¨Hexoå…‹éš†ä¸»é¢˜æ‰§è¡Œå®Œ hexo init å‘½ä»¤åŽä¼šç»™ä¸€ä¸ªé»˜è®¤çš„ä¸»é¢˜ï¼šlandscapeé‡Œé¢è¿˜æœ‰ä¸€ç¯‡å†™å¥½çš„ç¤ºä¾‹æ–‡ç« ï¼šHello World ä½ ä¹Ÿå¯ä»¥åˆ°å®˜ç½‘ä½ å–œæ¬¢çš„ä¸»é¢˜è¿›è¡Œä¸‹è½½:hexo themesçŸ¥ä¹Žï¼šæœ‰å“ªäº›å¥½çœ‹çš„ Hexo ä¸»é¢˜ï¼Ÿ æ‰¾åˆ°ä¹‹åŽé€šè¿‡gitå‘½ä»¤ä¸‹è½½ç•Œé¢å³ä¾§ï¼Œåœ¨ä¸»é¢˜çš„repositoryç‚¹å‡»clone å¤åˆ¶ä¸€ä¸‹é‚£ä¸ªåœ°å€1$ git clone +å¤åˆ¶çš„åœ°å€+themes/typing åŽé¢å°±æ˜¯cloneä¹‹åŽæ”¾åˆ°ä½ æœ¬åœ°çš„åšå®¢æ–‡ä»¶å¤¹themesæ–‡ä»¶å¤¹ä¸‹åŽé¢è¿˜å¯ä»¥å°†è‡ªå·±åšå®¢ä¸ªæ€§åŒ–è£…é¥°~ ä¿®æ”¹æ•´ç«™é…ç½®æ–‡ä»¶è‡ªå·±æŠŠblog.ioä¸­æ–‡ä»¶éƒ½ç‚¹å¼€çœ‹ä¸€éï¼Œä¸»è¦é…ç½®æ–‡ä»¶æ˜¯ _config.ymlï¼ŒæŽ¨èä½¿ç”¨ nodepad++ æ‰“å¼€ã€‚ ä¿®è®¢æ¸…å•å¦‚ä¸‹ï¼Œæ–‡æ¡£å†…æœ‰è¯¦ç»†æ³¨é‡Šï¼Œå¯æŒ‰æ³¨é‡Šé€ä¸ªä¿®è®¢(1)åšå®¢åå­—åŠä½œè€…ä¿¡æ¯ï¼š_config.yml(2)ä¸ªäººä»‹ç»é¡µé¢ï¼šabout.md 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889è¿™é‡Œè´´ä¸€ä»½ç½‘ä¸Šçœ‹åˆ°çš„ å¯ä»¥å¤åˆ¶æ›¿æ¢åŽŸæ¥çš„ ä½†æ˜¯æ›¿æ¢ä¹‹å‰æœ€å¥½å¤‡ä»½ å¯èƒ½ä¼šå‡ºé”™é‚£è¦ä¹ˆä½ å°±å¯¹ç…§ç€çœ‹ä¸€ä¸‹æ”¹å°±å¥½# Hexo Configuration## Docs: http://zespia.tw/hexo/docs/configure.html## Source: https://github.com/tommy351/hexo/# Site è¿™é‡Œçš„é…ç½®ï¼Œå“ªé¡¹é…ç½®åæ˜ åœ¨å“ªé‡Œï¼Œå¯ä»¥å‚è€ƒæˆ‘çš„åšå®¢title: My Blog #åšå®¢åsubtitle: to be continued... #å‰¯æ ‡é¢˜description: My blog #ç»™æœç´¢å¼•æ“Žçœ‹çš„ï¼Œå¯¹ç½‘ç«™çš„æè¿°ï¼Œå¯ä»¥è‡ªå®šä¹‰author: Yourname #ä½œè€…ï¼Œåœ¨åšå®¢åº•éƒ¨å¯ä»¥çœ‹åˆ°email: yourname@yourmail.com #ä½ çš„è”ç³»é‚®ç®±language: zh-CN #ä¸­æ–‡ã€‚å¦‚æžœä¸å¡«åˆ™é»˜è®¤è‹±æ–‡# URL #è¿™é¡¹æš‚ä¸é…ç½®ï¼Œç»‘å®šåŸŸååŽï¼Œæ¬²åˆ›å»ºsitemap.xmléœ€è¦é…ç½®è¯¥é¡¹## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/tag_dir: tagsarchive_dir: archivescategory_dir: categories# Writing æ–‡ç« å¸ƒå±€ã€å†™ä½œæ ¼å¼çš„å®šä¹‰ï¼Œä¸ä¿®æ”¹new_post_name: :title.md # File name of new postsdefault_layout: postauto_spacing: false # Add spaces between asian characters and western characterstitlecase: false # Transform title into titlecasemax_open_file: 100filename_case: 0highlight: enable: true backtick_code_block: true line_number: true tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Archives é»˜è®¤å€¼ä¸º2ï¼Œè¿™é‡Œéƒ½ä¿®æ”¹ä¸º1ï¼Œç›¸åº”é¡µé¢å°±åªä¼šåˆ—å‡ºæ ‡é¢˜ï¼Œè€Œéžå…¨æ–‡## 2: Enable pagination## 1: Disable pagination## 0: Fully Disablearchive: 1category: 1tag: 1# Server ä¸ä¿®æ”¹## Hexo uses Connect as a server## You can customize the logger format as defined in## http://www.senchalabs.org/connect/logger.htmlport: 4000logger: falselogger_format:# Date / Time format æ—¥æœŸæ ¼å¼ï¼Œå¯ä»¥ä¿®æ”¹æˆè‡ªå·±å–œæ¬¢çš„æ ¼å¼## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-M-Dtime_format: H:mm:ss# Pagination æ¯é¡µæ˜¾ç¤ºæ–‡ç« æ•°ï¼Œå¯ä»¥è‡ªå®šä¹‰ï¼Œè´´ä¸»è®¾ç½®çš„æ˜¯10## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Disqus Disqusæ’ä»¶ï¼Œæˆ‘ä»¬ä¼šæ›¿æ¢æˆâ€œå¤šè¯´â€ï¼Œä¸ä¿®æ”¹disqus_shortname:# Extensions è¿™é‡Œé…ç½®ç«™ç‚¹æ‰€ç”¨ä¸»é¢˜å’Œæ’ä»¶ï¼Œæš‚æ—¶é»˜è®¤## Plugins: https://github.com/tommy351/hexo/wiki/Plugins## Themes: https://github.com/tommy351/hexo/wiki/Themestheme: landscapeexclude_generator:plugins:- hexo-generator-feed- hexo-generator-sitemap# Deployment ç«™ç‚¹éƒ¨ç½²åˆ°githubè¦é…ç½®## Docs: http://zespia.tw/hexo/docs/deploy.htmldeploy: type: git repository: branch: master å¯ç”¨æ–°ä¸‹è½½çš„ä¸»é¢˜åœ¨åˆšæ‰“å¼€çš„çš„_config.yml æ–‡ä»¶ä¸­ï¼Œæ‰¾åˆ°â€œ# Extensionsâ€ï¼ŒæŠŠé»˜è®¤ä¸»é¢˜ landscape ä¿®æ”¹ä¸ºåˆšåˆšä¸‹è½½ä¸‹æ¥çš„ä¸»é¢˜åï¼š ã€æç¤ºã€‘username.github.io é‡Œæœ‰ä¸¤ä¸ª config.yml æ–‡ä»¶ï¼Œä¸€ä¸ªåœ¨æ ¹ç›®å½•ï¼Œä¸€ä¸ªåœ¨ theme ä¸‹ï¼ŒçŽ°åœ¨ä¿®æ”¹çš„æ˜¯åœ¨æ ¹ç›®å½•ä¸‹çš„ã€‚ æ›´æ–°ä¸»é¢˜git bash é‡Œæ‰§è¡Œ 12$ cd themes/ä¸»é¢˜å$ git pull æœ¬åœ°æŸ¥çœ‹è°ƒè¯•æ¯æ¬¡ä¿®æ”¹éƒ½è¦hexo g ç”Ÿæˆä¸€ä¸‹ 12$ hexo g #ç”Ÿæˆ$ hexo s #å¯åŠ¨æœ¬åœ°æœåŠ¡ï¼Œè¿›è¡Œæ–‡ç« é¢„è§ˆè°ƒè¯•ï¼Œé€€å‡ºæœåŠ¡ç”¨Ctrl+c æµè§ˆå™¨è¾“å…¥ localhostï¼š4000 é¢„è§ˆæ•ˆæžœ 10. å°†åšå®¢éƒ¨ç½²åˆ°username.github.io å¤åˆ¶SSHç è¿›å…¥ Github ä¸ªäººä¸»é¡µä¸­çš„ Repositoryï¼Œå¤åˆ¶æ–°å»ºçš„ç‹¬ç«‹åšå®¢é¡¹ç›®username.github.ioçš„ SSHç  ç¼–è¾‘æ•´ç«™é…ç½®æ–‡ä»¶æ‰“å¼€ D:/username.github.io/_config.yml,æŠŠåˆšåˆšå¤åˆ¶çš„ SSHç ç²˜è´´åˆ°repositoryï¼šåŽé¢ï¼Œåˆ«å¿˜äº†å†’å·åŽè¦ç©ºä¸€æ ¼ã€‚ 1234deploy: type: git repository: git@github.com:username/username.github.io.git branch: master æ‰§è¡Œä¸‹åˆ—æŒ‡ä»¤å³å¯å®Œæˆéƒ¨ç½²ã€æç¤ºã€‘æ¯æ¬¡ä¿®æ”¹æœ¬åœ°æ–‡ä»¶åŽï¼Œéœ€è¦ hexo g æ‰èƒ½ä¿å­˜ã€‚æ¯æ¬¡ä½¿ç”¨å‘½ä»¤æ—¶ï¼Œéƒ½è¦åœ¨ä½ çš„åšå®¢æ–‡ä»¶å¤¹ç›®å½•ä¸‹ï¼šåœ¨D:/username.github.io/ å³é”®æ‰“å¼€ Git Bash Here 1234# é»„é‡‘ä¸‰å‘½ä»¤$ hexo g //(g = generate ä¿®æ”¹ç”Ÿäº§)$ hexo s //(s = server ä¿®æ”¹é¢„è§ˆ)$ hexo d //(d = deploy ä¿®æ”¹éƒ¨ç½²) ã€æç¤ºã€‘å¦‚æžœåœ¨é…ç½® SSH key æ—¶è®¾ç½®äº†å¯†ç ï¼Œæ‰§è¡Œ hexo d å‘½ä»¤ä¸Šä¼ æ–‡ä»¶æ—¶éœ€è¦è¾“å…¥å¯†ç è¿›è¡Œç¡®è®¤ï¼Œä¼šå‡ºçŽ°ä¸€ä¸ªå°æ¡†æ¡†ã€‚è¾“å…¥å¯†ç ä¹‹åŽåœ¨æµè§ˆå™¨è¾“å…¥ï¼šusername.github.io SurpriseðŸŽ‰ï¼æ­å–œä½ ~ä½ å·²ç»æ‹¥æœ‰ä¸€ä¸ªå±žäºŽä½ è‡ªå·±çš„ä¸ªäººç½‘ç«™å•¦~å˜¿å˜¿ 11. å†™åšå®¢å•¦ï¼å†…æ¶µæ‰æ˜¯é‡ç‚¹ï¼åœ¨D:\username.github.io\source_postsçš„ç©ºç™½å¤„å³é”®Git Bash Here1hexo new &apos;article&apos; æ­¤æ—¶å·²ç»åœ¨D:\username.github.io\source_postsç›®å½•ä¸‹æœ‰ä¸€ä¸ª article.mdçš„Markdownæ–‡ä»¶Hexoçš„åšå®¢éƒ½æ˜¯ç”¨Markdownå†™çš„ã€‚æˆ‘å°±éšä¾¿å†™äº†ç‚¹è¯•è¯•æˆ‘çš„æ–°åšå®¢å•¦~~ å†™åšæ–‡å‚è€ƒï¼šå¦‚ä½•å†™ä¸€ç¯‡hexoåšå®¢ æ­å»ºå‚è€ƒï¼š è¶…è¯¦ç»†Hexo+Githubåšå®¢æ­å»ºå°ç™½æ•™ç¨‹ å¦‚ä½•æ­å»ºä¸€ä¸ªç‹¬ç«‹åšå®¢â€”â€”ç®€æ˜ŽGithub Pagesä¸ŽHexoæ•™ç¨‹ æŠ€æœ¯å°ç™½æ­å»ºä¸ªäººåšå®¢ github+hexo å…¶ä»–å‚è€ƒï¼š ä¸ºä»€ä¹ˆä½ åº”è¯¥å†™åšå®¢ ä¸ºä»€ä¹ˆè¦è‡ªå»ºåšå®¢ å¦‚æžœæ­å»ºæˆåŠŸäº†~æ¬¢è¿Žæ‰“èµå“ˆå“ˆå“ˆ~ðŸ¤‘æˆ‘çš„æ–‡ç« å°†æŒç»­æ›´æ–°åœ¨æˆ‘çš„ codewithzhangyi.com ( = yzhang1270.github.io) é‡Œä»»ä½•ç–‘é—®è¯·åœ¨ä¸‹æ–¹ç•™è¨€ï¼Œä¹Ÿå°†åœ¨ä¸‹ä¸€æœŸæ•™å¦‚ä½•åˆ¶ä½œç•™è¨€æ¿~æ•¬è¯·æœŸå¾…~â¤]]></content>
      <tags>
        <tag>github</tag>
        <tag>hexo</tag>
        <tag>personal website</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[åŸºäºŽRçš„ä¿¡ç”¨è¯„çº§/è¯„åˆ†å¡æ¨¡åž‹åˆ¶ä½œæ•™ç¨‹]]></title>
    <url>%2F2018%2F04%2F16%2F%E5%9F%BA%E4%BA%8ER%E7%9A%84%E4%BF%A1%E7%94%A8%E8%AF%84%E7%BA%A7-%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B%E5%88%B6%E4%BD%9C%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[æ³¨ï¼š æœ¬ç¯‡æ˜¯æœºå™¨å­¦ä¹ /æ•°æ®æŒ–æŽ˜åœ¨äº’è”ç½‘é‡‘èžè¡Œä¸šçš„åº”ç”¨ï¼Œåªæ˜¯ä¸€ä¸ªæ¨¡åž‹å»ºç«‹çš„æµç¨‹ä»‹ç»ï¼Œä¸æ¶‰åŠè¯¦ç»†çš„æ•°æ®æ¸…æ´—é€»è¾‘ï¼Œä¸æ¶‰åŠæ¨¡åž‹çš„è°ƒä¼˜ã€‚ æœ¬ç¯‡ä½ éœ€è¦çŸ¥é“çš„ï¼šé€»è¾‘å›žå½’ã€WOEã€IVå€¼ã€ROCã€KSå€¼ã€‚ é€‚ç”¨äººç¾¤ï¼šæƒ³å…¥é—¨æˆ–è€…æƒ³è½¬åž‹æˆäº’é‡‘é£ŽæŽ§å»ºæ¨¡çš„æœ‹å‹ã€‚ å¦‚æœ‰ä»»ä½•ç–‘é—®æˆ–å»ºè®®è¯·åœ¨ä¸‹é¢ç•™è¨€æˆ–è€…è”ç³»æˆ‘ðŸ˜Ž èƒŒæ™¯ï¼šåœ¨é“¶è¡Œä¸šæ‚ ä¹…çš„åŽ†å²ä¸­ï¼Œä¿¡ç”¨è¯„åˆ†å¡(ScoreCard)æ¨¡åž‹å¹¿æ³›ä½¿ç”¨ï¼Œæ¥åˆ¤åˆ«è´·æ¬¾ç”³è¯·è€…çš„é€¾æœŸæ¦‚çŽ‡ã€‚çŽ°åœ¨æˆä¸ºäº’è”ç½‘é‡‘èžè¡Œä¸šæœ€ç«çˆ†çš„æœ€æ ¸å¿ƒçš„é£ŽæŽ§æ¨¡åž‹ã€‚ æ¨¡åž‹ç±»åˆ«ï¼šç”³è¯·å¡æ¨¡åž‹ = Aå¡(Application Card)ï¼Œåœºæ™¯ï¼šè´·å‰è¡Œä¸ºå¡æ¨¡åž‹ = Bå¡(Behaviour Card)ï¼Œ åœºæ™¯ï¼šè´·ä¸­å‚¬æ”¶å¡æ¨¡åž‹ = Cå¡(Collection Card)ï¼Œ åœºæ™¯ï¼šè´·åŽåæ¬ºè¯ˆæ¨¡åž‹ = Få¡(Anti-Fraud Card)ï¼Œ åœºæ™¯ï¼šåæ¬ºè¯ˆ ç‰¹å¾ç»´åº¦ï¼šAå¡ï¼šç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ + è‡ªæœ‰appæ“ä½œè¡Œä¸ºæ•°æ® + ç¬¬ä¸‰æ–¹æ•°æ®Bå¡ï¼šç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ + è‡ªæœ‰appæ“ä½œè¡Œä¸ºæ•°æ® + ç¬¬ä¸‰æ–¹æ•°æ® + åŽ†å²è¿˜æ¬¾è¡Œä¸ºæ•°æ®Cå¡ï¼šç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ + è‡ªæœ‰appæ“ä½œè¡Œä¸ºæ•°æ® + ç¬¬ä¸‰æ–¹æ•°æ® + åŽ†å²è¿˜æ¬¾è¡Œä¸ºæ•°æ® + å‚¬æ¬¾è¡Œä¸ºæ•°æ® æœ¬è´¨ï¼šï¼ˆäºŒ/å¤šï¼‰åˆ†ç±»æ¨¡åž‹ ç”³è¯·å¡æ¨¡åž‹ä¸ºäº’é‡‘è¡Œä¸šæœ€é‡è¦ã€åº”ç”¨æœ€å¹¿æ³›çš„ä¸€å¼ å¡ï¼Œä»¥ä¸‹ä»‹ç»ä»¥Aå¡å±•å¼€ã€‚ æ­£æ–‡ï¼šè¯„åˆ†å¡(Aå¡)åˆ¶ä½œæµç¨‹ä¼ ç»Ÿè¯„åˆ†å¡ä½¿ç”¨çš„ç®—æ³•ï¼šé€»è¾‘å›žå½’(Logistics Regression)ä¼ ç»Ÿè¯„åˆ†å¡æž„å»ºæ­¥éª¤ï¼š æ ·æœ¬æ”¶é›†ã€æ•°æ®æ¸…æ´—ã€æ—¶çª—åˆ‡å‰² åˆ†ç®±ã€è®¡ç®—WOEå’ŒIVå€¼ã€WOEçš„æ€§è´¨ã€å˜é‡ç­›é€‰ã€å¾ªçŽ¯ä»¥ä¸Šæ­¥éª¤(å¦‚éœ€è¦) æž„å»ºé€»è¾‘å›žå½’æ¨¡åž‹ è¯„åˆ†å¡Scaling è¯„ä¼°ä¿¡ç”¨è¯„åˆ†å¡ é€‰æ‹©Cut-Offåˆ†æ•° 1. æ ·æœ¬æ”¶é›†ã€æ•°æ®æ¸…æ´—ã€æ—¶çª—åˆ‡å‰² æ ·æœ¬æ”¶é›†ï¼šå¯¼å…¥æ ·æœ¬æ•°æ® = data0 12345678910111213141516171819202122232425262728293031323334353637383940# åŒ…çš„ä¸‹è½½ä½¿ç”¨packages&lt;-c("ggplot2","dplyr","smbinning","data.table","woe","gmodels","ROCR","knitr","reshape2","Information","corrgram","corrplot","varhandle","ROCR","stringr","DT","partykit","tcltk","Daim","vcd","caret")UsePackages&lt;-function(p)&#123; if (!is.element(p,installed.packages()[,1]))&#123; install.packages(p)&#125; require(p,character.only = TRUE)&#125;for(p in packages)&#123; UsePackages(p)&#125;library(data.table)library(dplyr)library(ggplot2)library(reshape2)library(corrgram)library(corrplot)library(stats)library(smbinning)library(woe)library(gmodels)library(Information)library(knitr)library(varhandle)library(ROCR)library(stringr)library(DT)library(partykit)library(tcltk)library(Daim)library(vcd)library(caret)options(warn=-1)# æºæ•°æ® data0 åœ¨dataç›®å½•ä¸‹load("data/data0_LR.RData") # æ ¹æ®åŽ†å²é€¾æœŸå¤©æ•°overduedays å¢žåŠ yå˜é‡bad ï¼šé€¾æœŸè¶…è¿‡30å¤©ä¸ºåå®¢æˆ·ï¼Œå¦åˆ™å¥½å®¢æˆ·data0$bad = ifelse(data0$overduedays&gt;30, 1, 0) æ•°æ®æ¸…æ´—ï¼šæœ¬ç¯‡ä¸ºå‡é€ æ•°æ®ï¼Œåªä¸ºè·‘é€šç¨‹åºåšæ¼”ç¤ºï¼Œä¸é€‚åˆåšæ•°æ®æ¸…æ´—æ•™ç¨‹ï¼Œæ•…æ­¤æ­¥éª¤ç›´æŽ¥ç”¨æ¸…æ´—å¥½çš„æ•°æ®ã€‚ 1234567891011121314&gt; names(data0) # æ‰€æœ‰ç‰¹å¾å # [1] "extration_amount" "ApplyHour" "AGE_Value" # [4] "CALL_RECORD_FLAG_Value" "CONTACTS_RELATIVES_COUNT_Value" "DEGREE_Value" # [7] "IDENTIFICATION_RESULT_Value" "MARITAL_STATUS_Value" "MONTH_INCOME_Value" # [10] "POSITION_Value" "REJECT_COUNT_Value" "GENDER_Value_ID_CARD" # [13] "WORK_MONTH" "dt_7day" "dt_1month" # [16] "dt_3month" "FINAL_SCORE" "ZM_SCORE" # [19] "state" "bad" "ZM_SCORE_EXIST" # [22] "MISSING_COUNT" &gt; nrow(data0) # æ ·æœ¬æ•°é‡# [1] 23610&gt; ncol(data0) # ç‰¹å¾æ•°é‡# [1] 22 æ—¶çª—åˆ‡å‰²ï¼šä¿¡ç”¨è¯„ç­‰æœ€ä¸»è¦çš„åŠŸèƒ½ä¸ºé¢„æµ‹å®¢æˆ·æœªæ¥çš„è¿çº¦è¡Œä¸ºï¼Œå› æ­¤å¿…é¡»é’ˆå¯¹é¢„æµ‹æ—¶é—´ç‚¹è¿›è¡Œæ˜Žç¡®çš„å®šä¹‰ã€‚è¿™ä¸ªå°±æ˜¯æ—¶çª—åˆ‡å‰²(Time Windows)ã€‚æ—¶çª—çš„æ—¶é—´æ ¹æ®æ¯ä¸ªäº§å“çš„ä¸šåŠ¡é€»è¾‘ç¡®å®šã€‚åœ¨æ­¤æˆ‘å›žæº¯è¿‡åŽ»åŠå¹´æ•°æ®æ¥é¢„æµ‹æœªæ¥æ–°ç”¨æˆ·çš„é€¾æœŸæ¦‚çŽ‡ã€‚æŠ½æ ·æ—¶çª—(Sample Windows)ï¼šè¿›è¡Œé¢„æµ‹æ—¶ï¼Œå¿…é¡»å›žæº¯å¤šä¹…ä»¥å‰çš„å®¢æˆ·åŽ†å²è¡Œä¸ºæ•°æ®ã€‚è§‚å¯Ÿæ—¶çª—(Performance Windows)ï¼šè¿›è¡Œé¢„æµ‹æ—¶ï¼Œè¦é¢„ä¼°æœªæ¥å¤šä¹…å®¢æˆ·çš„è¡Œä¸ºç»“æžœã€‚ 2. åˆ†ç®±ã€è®¡ç®—WOEå’ŒIVå€¼ã€WOEçš„æ€§è´¨ã€å˜é‡ç­›é€‰ã€å¾ªçŽ¯ä»¥ä¸Šæ­¥éª¤(å¦‚éœ€è¦) åˆ†ç®±(Binning)ï¼šå¯¹è¿žç»­å˜é‡ç¦»æ•£åŒ–(Discretization),å¯¹ç¦»æ•£å˜é‡ä¹Ÿå¯è¿›è¡Œé‡æ–°åˆ†ç®±ã€ç»„åˆã€‚åˆ†ç®±æ–¹å¼ï¼šç­‰å®½åˆ†ç®±ã€ç­‰é¢‘åˆ†ç®±ã€æœ€ä¼˜åˆ†ç®±ç­‰ã€‚æœ¬æ–‡ä½¿ç”¨æœ€ä¼˜åˆ†ç®±ï¼ŒåŸºäºŽæœ€å°ç†µåŽŸåˆ™ã€‚ WOE(Weight of Evidence)å’ŒIV(Infomation Value)ï¼šé€»è¾‘å›žå½’æ˜¯çº¿æ€§çš„ç»Ÿè®¡æ¨¡å¼ï¼Œå› æ­¤é‡åˆ°éžçº¿æ€§è¶‹åŠ¿çš„å˜æ•°ä¼šé€ æˆæ— æ³•æœ‰æ•ˆçš„å»ºç«‹é¢„æµ‹æ¨¡åž‹,å› æ­¤éœ€è¦WOEã€‚è®¡ç®—é€»è¾‘ç‚¹å‡»è¿™é‡Œ WOE = ln(Odds) = ln(%Good/%Bad) = ln(p/(1-p))IV= âˆ‘(%Good-%Bad)*WOE = âˆ‘(%Good-%Bad)*ln(%Good/%Bad) ðŸŒWOEçš„æ€§è´¨(åˆ’é‡ç‚¹!)ï¼š (1) WOEä¸Žé£Žé™©æ­£ç›¸å…³ï¼ŒWOEè¶Šå¤§ï¼Œé£Žé™©è¶Šé«˜ï¼Œä»£è¡¨è¯¥å±‚çº§çš„å®¢æˆ·å“è´¨è¶Šå·®ã€‚å¦‚æžœWOEæŽ¥è¿‘ï¼ï¼Œè¡¨ç¤ºæŽ¥è¿‘å¹³å‡æ°´å¹³ã€‚ï¼ˆæ­£è´Ÿç›¸å…³å¯ä»¥è°ƒèŠ‚ï¼‰(2)è¿›è¡ŒWOEæ£€å®šæ—¶ï¼Œè§‚å¯ŸWOEåˆ†å¸ƒçš„å˜åŠ¨è¶‹åŠ¿æ˜¯å¦ç¬¦åˆé€»è¾‘(Logical Trend).æ‰€è°“Logical TrendæŒ‡çš„æ˜¯WOEå˜åŠ¨è¶‹åŠ¿å¿…é¡»å‘ˆçŽ°é€’å¢žã€é€’å‡ï¼Œæˆ–è€…æ˜¯å•çº¯è½¬æŠ˜æ¨¡å¼(uåž‹æˆ–nåž‹)ã€‚(3)å¦‚æžœWOEè¶‹åŠ¿å‘ˆçŽ°ä¸ç¨³å®šçš„é”¯é½¿çŠ¶æ³¢åŠ¨(Wåž‹æˆ–Måž‹)æˆ–è€…æ˜¯ä¸åŒæ—¶çª—å‘ˆçŽ°ä¸ä¸€è‡´çš„è¶‹åŠ¿ï¼Œæ­¤æ—¶å°±å¿…é¡»é€šè¿‡é‡æ–°åˆ†ç®±æ¥è°ƒæ•´ï¼Œå¦åˆ™å°±å¿…é¡»æ”¾å¼ƒæ­¤å˜é‡ã€‚(4)WOEä¸ä¼šå› ä¸ºæŠ½æ ·è¯¯å·®é€ æˆæ•°å€¼å¤§å¹…å˜åŒ–ã€‚è€Œä¸”WOEåˆ¶ä½œçš„è¯„åˆ†å¡å¯è§£é‡Šæ€§å¼ºï¼Œä¹Ÿæ˜¯è¿™å¥—è¯„åˆ†å¡æ°¸æµä¼ çš„ç²¾é«“ä¹‹ä¸€ã€‚ å˜é‡ç­›é€‰ï¼šæ ¹æ®æ¯ä¸ªå˜é‡çš„åˆ†ç®±ç»“æžœè®¡ç®—IVå€¼ï¼Œç•™ä¸‹IV&gt;0.1çš„å˜é‡ã€‚è¿™ä¸ª0.1çš„æ•°å€¼å¯ä»¥æ”¹å˜ã€‚123456# è®¡ç®—dataframeé‡Œæ‰€æœ‰ç‰¹å¾çš„IVå€¼IV &lt;- create_infotables(data=data0, y="bad",bins = 10, ncore = NULL, parallel=FALSE)# æ˜¾ç¤ºIVè®¡ç®—ç»“æžœ(Summary&lt;-IV$Summary) ç»˜åˆ¶æ¯ä¸ªå˜é‡çš„WOEåˆ†ç®±æŸ±çŠ¶å›¾12345678910111213141516171819202122232425# ç­›é€‰å˜é‡ï¼šç•™ä¸‹IV&gt;0.1çš„å˜é‡Summary=Summary%&gt;% filter(Summary$IV&gt;0.1)%&gt;% as.data.frame()(selected_names&lt;-Summary$Variable) # æ˜¾ç¤ºç­›é€‰åŽçš„å˜é‡å# [1] "ZM_SCORE" "IDENTIFICATION_RESULT_Value" # [3] "extration_amount" "CONTACTS_RELATIVES_COUNT_Value"# [5] "POSITION_Value" "ZM_SCORE_EXIST" num&lt;-length(selected_names) # ç­›é€‰åŽçš„å˜é‡ä¸ªæ•°# ç»˜åˆ¶æ¯ä¸ªå˜é‡çš„WOEåˆ†ç®±æŸ±çŠ¶å›¾names &lt;- selected_names # LOOP for ALL: names&lt;-names(IV$Tables)plots &lt;- list()IVtable&lt;- IV$Tablesfor (i in 1:length(selected_names))&#123; plots[[i]] &lt;- plot_infotables(IV, names[i],same_scales=FALSE,show_values = TRUE) IVtable[i]&lt;-IV$Tables$names[i]&#125;# Showing the variables whose iv &gt;0.1plots[1:length(selected_names)]# MultiPlot(IV, IV$Summary$Variable[1:num]) # ç»˜åˆ¶ç»¼åˆå›¾codeIVtable[selected_names] æ ¹æ®ä¸Šæ–‡æåˆ°çš„(Logical Trend)æ¥è§‚å¯Ÿä¸Šé¢6ä¸ªWOEåˆ†å¸ƒå›¾ï¼ŒZM_SCORE, IDENTIFICATION_RESULT_Value, CONTACTS_RELATIVES_COUNT_Value, POSITION_Value, ZM_SCORE_EXISTéƒ½ç¬¦åˆLogical Trendã€‚åªæœ‰extration_amountçš„WOEåˆ†å¸ƒå‘ˆçŽ°æ³¢æµªä¸è§„åˆ™åž‹ï¼Œéœ€è¦æ•´æ”¹ã€‚ ç›¸å…³æ€§åˆ†æž(CORRplot):è¿™é‡Œåªå…ˆç¤ºèŒƒåæ–¹å·®çŸ©é˜µå›¾1234567891011121314151617col1 &lt;- colorRampPalette(c("#7F0000","red","#FF7F00","yellow","white", "cyan", "#007FFF", "blue","#00007F"))col2 &lt;- colorRampPalette(c("#67001F", "#B2182B", "#D6604D", "#F4A582", "#FDDBC7", "#FFFFFF", "#D1E5F0", "#92C5DE", "#4393C3", "#2166AC", "#053061"))col3 &lt;- colorRampPalette(c("red", "white", "blue"))col4 &lt;- colorRampPalette(c("#7F0000","red","#FF7F00","yellow","#7FFF7F", "cyan", "#007FFF", "blue","#00007F"))wb &lt;- c("white","black")par(ask = TRUE)data0= data0%&gt;% select(selected_names,bad)%&gt;% as.data.frame()M=data0[complete.cases(data0),]M&lt;-cor(M)corrplot(M, method="color", col=col1(20), cl.length=21,order = "AOE",tl.cex = 0.6,addCoef.col="grey") åˆ åŽ»CONTACTS_RELATIVES_COUNT_Value å¾ªçŽ¯åˆ†ç®±æ­¥éª¤ï¼ˆåˆ†ç®±è°ƒæ•´ï¼‰(1)extration_amount1234567891011data0$extration_amount=as.numeric(data0$extration_amount)data_tmp=data0%&gt;% select(c(extration_amount,bad))%&gt;% apply(2,as.numeric)%&gt;% data.frame()IV &lt;- create_infotables(data_tmp, y='bad', ncore=2,bins=5) # binsçš„æ•°å€¼éšæ„å®šï¼Œä¸€èˆ¬2~10data0$extration_amount=cut(data0$extration_amount,breaks=c(-Inf,475,671,771,971,Inf),labels = IV$Tables$v$WOE[1:length(IV$Tables$extration_amount$WOE)])ggplot(IV$Tables$extration_amount,aes(x=extration_amount,y=WOE))+ geom_bar(stat='identity',fill='lightblue') 12345# WOEè®¡ç®—ç»“æžœä¿ç•™ï¼Œåœ¨æ­¥éª¤4-Scalingä¼šå†æ¬¡ç”¨åˆ°IV$Tables$extration_amount$WOE# [1] -0.4414730 -0.2142640 0.4596698 0.4386113 -0.3501923IV$Summary# 0.1327355 #IVå€¼ (2)POSITION_Value12345678910data0$POSITION_Value=as.numeric(data0$POSITION_Value)data_tmp=data0%&gt;% select(c(POSITION_Value,bad))%&gt;% apply(2,as.numeric)%&gt;% data.frame()IV &lt;- create_infotables(data_tmp, y='bad', ncore=2,bins=6)ggplot(IV$Tables$POSITION_Value,aes(x=POSITION_Value,y=WOE))+ geom_bar(stat='identity',fill='lightblue')data0$POSITION_Value=cut(data0$POSITION_Value,breaks=c(-Inf,0,1,5),labels = IV$Tables$POSITION_Value$WOE[1:length(IV$Tables$POSITION_Value$WOE)]) 12345# WOEè®¡ç®—ç»“æžœä¿ç•™ï¼Œåœ¨æ­¥éª¤4-Scalingä¼šå†æ¬¡ç”¨åˆ°IV$Tables$POSITION_Value$WOE# [1] 0.5817640 -0.2522849 -0.2905931IV$Summary# 0.1528563 (3)ZM_SCORE1234567891011data0$ZM_SCORE=as.numeric(data0$ZM_SCORE)data_tmp=data0%&gt;% select(c(ZM_SCORE,bad))%&gt;% apply(2,as.numeric)%&gt;% data.frame()IV &lt;- create_infotables(data_tmp, y='bad', ncore=2,bins=10)ggplot(IV$Tables$ZM_SCORE,aes(x=ZM_SCORE,y=WOE))+ geom_bar(stat='identity',fill='lightblue')data0$ZM_SCORE=cut(data0$ZM_SCORE,breaks=c(-Inf,549,569,592,609,635,Inf),labels = IV$Tables$ZM_SCORE$WOE[1:length(IV$Tables$ZM_SCORE$WOE)]) 123456# WOEè®¡ç®—ç»“æžœä¿ç•™ï¼Œåœ¨æ­¥éª¤4-Scalingä¼šå†æ¬¡ç”¨åˆ°IV$Tables$ZM_SCORE$WOE# [1] 0.40926664 0.30817452 -0.01635135# [4] -0.38743811 -0.74663108 -1.52210534IV$Summary# 0.2749328 (4)ZM_SCORE_EXIST1234567891011data0$ZM_SCORE_EXIST=as.numeric(data0$ZM_SCORE_EXIST)data_tmp=data0%&gt;% select(c(ZM_SCORE_EXIST,bad))%&gt;% apply(2,as.numeric)%&gt;% data.frame()IV &lt;- create_infotables(data_tmp, y='bad', ncore=2,bins=2)ggplot(IV$Tables$ZM_SCORE_EXIST,aes(x=ZM_SCORE_EXIST,y=WOE))+ geom_bar(stat='identity',fill='lightblue')data0$ZM_SCORE_EXIST=cut(data0$ZM_SCORE_EXIST,breaks=c(-Inf,0,1),labels = IV$Tables$ZM_SCORE_EXIST$WOE[1:length(IV$Tables$ZM_SCORE_EXIST$WOE)]) 12345# WOEè®¡ç®—ç»“æžœä¿ç•™ï¼Œåœ¨æ­¥éª¤4-Scalingä¼šå†æ¬¡ç”¨åˆ°IV$Tables$ZM_SCORE_EXIST$WOE# [1] 0.2976555 -0.3847163IV$Summary# 0.1134327 (5)IDENTIFICATION_RESULT_Value1234567891011data0$IDENTIFICATION_RESULT_Value=as.numeric(data0$IDENTIFICATION_RESULT_Value)data_tmp=data0%&gt;% select(c(IDENTIFICATION_RESULT_Value,bad))%&gt;% apply(2,as.numeric)%&gt;% data.frame()IV &lt;- create_infotables(data_tmp, y='bad', ncore=2,bins=5)ggplot(IV$Tables$IDENTIFICATION_RESULT_Value,aes(x=IDENTIFICATION_RESULT_Value,y=WOE))+ geom_bar(stat='identity',fill='lightblue')data0$IDENTIFICATION_RESULT_Value=cut(data0$IDENTIFICATION_RESULT_Value,breaks=c(-Inf,2,3,4),labels = IV$Tables$IDENTIFICATION_RESULT_Value$WOE[1:length(IV$Tables$IDENTIFICATION_RESULT_Value$WOE)]) 12345# WOEè®¡ç®—ç»“æžœä¿ç•™ï¼Œåœ¨æ­¥éª¤4-Scalingä¼šå†æ¬¡ç”¨åˆ°IV$Tables$IDENTIFICATION_RESULT_Value$WOE# [1] 0.6084594 -0.2568459 -0.4399638IV$Summary# 0.1897237 ä»¥ä¸Šçš„5ä¸ªå˜é‡çš„IV&gt;0.1,ä¸”WOEåˆ†å¸ƒå‘ˆLogical Trendï¼Œä¿å­˜æ•°æ®1data1 = data0 #å¤‡ä»½æ•°æ®ï¼Œä»¥ä¸‹éƒ½å¯¹data1è¿›è¡Œå¤„ç† 3. æž„å»ºé€»è¾‘å›žå½’æ¨¡åž‹1234567891011121314151617data1[, c(1:length(data1))] &lt;- sapply(data1[, c(1:length(data1))], as.numeric)cbind(apply(data1,2,function(x)length(unique(x))),sapply(data1,class))#æ‹†åˆ†è®­ç»ƒé›†ä¸Žæµ‹è¯•é›†ï¼Œå»ºæ¨¡#----------------------------------------------------------# train &amp; test(80%-20%) select randomly#----------------------------------------------------------nrow(data1)a = round(nrow(data1)*0.8)b = sample(nrow(data1), a, replace = FALSE, prob = NULL)data_train= data1[b,]data_test = data1[-b,]# é€»è¾‘å›žå½’å»ºæ¨¡m1=glm(bad~., data=data_train,binomial(link='logit'))summary(m1) 1234é€šè¿‡æ£€éªŒæˆªè·ä¸º0.72215å„ä¸ªç³»æ•°ä¸º-0.39815, -0.42831, 0.08642, -0.24813, 0.25519è¿™äº›å‚æ•°éƒ½ååˆ†é‡è¦ï¼Œåœ¨Scalingä¸­å†æ¬¡ç”¨åˆ° 123456anova(m1,test="Chisq") # ANOVA æ£€éªŒé€šè¿‡model=m1# yå€¼é¢„æµ‹yhat_train = fitted(model)yhat_test = predict(model,newdata=data_test,type='response') 4. è¯„åˆ†å¡Scaling å°†WOEå€¼è½¬æ¢ä¸ºä¿¡ç”¨é£Žé™©åˆ†æ•°12345678data2 = data1 #æ•°æ®å¤‡ä»½odds=sum(data2$bad==1)/sum(data2$bad==0)log(odds,base=exp(1))B=40/log(2,base=exp(1))A=200-B*log(odds,base=exp(1))score=yhat_test*B+A #Score=258.152490+57.707802*yhat 1234summary(score)# Min. 1st Qu. Median Mean 3rd Qu. Max. # 261 268 273 274 278 295 #è¿™æ‰¹å®¢æˆ·çš„ä¿¡ç”¨é£Žé™©åˆ†å€¼å·²ç»å¾—å‡º å¥½/åå®¢æˆ·åˆ†æ•°åˆ†å¸ƒåˆ†æ•°è¶Šé«˜ï¼Œå®¢æˆ·çš„é€¾æœŸé£Žé™©è¶Šé«˜ï¼Œå› æ­¤åå®¢æˆ·åº”è¯¥é›†ä¸­åœ¨åˆ†æ•°åé«˜åŒºåŸŸï¼Œåä¹‹ï¼Œå¥½å®¢æˆ·åº”è¯¥é›†ä¸­åœ¨é£Žé™©åˆ†æ•°ä½Žåˆ†åŒºåŸŸã€‚123456index=which(data_test$bad==1)m=seq(260,300,by=5) #260ï¼Œ300æ˜¯æ ¹æ®åˆ†æ•°å€¼åŸŸå®šçš„ï¼Œ5ä¸ºé—´éš”æ•°å€¼bad=cut(score[index],m)%&gt;%table%&gt;%data.framecolnames(bad)=c('level','count')ggplot(data = bad,aes(x =level,y=count)) + geom_bar(stat = 'identity') 1234index=which(data_test$bad==0)good=cut(score[index],m)%&gt;%table%&gt;%data.framecolnames(good)=c('level','count')ggplot(data = good,aes(x =level,y=count)) + geom_bar(stat = 'identity') 5. è¯„ä¼°ä¿¡ç”¨è¯„åˆ†å¡ KSæ£€éªŒï¼šæ¨¡åž‹åŒºåˆ†å¥½åå®¢æˆ·çš„åŠ›åº¦KS&gt;0.3æ—¶ï¼Œæ¨¡åž‹æ‰èƒ½ç”¨ã€‚ ROCæ£€éªŒï¼šæ¨¡åž‹åˆ¤åˆ«çœŸå‡çš„å‡†ç¡®åº¦AUC&gt;0.7æ—¶ï¼Œæ¨¡åž‹æ‰èƒ½ç”¨ã€‚æ­¤æ¨¡åž‹å› ä¸ºæ•°æ®è´¨é‡å’Œåˆ†ç®±è´¨é‡ï¼Œæ‰€ä»¥ä¸å…·æœ‰å‚è€ƒæ€§ï¼Œä»…åšè·‘é€šæ¨¡åž‹ä¹‹ç”¨ï¼Œå…·ä½“çš„æ¨¡åž‹è°ƒä¼˜å°†å¦å¤–å†™ã€‚ æˆ‘çš„å¤©çœŸçš„æ˜¯ä¸çŸ¥ä¸è§‰å†™é‚£ä¹ˆé•¿ï¼Œå…¶å®žè¿˜æœ‰å¾ˆå¤šè¿˜æ²¡è¯´ï¼ŒåŽç»­ä¼šæ…¢æ…¢å†™åŽç»­çš„ã€‚é‚£ä¹ˆæ­å–œä½ ï¼Œåˆ°æ­¤ï¼Œä½ çš„è¯„åˆ†å¡å·²ç»åšå®Œå•¦~~æ¯ä¸ªå®¢æˆ·åªè¦å¡«å†™æ ¹æ®ä½ ç­›é€‰å‡ºæ¥çš„å˜é‡çš„ç›¸å…³ä¿¡æ¯ï¼Œå°±èƒ½å¾—åˆ°æ¯ä¸ªäººä¸“å±žçš„ä¿¡ç”¨é£Žé™©åˆ†å•¦ï¼ðŸ–– 6. é€‰æ‹©Cut-Offåˆ†æ•°æ¨¡åž‹åšå®Œï¼Œä¸‹ä¸€æ­¥å°±æ˜¯è¦è·Ÿä¸šåŠ¡ç»“åˆå•¦~æ¨¡åž‹çš„ä½œç”¨å°±æ˜¯è¯„ä¼°è´·æ¬¾ç”³è¯·è€…çš„æœªæ¥é€¾æœŸæ¦‚çŽ‡ã€‚é£Žé™©é«˜çš„æ‹’æŽ‰ï¼Œé£Žé™©ä½Žçš„é€šè¿‡ç”³è¯·ï¼Œé‚£ä¹ˆå¦‚ä½•åˆ’å®šè¿™ä¸ªå†³ç­–çš„åˆ†æ•°ç•Œé™å‘¢ï¼Ÿå¤šå°‘åˆ†æ•°åº”è¯¥é€šè¿‡ï¼Œå¤šå°‘åˆ†æ•°åº”è¯¥æ‹’ç»ï¼Ÿ ç”»å‡ºæ¯é˜¶å±‚çš„KSå€¼ï¼Œæœ€é«˜å€¼å¯¹åº”é˜¶å±‚ä¸ºå†³ç­–é˜ˆå€¼æ ¹æ®ä¸Šå›¾å¯ä»¥çœ‹å‡ºæ˜¯ç¬¬ä¸‰é˜¶å±‚çš„KS=0.2637æœ€å¤§ã€‚æŒ‰ç…§ä¹‹å‰åˆ†å±‚çš„ç»“æžœï¼Œè¿™å±‚çº§çš„å®¢æˆ·çš„åˆ†æ•°åŒºé—´æ˜¯(270,275](1)270åˆ†ä»¥ä¸‹å®¢æˆ·ï¼šé€šè¿‡(2)270-275åˆ†ï¼šäººå·¥å®¡æ ¸(3)275åˆ†ä»¥ä¸Šå®¢æˆ·ï¼šæ‹’ç» ä½†æ˜¯ï¼ŒKSåªæ˜¯åšå†³ç­–çš„æŸæ–¹é¢æ ¹æ®è€Œå·²ï¼Œä¹Ÿå¯æ ¹æ®æ¯é˜¶å±‚çš„è¿çº¦çŽ‡å†³å®šå†³ç­–é˜ˆå€¼ï¼ŒåŒæ—¶ä¹Ÿè¦è§‚å¯Ÿæ¯é˜¶å±‚çš„äººæ•°åˆ†å¸ƒã€‚æœ€æžç«¯çš„ä¾‹å­å°±æ˜¯ï¼Œä¸€ä¸‹å­æ‹’ç»æŽ‰æ‰€æœ‰ç”³è¯·è€…ï¼Œè¿™æ ·é€¾æœŸçŽ‡å°±æ˜¯0äº†ï¼Œä½†æ˜¯ä¹Ÿå°±å…³é—¨å¤§å‰å•¦~æ„Ÿè°¢çœ‹åˆ°è¿™é‡Œçš„æœ‹å‹ï¼ŒAå¡åˆ¶ä½œå°±åœ¨æ­¤å‘Šä¸€æ®µè½å•¦ðŸ˜Š è¯¾åŽé¢˜ðŸ˜‰ è®¡ç®—ä¸‹é¢æ¯ä¸ªå˜é‡çš„WOEå€¼å¯¹åº”çš„ä¿¡ç”¨é£Žé™©åˆ†æ•°ï¼š 1234567891011121314151617181920# ä¼ªä»£ç ...extration_amount[WOE] -0.4414730 -0.2142640 0.4596698 0.4386113 -0.3501923[ç³»æ•°b] 0.08642POSITION_Value[WOE] 0.5817640 -0.2522849 -0.2905931[ç³»æ•°b] -0.24813ZM_SCORE[WOE] 0.40926664 0.30817452 -0.01635135 -0.38743811 -0.74663108 -1.52210534[ç³»æ•°b] -0.39815ZM_SCORE_EXIST[WOE] 0.2976555 -0.3847163[ç³»æ•°b] 0.25519IDENTIFICATION_RESULT_Value$WOE[WOE] 0.6084594 -0.2568459 -0.4399638[ç³»æ•°b] -0.42831 æ€è€ƒåˆ†æ•°æ¢ç®—çš„Scalingä¸­ï¼ŒAä¸ŽBçš„ä½œç”¨ æç¤ºï¼šåœ¨ç¬¬å››å¤§æ­¥éª¤Scalingä¸­ï¼Œæœ‰è®¡ç®—é€»è¾‘ ç¦åˆ©â¤æºæ•°æ®ä¸‹è½½ðŸ‘ æ‰“èµè€…åœ¨å¤‡æ³¨ç•™ä¸‹é‚®ç®±éƒ½ä¼šèµ äºˆç›¸å…³ææ–™ä¹‹åŽä¼šå¢žåŠ åŸºäºŽå¤§æ€å™¨XGBOOSTåˆ¶ä½œçš„è¯„åˆ†å¡ï¼Œè€Œä¸”æœ‰Ræœ‰Pythonï¼æ•¬è¯·æœŸå¾…~]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>R</tag>
        <tag>model</tag>
        <tag>scorecard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017æ ¡æ‹›/ç¤¾æ‹›æ€»ç»“ï¼ˆäººå·¥æ™ºèƒ½å²—ï¼‰]]></title>
    <url>%2F2018%2F04%2F03%2F2017%E6%A0%A1%E6%8B%9B%E7%A4%BE%E6%8B%9B%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B2%97%EF%BC%89%2F</url>
    <content type="text"><![CDATA[è¿™ä¸ªå²—ä½çš„å«æ³•å¾ˆå¤šï¼šäººå·¥æ™ºèƒ½/æ•°æ®æŒ–æŽ˜/æœºå™¨å­¦ä¹ /â€¦æ ¸å¿ƒæ˜¯åæ•°å­¦ç®—æ³•çš„ï¼Œè·Ÿä¼ ç»Ÿå†™C/Javaç±»ç¨‹åºå‘˜ä¸å¤ªä¸€æ ·ã€‚ ã€æœ¬äººèƒŒæ™¯ã€‘ å¯ä½œä¸º2016/17/18åº”å±Šç”Ÿ 2012.9-2016.7 &nbsp;&nbsp;&nbsp; degree of bachelor 2016.9-2017.11&nbsp;&nbsp; degree of master (Hong Kong) å¦‚ä½•å‡†å¤‡æ ¡æ‹›ã€æ•´ä½“èŠ‚å¥çš„æŠŠæŽ§ã€‘å¤§åŽ‚çš„å¼€æ”¾æ—¶é—´ä¼šæ¯”è¾ƒæ—©ï¼Œå¯†åˆ‡å…³æ³¨ç½‘ç”³æ—¶é—´èŠ‚ç‚¹ï¼š 2016å±Šçš„ç§‹æ‹›ï¼š2016å¹´7æœˆ - 2016å¹´11æœˆ 2017å±Šçš„æ˜¥æ‹›ï¼š2017å¹´2æœˆ - 2017å¹´4æœˆ 2017å±Šçš„æš‘æœŸå®žä¹ ï¼š2017å¹´3æœˆ - 2017å¹´5æœˆ 2017å±Šçš„ç§‹æ‹›ï¼š2017å¹´7æœˆ - 2017å¹´11æœˆ 2018å±Šçš„æ˜¥æ‹›ï¼š2018å¹´2æœˆ - 2018å¹´4æœˆ 2018å±Šçš„ç§‹æ‹›ï¼š2018å¹´7æœˆ - 2018å¹´11æœˆ ã€æ‰‹æ’•ä»£ç èƒ½åŠ›ã€‘è¿™ä¸ªææ—©å‡†å¤‡æ²¡é”™çš„ã€‚ä»¥å‰è¢«é—®æ•²è¿‡å‡ è¡Œpythonè§‰å¾—è¿™ä¸ªé—®é¢˜å¾ˆè ¢ï¼Œä½†å®žé™…å°±æ˜¯æ•²çš„è¶Šå¤šä½ è¸©è¿‡çš„å‘å°±è¶Šå¤šï¼Œä½ èƒ½å¿«é€Ÿç‹¬ç«‹è§£å†³çš„é—®é¢˜å°±è¶Šå¤šã€‚ ä¸»æµè¯­è¨€ï¼šRã€Python å¤šæ•°åœ¨å¤§åŽ‚çš„é¢è¯•ï¼šç›¸å…³åŒ…çš„ä½¿ç”¨ï¼ŒåŠå…·ä½“åœºæ™¯é—®é¢˜ï¼Œæ¯”å¦‚å†…å­˜ä¸å¤Ÿçš„è§£å†³æ–¹æ¡ˆç­‰ã€‚ â€œç ”å‘â€å’Œâ€œå¤§æ•°æ®â€è·Ÿæœºå™¨å­¦ä¹ æœ‰æœ¬è´¨çš„åŒºåˆ«ï¼Œä½†åœ¨å®žé™…å·¥ä½œåº”ç”¨ä¸­æœ‰è®¸å¤šç©¿æ’å·¥ä½œï¼Œé¿å…ä¸äº†çš„è¢«æé—®â€œæ•°æ®ç»“æž„åŠç®—æ³•ç±»(C/Java)â€å’Œâ€œå¤§æ•°æ®ç±»(Hadoop)â€çš„é—®é¢˜ã€‚å½“ç„¶ä¸å¼ºæ±‚ï¼ŒçŸ¥é“æ›´å¤šå°±æ˜¯åŠ åˆ†é¡¹ã€‚ å»ºè®®ææ—©åŠå¹´å¼€å§‹å‡†å¤‡ã€‚æˆ‘çš„ä»£ç ä¹Ÿæ˜¯ä»Žå®žä¹ å¼€å§‹æ•²èµ·ï¼Œæ•²äº†åŠå¹´æ‰è§‰å¾—ä¸‹æ‰‹å¦‚æœ‰ç¥žå“ˆå“ˆã€‚ä¸è¦åšæ²¡å®žé™…æ„ä¹‰çš„è¯¾åŽé¢˜ï¼Œä¹Ÿä¸è¦ç…§ç€ä¹¦æœ¬ä¾‹é¢˜æ•²ï¼Œæ•²å®Œä½ å°±å¿˜äº†ï¼Œä¹¦æœ¬è¿™äº›éƒ½æ˜¯å·²ç»æŽ’é™¤ä¸‡éš¾çš„ä¸œè¥¿ï¼Œå¾—ä¸åˆ°ä»€ä¹ˆæˆé•¿ã€‚ å…¥é—¨ä¿®ç‚¼ï¼šå…¨å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«žèµ›ã€å…¨ç¾Žå¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«žèµ›ã€kaggleã€å¤©æ± â€¦ ã€é¡¹ç›®ç»åŽ†/å®žä¹ ç»åŽ†ã€‘å¦‚æžœæ˜Žç¡®è‡ªå·±çš„èŒä¸šæ–¹å‘ä¸ºäººå·¥æ™ºèƒ½/æ•°æ®æŒ–æŽ˜ç±»çš„ï¼Œè¯·ä¸è¦æµªè´¹æ—¶é—´åŽ»ç”³è¯·å…¶ä»–ä¸ŽæŠ€æœ¯æ— å…³çš„å®žä¹ ã€‚ç«¯èŒ¶é€æ°´ï¼Œå¤–å–è·‘è…¿ï¼Œæ‰“å°çº¸å¹¶ä¸èƒ½å¸®ä½ ã€‚å½“æ—¶ç”±äºŽèº«è¾¹åŒå­¦éƒ½æ–­æ–­ç»­ç»­å‡ºåŽ»å®žä¹ ï¼Œé¢å‰æœ‰ä¸€ä»½å¤§åŽ‚è¡Œæ”¿çš„å®žä¹ ï¼Œæˆ‘â€¦ç«Ÿç„¶çŠ¹è±«äº†ä¸€ä¸‹ï¼Œå¥½åœ¨ä¹Ÿè¿˜æ˜¯æ‹’ç»äº†ã€‚ å°½é‡é€‰æ‹©å¤§åŽ‚çš„æŠ€æœ¯å®žä¹ ï¼Œæ¯•ç«Ÿä»¥åŽæƒ³è¿›åŽ»ä¼šæ›´éš¾ã€‚ä½†æ˜¯ä¸è¦å› ä¸ºä¸€ä¸ªæœˆæ‹¿3000å—å°±åªå¹²3000å—çš„æ´»ã€‚æŠŠæ•´ä¸ªé¡¹ç›®è·Ÿä¸‹æ¥ï¼Œäº†è§£æ¡†æž¶çš„æž¶æž„ï¼Œä¼˜åŒ–çš„æ–¹å‘ï¼Œå¤šåŽ»å°è¯•ï¼Œå°±ç®—åŠ ç­ï¼ˆåŠ ç­åœ¨æ·±åœ³å¾ˆæ­£å¸¸ï¼‰ä¹Ÿæ˜¯ä½ èµšåˆ°ï¼Œæ€è€ƒå¦‚ä½•ç®€åŒ–é‡å¤æ€§å·¥ä½œï¼ŒåŽ»å°è¯•äº†è§£è‡ªå·±éƒ¨é—¨å’Œå…¶ä»–éƒ¨é—¨çš„å·¥ä½œå†…å®¹ä¸Žæ–¹å‘ï¼Œäº†è§£çš„è¶Šå¤šä½ å¯¹è‡ªå·±æƒ³åšçš„äº‹æƒ…äº†è§£çš„ä¹Ÿè¶Šå¤šã€‚æˆ‘å®žä¹ åšçš„è¯„åˆ†å¡æ¨¡åž‹ï¼Œå°±æ˜¯äºŒåˆ†ç±»ï¼Œé™¤äº†ä¼ ç»Ÿé€»è¾‘å›žå½’ï¼Œä¹Ÿå°è¯•æ–°çš„XGBç­‰ç­‰ï¼Œè€Œä¸”è™½ç„¶åˆ«äººä¹Ÿåœ¨åšï¼Œä½†æ˜¯ç§ä¸‹è‡ªå·±ä¼šæŠŠæ•´ä¸ªæ¨¡åž‹å†™ä¸€éï¼ŒåŒ…å«æ•°æ®æ¸…æ´—å’Œæ¨¡åž‹è°ƒä¼˜ç­‰ï¼Œè¿™æ ·å¯¹ä¸šåŠ¡çš„äº†è§£ä¹Ÿæ›´é€å½»ï¼Œé¢è¯•èµ·æ¥æ‰€æœ‰çš„ç»†èŠ‚éƒ½æ˜¯äº²æ‰‹åšè¿‡çš„ï¼Œä¹Ÿå°±æ¯”è¾ƒé¡ºäº†ã€‚ å¦‚æžœæ²¡æœ‰å®žä¹ åœ¨æ‰‹ï¼Œä¸–ç•Œç»™æˆ‘ä»¬æ•°æ®æŒ–æŽ˜é€‰æ‰‹çš„å¤§é—¨è¿˜æ˜¯æ•žå¼€ç€çš„ã€‚å¤§å­¦ç”Ÿæœ‰å…¨å›½çš„æ•°å­¦å»ºæ¨¡å¤§èµ›ï¼Œå†é«˜é€¼æ ¼çš„è¿˜æœ‰kaggleï¼ŒçœŸåˆ€çœŸæžªçš„é¢˜ä¸‹ä¸äº†æ‰‹ï¼Œkaggleä¸Šè¿˜æœ‰ä¸“é—¨ç»™æ•°æ®æŒ–æŽ˜å…¥é—¨è€…çš„ç»ƒä¹ åœºã€‚ç›¸å…³çš„æ¯”èµ›è¿˜æœ‰å¾ˆå¤šï¼ŒåŒ…æ‹¬è…¾è®¯ã€é˜¿é‡Œç­‰å¤§åŽ‚ä¹Ÿæ—¶ä¸æ—¶ä¼šå‘å¸ƒç®—æ³•å¤§èµ›ï¼Œç›®æµ‹è¿™æ ·çš„ç®—æ³•å¤§èµ›åªä¼šè¶Šæ¥è¶Šå¤šï¼Œä½ åšæŒåšå®Œä¸€ä¸ªé¡¹ç›®ï¼Œä½ åœ¨å¹³å°ä¸Šè¿˜å¯ä»¥å¾—åˆ°ç›¸å…³åæ¬¡ï¼Œåæ¬¡è¶Šé å‰è¶Šæœ‰åˆ©å“ˆå“ˆå“ˆè¿™æ˜¯åºŸè¯ã€‚ ã€ä¸´æ—¶ä½›è„šè¯¥æŠ±è¿˜å¾—æŠ±ã€‘ æ•°æ®åº“ï¼šsqlã€hiveè¯­å¥çš„åŸºæœ¬ç”¨æ³•W3school åŸºæœ¬ç®—æ³•çš„æŽ¨å¯¼ï¼šSVM/BP/LR/CART/GBDT/XGB/KNN/KMEANS/â€¦ åŸºæœ¬åŽŸç†çš„é€»è¾‘ï¼šå†³ç­–æ ‘çš„å‰ªæžç­‰/ç¥žç»ç½‘ç»œçš„dropoutç­‰/æŸå¤±å‡½æ•°/æ­£åˆ™åŒ–/â€¦ åŸºæœ¬çš„åŒ…çš„ä½¿ç”¨ï¼špythonçš„pandas/numpy/sklearn; Rçš„dplyr/data.table/â€¦ å¿…å¤‡é—®é¢˜ï¼šä¸ºä»€ä¹ˆæƒ³æ¥è¿™å®¶å…¬å¸ï¼Ÿ/ä¸ºä»€ä¹ˆç¦»å¼€ä¸Šå®¶å…¬å¸ï¼Ÿï¼ˆæå‰äº†è§£é¢è¯•å…¬å¸èƒŒæ™¯å’Œäº§å“ï¼‰ å®Œæ•´é¢ç­‹+æ€»ç»“ æ ¡æ‹›== å¾®ä¼—é“¶è¡Œ ==ç½‘ç”³ / åœ¨çº¿ç¬”è¯•ï¼ˆ4å¤§é¢˜ï¼‰ æœ‰è¿‘ä¸€å¹´å„ç±»æ•°æ®ï¼Œé™¤åŽ»èŠ‚å‡æ—¥å¸¦æ¥çš„é”€å”®å¢žé•¿ï¼Œé¢„æµ‹æŸå®¶å•†åœºåœ¨å›½åº†èŠ‚å‡æ—¥åŽŸæœ¬çš„é”€å”®é‡ã€‚ï¼ˆç”¨æ—¶é—´åºåˆ—æ¨¡åž‹è§’åº¦è§£é¢˜ï¼‰ ä¸€ä¸ªå¸‚ä¸­å¿ƒçš„å® ç‰©åº—ï¼Œä¸ºä½•å‘¨æœ«çš„é¡¾å®¢è®¿é—®é‡ä½ŽäºŽä¸Šç­æ—¥ï¼Ÿï¼ˆè€ƒè™‘å…»å® ç‰©çš„äººç¾¤åœ¨å·¥ä½œæ—¥ä¸Žå‘¨æœ«çš„ä¹ æƒ¯ï¼‰ ä¸­ä½æ•°ä¸Žå¹³å‡æ•°çš„å…³ç³»ï¼ˆç®±å›¾è§£é‡Šå¤§äºŽç­‰äºŽå°äºŽä¸‰ç§æ ·æœ¬ç±»åž‹ï¼‰ å¦‚ä½•åˆ¤åˆ«æŸé«˜æ ¡é£Ÿå ‚çš„ç‚¹é¤è€…æ˜¯åœ¨æ ¡å­¦ç”Ÿè¿˜æ˜¯ç¤¾ä¼šäººå£«ï¼Ÿï¼ˆå‘¨æœŸæ€§çš„ä¹°é¤æ—¶ç‚¹ä¸Žé¢‘çŽ‡ç­‰ï¼‰ æ„Ÿæƒ³å°±æ˜¯ï¼Œè‡ªå·±åˆ¤åˆ«æ˜¯èšç±»è¿˜æ˜¯åˆ†ç±»ç­‰ï¼Œè‡ªå·±å†³å®šå“ªäº›æ˜¯éœ€è¦çš„ç‰¹å¾ï¼Œå“ªä¸ªæ˜¯yå€¼ï¼Œå››å¤§é¢˜éƒ½æ˜¯æ€è·¯é¢˜ï¼Œæ²¡æœ‰ç¼–ç¨‹å¤§é¢˜ã€‚ == é¡ºä¸°ç§‘æŠ€ ==ç½‘ç”³ / åœ¨çº¿ç¬”è¯•ï¼ˆ10ä¸ªé€‰æ‹©é¢˜ + 1é“æ€è·¯é¢˜ï¼‰ é€‰æ‹©é¢˜èŒƒå›´ï¼šæŸå¤±å‡½æ•°/æ­£åˆ™åŒ–/åŸºæœ¬ç®—æ³•/æ ˆ/â€¦ å¤§é¢˜ï¼šSVMçš„åº”ç”¨ï¼šæ£€æµ‹å¿«é€’åŒ…è£¹å†…è¿ç¦ç‰©å“ï¼ˆå†³å®šxå€¼ä¸Žyå€¼ï¼‰ == ç¾Žå›¾ç§€ç§€ ==ç½‘ç”³ / åœ¨çº¿ç¬”è¯•ï¼ˆ10ä¸ªé€‰æ‹©é¢˜ + 1é“ç¼–ç¨‹é¢˜ï¼‰æ•´åœºç¬”è¯•é¢˜æ¯”è¾ƒåå‘ç ”å‘ç±»ï¼Œè€ƒåˆ°è¾ƒå¤šçš„Javaå’ŒCç›¸å…³ï¼Œè€ŒåŸºæœ¬æ²¡æœ‰Rä¸ŽPythonã€‚æ‰€ä»¥ä¸æŽ¨èæ“…é•¿æ•°å­¦ä¸“ä¸šçš„å­¦ç”Ÿå°è¯•ï¼ŒæŽ¨èè®¡ç®—æœºä¸“ä¸šçš„çˆ±æ»¤é•œçˆ±ç¾Žé¢œçš„åŒå­¦åŽ»è¯•è¯•å“ˆ~ ç¤¾æ‹›ï¼ˆå…¨éƒ¨ä¸ºå†…æŽ¨æœºä¼šï¼‰== å¾®ä¿ ==è½¦ä¿åæ¬ºè¯ˆ / ä¸€é¢ï¼ˆçŽ°åœºï¼‰ çŽ°åœ¨åœ¨å¾®ä¿¡çš„è…¾è®¯æœåŠ¡æ¨¡å—å·²ç»ä¸Šçº¿äº†ï¼Œç¬¬ä¸€æ¬¡è¿™ä¹ˆé è¿‘è…¾è®¯å¤§åŽ¦å‘€å†…å¿ƒè¿™ä¸ªæ¿€åŠ¨çš„ï¼Œæœ¬äººå½“æ—¶å¯¹è…¾è®¯æœ‰ç§ç›²ç›®çš„å´‡æ‹œã€‚ è‡ªæˆ‘ä»‹ç» é¡¹ç›®ä»‹ç» é¡¹ç›®LRå’ŒXGBåŽŸç†ã€åŒºåˆ« ç±»åˆ«ä¸å‡è¡¡å¦‚ä½•å¤„ç† åæ¬ºè¯ˆåœºæ™¯ï¼šå¦‚ä½•åˆ¤åˆ«éª—ä¿è¡Œä¸ºï¼ˆå‡è®¾ä½ å¯ä»¥æ‹¥æœ‰æ‰€æœ‰ä½ æƒ³è¦çš„æ•°æ®ï¼Œå½“æ—¶è§‰å¾—çœŸä¸æ„§è…¾è®¯çˆ¸çˆ¸ï¼‰ ä¸ºä»€ä¹ˆXGBæ¯”GBDTå¥½ æ•°æ®æ¸…æ´—æœ‰å“ªäº› PCAçš„åŽŸç†ï¼Œè®¡ç®—æŽ¨å¯¼ å˜é‡ç­›é€‰æœ‰å“ªäº›æ–¹æ³• ä¿¡æ¯å¢žç›Šçš„è®¡ç®—å…¬å¼ æ ·æœ¬é‡å¾ˆå°‘æƒ…å†µä¸‹å¦‚ä½•å»ºæ¨¡ äº¤å‰æ£€éªŒçš„å®žçŽ° å†³ç­–æ ‘å¦‚ä½•å‰ªæž WOE/IVå€¼è®¡ç®—å…¬å¼ åˆ†ç®±æœ‰å“ªäº›æ–¹æ³• æœ€ä¼˜åˆ†ç®±çš„åŽŸç† == é¹…åŽ‚ ==è§†é¢‘æŽ¨èæ•°æ®æŒ–æŽ˜å·¥ç¨‹å¸ˆ / ä¸€é¢ï¼ˆçŽ°åœºï¼‰ è‡ªæˆ‘ä»‹ç» æ‰‹æŽ¨SVMï¼šç›®æ ‡å‡½æ•°ï¼Œè®¡ç®—é€»è¾‘ï¼Œå…¬å¼éƒ½å†™å‡ºæ¥ï¼Œå¹³é¢ä¸Žéžå¹³é¢ é¡¹ç›®ä»‹ç» XGBåŽŸç†ä»‹ç»ï¼Œå‚æ•°ä»‹ç»ï¼Œå†³ç­–æ ‘åŽŸç†ä»‹ç» æ•°æ®æ¸…æ´—æ­¥éª¤ Linuxç†Ÿæ‚‰ç¨‹åº¦ C/Javaç†Ÿæ‚‰ç¨‹åº¦ è¿‡æ‹Ÿåˆå¦‚ä½•è§£å†³ é™¤äº†LR/XGBè¿˜å®žçŽ°è¿‡ä»€ä¹ˆç®—æ³• åˆ†ç®±æœ‰å“ªäº› æ ¸å‡½æ•°æœ‰å“ªäº› è…¾è®¯æ¸¸æˆï¼šç”¨æˆ·ç”»åƒå¤§æ•°æ®åˆ†æžå·¥ç¨‹å¸ˆ / ä¸€é¢ï¼ˆç”µè¯é¢ï¼‰ è‡ªæˆ‘ä»‹ç» é¡¹ç›®ä»‹ç» LRçš„åŽŸç†ï¼Œç›®æ ‡å‡½æ•° XGBæ¯”GBDTå¥½åœ¨å“ªé‡Œ ç†Ÿæ‚‰è…¾è®¯å“ªæ¬¾æ¸¸æˆï¼Ÿï¼ˆçŽ‹è€…è£è€€ï¼‰ æ¸¸æˆåæ¬ºè¯ˆåœºæ™¯é¢˜ï¼šåœ¨çŽ‹è€…è£è€€ä¸­ï¼Œå‡è®¾ä½ æ‹¥æœ‰æ‰€æœ‰ä½ æ‰€éœ€æ•°æ®ï¼Œå¦‚ä½•åˆ¤åˆ«çŽ©å®¶æŒ‚æœºï¼Ÿ æ¸¸æˆåæ¬ºè¯ˆåœºæ™¯é¢˜ï¼šåœ¨çŽ‹è€…è£è€€çš„æŒ‚æœºçŽ©å®¶ä¸­ï¼Œå¦‚ä½•åˆ¤åˆ«å¯¹æ–¹æ˜¯ä¸»åŠ¨æŒ‚æœºï¼ˆå‘é˜Ÿå‹ï¼‰è¿˜æ˜¯è¢«åŠ¨æŒ‚æœºï¼ˆè¿›å…¥åœ°é“ä¿¡å·ä¸å¥½ç­‰ï¼‰ï¼Ÿ å¹³æ—¶é€šè¿‡ä»€ä¹ˆæ¸ é“å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿï¼ˆå¥½é—®é¢˜å€¼å¾—å¥½å¥½å‡†å¤‡ï¼‰ å¼€å‘éœ€æ±‚ï¼šLINUXçŽ¯å¢ƒçš„ç†Ÿæ‚‰ç¨‹åº¦ Java/C++çš„åŸºæœ¬æ“ä½œä¸ŽåŽå°ç»´æŠ¤ pythonå¦‚ä½•å®žçŽ°æ•°å€¼ä¼ é€’ == é¡ºä¸°ç§‘æŠ€ ==å¤§æ•°æ®æŒ–æŽ˜ä¸Žåˆ†æžå·¥ç¨‹å¸ˆ / ä¸€é¢ï¼ˆçŽ°åœºï¼‰ è¿ç­¹å­¦æœ€ç†Ÿæ‚‰å“ªä¸ªç®—æ³• åœºæ™¯é¢˜ï¼šåŒåä¸€å¿«é€’çˆ†å‘æ—¶ï¼Œå¦‚ä½•å‡å°‘å¿«é€’å‘˜çš„åŠ³åŠ¨åŠ›ï¼Ÿï¼ˆä»“åº“é€‰å€é—®é¢˜ï¼‰ XGBä¸ŽGBDTçš„åŒºåˆ« XGBæœ‰å“ªäº›å‚æ•°ï¼Œå–æŒ‡èŒƒå›´ï¼Œå„ä»£è¡¨ä»€ä¹ˆæ„æ€ æŽ’åˆ—ç»„åˆæ¦‚çŽ‡é¢˜ å¦‚ä½•é€šè¿‡çº¿æ€§è§„åˆ’å®žçŽ°æœ€ä¼˜é€è´§è·¯å¾„ å†³ç­–æ ‘å…ˆå‰ªæžè¿˜æ˜¯åŽå‰ªæžå¥½ æŸå¤±å‡½æ•°æœ‰å“ªäº› æœ€è¿‘æœºå™¨å­¦ä¹ æ¯”è¾ƒç«çš„ä¹¦ç± é™¤äº†Rä¸ŽPythonå¤–ï¼ŒMATLABç†Ÿæ‚‰ç¨‹åº¦ åå‘åšæ•°æ®æŒ–æŽ˜è¿˜æ˜¯ç®—æ³•ç ”ç©¶ è¯´ä¸€ä¸ªèšç±»ç®—æ³• C++çš„è¯­æ³•çŸ¥é“å“ªäº› ç±»åˆ«ä¸å‡è¡¡çš„å¤„ç†æ–¹å¼æœ‰å“ªäº› baggingä¸Žboostingçš„åŒºåˆ« æ•°æ®æ ‡å‡†åŒ–æœ‰å“ªäº›æ–¹æ³• æ¨¡åž‹è¯„ä¼°æŒ‡æ ‡æœ‰å“ªäº› è§£é‡Šæ¨¡åž‹å¤æ‚åº¦ onehotåŽŸç† æ­£åˆ™åŒ–å¦‚ä½•å®žçŽ° è¯´å‡ºä¸€ä¸ªèšç±»ç®—æ³•(KMEANS) æœºå™¨å­¦ä¹ ä¸Žäººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆ / ä¸€é¢ï¼ˆçŽ°åœºï¼‰ è‡ªæˆ‘ä»‹ç» é¡¹ç›®ä»‹ç»-è¯„åˆ†å¡æ¨¡åž‹ è¯„åˆ†å¡æ¨¡åž‹çš„åŽŸç† = LRåŽŸç† æˆ– äºŒåˆ†ç±»åŽŸç† è¯„åˆ†å¡æ¨¡åž‹çš„æœ€ç»ˆç»“æžœå¦‚ä½•è½¬æ¢æˆå†³ç­–è§„åˆ™ï¼ŒKSå€¼èµ·åˆ°ä»€ä¹ˆä½œç”¨ ä¸ºä»€ä¹ˆè¦è½¬è¡Œ/ä¸ºä»€ä¹ˆæ¥é¡ºä¸° æ•°æ®æ¸…æ´—æ–¹æ³•æœ‰å“ªäº› WOEçš„ä½œç”¨ ç¼ºå¤±å€¼å¡«å……æ–¹å¼æœ‰å“ªäº› å˜é‡ç­›é€‰æ–¹æ³•æœ‰å“ªäº› ROCè®¡ç®—é€»è¾‘ æœºå™¨å­¦ä¹ ä¸Žäººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆ / äºŒé¢BOSSé¢ï¼ˆçŽ°åœºï¼‰ ç»Ÿè®¡æ£€éªŒæœ‰å“ªäº› tæ£€éªŒå¦‚ä½•ä½¿ç”¨ åœºæ™¯é¢˜ï¼šä¸¤ä¸ªå–åœºå·®ä»·åªæœ‰1åˆ†é’±ï¼Œä½ ä»Žå“ªä¸ªå–åœºè¿›è´§ï¼Ÿï¼ˆè¶…çº§ç®€å•çš„é—®é¢˜ï¼Œæ²¡æœ‰é™åˆ¶æ¡ä»¶ï¼Œå½“ç„¶ä»Žä¾¿å®œçš„é‚£å®¶ï¼‰ é¡¹ç›®-è¯„åˆ†å¡åˆ¶ä½œä»‹ç» ä»€ä¹ˆæ—¶å€™èƒ½ä¸Šç­ å½“æ—¶é¢è¿‡äº†æŠ€æœ¯é¢å’ŒBOSSé¢ï¼Œå¡åœ¨HRé¢è¯´æ˜¯å› ä¸ºæˆ‘æ²¡æœ‰å·¥ä½œç»éªŒå°±æŠŠæˆ‘æ‹’äº†ï¼Œå†…å¿ƒçœŸçš„è®¨åŽŒé¡ºä¸°HRï¼Œæ‰€ä»¥å¯¹é¡ºä¸°æ²¡ä»€ä¹ˆå¥½æ„Ÿï¼Œå¯èƒ½å½“æ—¶çœŸçš„å¯„äºˆåŽšæœ›çš„ï¼Œå¤ªè®©äººå¤±æœ›äº† == é­…æ—ç§‘æŠ€ ==äººå·¥æ™ºèƒ½ä¸Žæ•°æ®æŒ–æŽ˜å·¥ç¨‹å¸ˆ / ä¸€é¢ï¼ˆæŠ€æœ¯+BOSSï¼‰ï¼ˆçŽ°åœºï¼‰ è‡ªæˆ‘ä»‹ç» é¡¹ç›®ä»‹ç» å†³ç­–æ ‘çš„ä¼˜ç‚¹ å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªæ¨¡åž‹ä¸­çš„å˜é‡å¤ªå¤š å†³ç­–æ ‘ä¸Žå…¶ä»–æ¨¡åž‹çš„æŸå¤±å‡½æ•°ã€å¤æ‚åº¦çš„æ¯”è¾ƒ pythoné‡åˆ°å¤§æ•°æ®é‡æ—¶å¦‚ä½•å¤„ç†ç©ºé—´é—®é¢˜ å†³ç­–æ ‘èƒ½å¦æœ‰éžæ•°å€¼åž‹å˜é‡ å†³ç­–æ ‘ä¸Žç¥žç»ç½‘ç»œçš„åŒºåˆ«ä¸Žä¼˜ç¼ºç‚¹å¯¹æ¯” è¿‡æ‹Ÿåˆå¦‚ä½•å¤„ç† ç±»åˆ«ä¸å‡è¡¡å¦‚ä½•å¤„ç† æ•°æ®ç»“æž„æœ‰å“ªäº› XGBçš„å‚æ•°æœ‰å“ªäº› model ensemblingçš„æ–¹æ³•æœ‰å“ªäº› æ¨¡åž‹å¤æ‚åº¦ä¸Žä»€ä¹ˆæœ‰å…³ çº¿æ€§å›žå½’ä¸Žé€»è¾‘å›žå½’çš„åŒºåˆ« å¤§æ•°æ®äº†è§£å¤šå°‘ï¼ŸHadoopä½¿ç”¨ç¨‹åº¦ == ç»¿ç±³(offer) ==äººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆ / ä¸€é¢ï¼ˆè‹±æ–‡çŽ°åœºï¼‰ å¾ˆå¯çˆ±çš„è€å¤–æŠ€æœ¯å®˜ï¼Œç›´æŽ¥æŠŠæˆ‘çš„ç®€åŽ†ç¿»è¿‡åŽ»ï¼Œç©ºç™½é¢æœä¸Šã€‚å¹¶è¯´ä»–çš„é¢è¯•é£Žæ ¼å°±æ˜¯ä¸Žå¯¹é¢çš„äººæ²Ÿé€šè€Œä¸æ³¨é‡çº¸ä¸Šçš„å­—ã€‚ç„¶åŽå°±ä»¥é—®æˆ‘çš„åå­—è¯»éŸ³å’Œæˆ‘çš„å®¶ä¹¡å¼€å§‹äº†æˆ‘ç»ˆèº«éš¾å¿˜çš„é¢è¯•~~ ä½ åœ¨é¦™æ¸¯è¯»ä¹¦ï¼Œå¬è¯´é¦™æ¸¯äººéƒ½å¾ˆæŽ’å¤–æ˜¯è¿™æ ·çš„å—ï¼Ÿ å‡å¦‚å’±ä»¬çš„æ¡Œå­ä¸ŠçŽ°åœ¨æœ‰ä¸ªé˜¿æ‹‰ä¸ç¥žç¯ï¼Œä½ ä¼šè®¸å“ªä¸‰ä¸ªæ„¿æœ›ï¼Ÿ ä½ æœ€è®¨åŽŒçš„äº‹æƒ…ï¼Ÿ é¢è¯•å®˜Tonyè®²äº†å¾ˆå¤šä»–çš„æ•…äº‹å¹¶ä¸”ç»™æˆ‘æŽ¨èäº†å‡ æœ¬ä¹¦ å±•ç¤ºäº†ç»¿ç±³çš„äº§å“ï¼Œä¹Ÿæ¿€åŠ¨çš„å‘Šè¯‰HR â€˜You must hire her!â€™ åˆ«çœ‹å¥½åƒè¿™ä¸ªé¢è¯•å¾ˆæ— èŠï¼Œé‡ç‚¹åœ¨ä¸‰ä¸ªæ„¿æœ›é‚£é‡Œï¼Œæˆ‘åœ¨æˆ‘æœ€ç„¦è™‘çš„æ—¶å€™æ—¥æ—¥å¤œå¤œæ€è€ƒæˆ‘åˆ°åº•è¦ä»€ä¹ˆï¼Œåˆ°åº•æ¥è¿™ä¸–é—´ä¸ºäº†ä»€ä¹ˆï¼Œåšä»€ä¹ˆäº‹æƒ…èƒ½è®©æˆ‘ä»Žå¿ƒåº•é‡Œå¼€å¿ƒè€Œä¸æ˜¯å°†è‡ªå·±æµ®äºŽä¼—äººçš„å”¾æ¶²ä¹‹ä¸‹ã€‚å¾ˆåº†å¹¸è¿™åœºé¢è¯•æ²¡æœ‰æŠ€æœ¯é¢ï¼Œä½†æ˜¯æˆ‘è¿žç»­ä¸‰å¤©ç†¬å¤œå®žæ‰“å®žçš„å‡†å¤‡äº†ï¼Œæ‰‹æŽ¨äº†æ‰€æœ‰æˆ‘çŸ¥é“çš„ç®—æ³•ï¼Œè¯´å®žè¯æˆ‘ä¹Ÿä¸æ€•å°±ç®—é—®åˆ°ä»€ä¹ˆæˆ‘ä¸çŸ¥é“è‡³å°‘æˆ‘å°½å…¨åŠ›äº†ã€‚ä½†æ˜¯å¥½åƒå°±æ˜¯é‚£ä¸ªèƒ½çœ‹æ‡‚æˆ‘çš„äººç»ˆäºŽå‡ºçŽ°ï¼Œä»–æ„¿æ„ä»¥å…¬å¸ç»™æˆ‘æä¾›å¹³å°ï¼Œè®©æˆ‘å®žçŽ°è‡ªå·±çš„æƒ³æ³•ä¸Žæ„¿æœ›ï¼ŒçœŸçš„åˆ«æ— æ‰€æ±‚ã€‚æ‰€ä»¥ï¼Œæ— è®ºé‚£äº›ç¬”è¯•æ˜¯å¦æœ‰å›žéŸ³ï¼Œæˆ‘ä¹Ÿä¸åœ¨ä¹Žäº†ï¼Œå°±æ˜¯ç»¿ç±³äº†ã€‚æˆ‘çš„æ±‚èŒä¹‹è·¯å°±åœ¨æ­¤å‘Šä¸€æ®µè½äº†ã€‚ æ€»ç»“ï¼š æ¯ä¸ªé¢è¯•çš„ç»“å°¾ï¼Œé¢è¯•å®˜ä¼šé—®ä½ æœ‰æ²¡æœ‰ä»€ä¹ˆæƒ³é—®çš„ï¼Œè¯·æ³¨æ„è¿™ä¸ªé—®é¢˜ä¹Ÿå¾ˆå…³é”®ï¼šæ¯”å¦‚ï¼šè¿™ä¸ªå°ç»„ç›®å‰åœ¨åšä»€ä¹ˆé¡¹ç›®/å®žçŽ°é¡¹ç›®ä¸»è¦ç”¨ä»€ä¹ˆè¯­è¨€å’Œç®—æ³•/â€¦å°½é‡ä¸è¦é—®åŠ ä¸åŠ ç­ï¼Œæœ‰æ²¡æœ‰åŠ ç­è´¹ä¹‹ç±»çš„ï¼Œåˆ«é—®æˆ‘ä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´â€¦åœ¨é¢è¯•ä¸­é‡åˆ°ä¸ç†è§£çš„ï¼Œæ¯”å¦‚C++è¯­æ³•ä¸æ‡‚ï¼Œå¯ä»¥é—®è¿™ä¸ªC++å…·ä½“åœ¨é¡¹ç›®ä¸­å®žçŽ°ä»€ä¹ˆåŠŸèƒ½å¦‚æžœä½ æå‡ºå¥½é—®é¢˜ï¼Œèƒ½å†æ¬¡å¼•èµ·é¢è¯•å®˜å¯¹ä½ çš„å…´è¶£ï¼Œé‚£å°±èƒ½å¢žåŠ é¢è¯•æˆåŠŸçŽ‡ åº”å±Šç”Ÿå°±å¥½å¥½å‡†å¤‡æ ¡æ‹›ï¼Œåˆ«æ‡’ï¼Œåˆ«æ€•è¾“ï¼Œåˆ«æ€•è¢«æ‹’ï¼Œä»Žå“ªé‡Œè·Œå€’ä»Žå“ªé‡Œèµ·æ¥ç¤¾æ‹›ä¸æ˜¯ä½ èƒ½æ‹›å‘¼çš„ï¼Œä¼šæ›´æŒ«è´¥ï¼Œå› ä¸ºä½ ä»€ä¹ˆä¹Ÿæ²¡åšè¿‡ è™½ç„¶æ˜¯åšæŠ€æœ¯çš„ï¼Œä½†æ˜¯æ—¥å¸¸socialä¸€ä¸‹è¿˜æ˜¯æ”¶ç›Šå¾ˆå¤§çš„å®žä¹ çš„æ—¶å€™ï¼Œä¹Ÿè¦ä¸Žå‘¨å›´åŒäº‹å’Œå¹³ç›¸å¤„ï¼Œå°¤å…¶æ˜¯è€å¤§å“¥ä»¬ï¼Œä¹Ÿè®¸å“ªå¤©ä»–å°±å¸®ä½ å†…æŽ¨å¤§åŽ‚åŽ»äº†å†…æŽ¨ä½ èƒ½çŸ¥é“æ„æƒ³ä¸åˆ°çš„ä¿¡æ¯ï¼Œé¢è¯•å®˜ï¼Œå²—ä½éœ€æ±‚ï¼Œæœ€è¿‘åœ¨åšä»€ä¹ˆé¡¹ç›®ä¹‹ç±»çš„ æŒ‘é€‰ç»™ä½ æœºä¼šçš„å…¬å¸ï¼Œä¸è¦æµªè´¹è‡ªå·±çš„æ—¶é—´ä¸è¦æ¯å®¶éƒ½åŽ»ï¼ŒåŽ»ä¹‹å‰äº†è§£è¿™å®¶å…¬å¸ä¸Žä½ çš„åŒ¹é…åº¦å°¤å…¶ç¤¾æ‹›ï¼Œä½ ä¸€æ”¹åŠ¨ç®€åŽ†å°±å¾ˆå¤šäººç»™ä½ æ‰“ç”µè¯ï¼Œä½ è¦æœ‰ç­–ç•¥çš„åŽ»è¿›è¡Œé¢è¯•ï¼ŒæŠŠæ¡æ€»ç»“æ¯ä¸ªæœºä¼šåƒæˆ‘å°±æ˜¯ä¸œä¸€æ¦”å¤´è¥¿ä¸€æ¦”å¤´çš„ï¼Œå¥½å¤šéƒ½æ˜¯æ­¢æ­¥äºŽç¬¬ä¸€é¢ï¼Œå°±æ²¡å›žä¿¡å„¿äº†ï¼Œå› ä¸ºæ¯æ¬¡é¢å®Œæ²¡æœ‰å¥½å¥½åæ€æ€»ç»“ï¼Œç­‰ä¸‹æ¬¡å†é‡åˆ°è¿™é—®é¢˜è¿˜æ˜¯æŠ“çžŽï¼Œååˆ†æ¶ˆè€—è‡ªå·±çš„æ—¶é—´å’Œä¿¡å¿ƒ åœ¨æ­¤éž èº¬æ„Ÿè°¢æ‰€æœ‰å¸®æˆ‘å†…æŽ¨å°å“¥å“¥å°å§å§ï¼Œé“­è®°äºŽå¿ƒï¼Œæ¥æ—¥å¿…æŠ¥ç­”ã€‚ å†™åœ¨æœ€åŽï¼šæ°¸è¿œä¸è¦æ”¾å¼ƒè‡ªå·±ï¼æ¯æ¬¡é¢è¯•å‰åšå……è¶³çš„å‡†å¤‡ï¼Œé»˜å¿µï¼šâ€œæ”¾è½»æ¾æ”¾è½»æ¾æ”¾è½»æ¾â€ï¼›é¢è¯•åŽè¿›è¡Œåæ€æ€»ç»“ï¼Œé»˜å¿µï¼šâ€œçœ‹ç¼˜åˆ†çœ‹ç¼˜åˆ†çœ‹ç¼˜åˆ†â€ ~]]></content>
      <tags>
        <tag>interview</tag>
        <tag>job</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggleæ—¥è®° - åŸºäºŽRçš„Titanicå¯è§†åŒ–åˆ†æž]]></title>
    <url>%2F2018%2F03%2F26%2Fkaggle%E6%8C%91%E6%88%98%E6%97%A5%E8%AE%B0--Titanic%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Titanicä¸ºKaggleå…¥é—¨èµ›ä¹‹ä¸€ï¼Œç±»åˆ«ä¸ºäºŒåˆ†ç±»çš„ç›‘ç£æ¨¡åž‹ã€‚æ ·æœ¬æ•°æ®å¯è‡ªè¡Œå‰å¾€å®˜ç½‘ä¸‹è½½ï¼Œcsvæ ¼å¼ï¼ˆtrain + testï¼‰ ä»¥ä¸‹ä¸ºæˆ‘ç”¨Rå¯¹æºå­—æ®µæ•°æ®çš„åˆ†æžï¼š get data12345df_train = read.csv(&quot;data/train.csv&quot;)%&gt;% as.data.table() df_test = read.csv(&quot;data/test.csv&quot;)%&gt;% as.data.table() dataAll = rbind(df_train, df_test, fill = T) å¤šå›¾ç»˜åˆ¶1234567891011121314151617181920212223242526272829303132333435multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) &#123; library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use &apos;cols&apos; to determine layout if (is.null(layout)) &#123; # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) &#125; if (numPlots==1) &#123; print(plots[[1]]) &#125; else &#123; # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) &#123; # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) &#125; &#125;&#125; sex123456789101112131415sex_analysis = function(trainDf)&#123; a&lt;-CrossTable(trainDf$Sex, trainDf$Survived) par(mfrow=c(1,2)) barplot(a$t[,2], beside = TRUE, sub = &quot;survival count of both sexes&quot;, ylab = &quot;survival count&quot;, xlab = &quot;Sex&quot;, ylim = c(0,250),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;)) # 250 shall change barplot(a$prop.row[,2], beside = TRUE, sub = &quot;survival rate of both sexes&quot;, ylab = &quot;survival rate&quot;, xlab = &quot;Sex&quot;, ylim = c(0,0.80),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;)) # 0.8 shall change&#125;sex_analysis(df_train) age1234567891011121314151617age_analysis = function(trainDf)&#123; ageDf = trainDf[!is.na(trainDf$Age),] ageDf$Age = as.integer(ageDf$Age) survivedDf = ageDf[ageDf$Survived == 1] m=seq(0,max(ageDf$Age),by=5) survivedAge=cut(survivedDf$Age,m)%&gt;%table%&gt;%data.frame ageDfAge=cut(ageDf$Age,m)%&gt;%table%&gt;%data.frame survivedAge = data.frame(survivedAge, round(survivedAge$Freq/ageDfAge$Freq, digits = 4)) colnames(survivedAge)=c(&apos;Age&apos;,&apos;count&apos;, &apos;prop&apos;) # aveRate = sum(survivedAge$prop[1:13])/13 p1 &lt;- ggplot(data = survivedAge,aes(x =Age,y=count)) + geom_bar(stat = &apos;identity&apos;) + ggtitle(&quot;age distribution of survivors&quot;) p2 &lt;- ggplot(data = survivedAge,aes(x =Age,y=prop)) + geom_bar(stat = &apos;identity&apos;) + ggtitle(&quot;age freq distribution of survivors&quot;) multiplot(p1, p2)&#125;age_analysis(df_train) fare12345678910111213141516171819fare_analysis = function(trainDf)&#123; trainDf$Fare = as.integer(trainDf$Fare) survivedDf = trainDf[trainDf$Survived == 1] notSurvivedDf = trainDf[trainDf$Survived == 0] survivedFare = survivedDf$Fare%&gt;%table%&gt;%data.frame colnames(survivedFare) = c(&apos;Fare&apos;, &apos;Count&apos;) notSurvivedFare = notSurvivedDf$Fare%&gt;%table%&gt;%data.frame colnames(notSurvivedFare) = c(&apos;Fare&apos;, &apos;Count&apos;) p1 &lt;- ggplot(data = survivedFare,aes(x =Fare,y=Count)) + geom_bar(stat = &apos;identity&apos;) + ggtitle(&quot;fare distribution of survivors&quot;) p2 &lt;- ggplot(data = notSurvivedFare,aes(x =Fare,y=Count)) + geom_bar(stat = &apos;identity&apos;) + ggtitle(&quot;fare distribution of NOT survivors&quot;) p3 &lt;- barplot(height = cbind(&apos;survived&apos;= mean(survivedDf$Fare), &apos;notSurvived&apos; = mean(notSurvivedDf$Fare)), main = &apos;average fare&apos;, ylab = &apos;fare&apos;, ylim = c(0,50),col = &quot;lightblue&quot;) # 50 shall change multiplot(p1, p2)&#125;fare_analysis(df_train) pclass123456789101112131415pclass_analysis = function(trainDf)&#123; a &lt;- CrossTable(trainDf$Pclass, trainDf$Survived) par(mfrow=c(1,2)) barplot(a$t[,2], beside = TRUE, sub = &quot;survival count of pclass&quot;, ylab = &quot;survival count&quot;, xlab = &quot;pclass&quot;, ylim = c(0,140),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 250 shall change barplot(a$prop.row[,2], beside = TRUE, sub = &quot;survival rate of pclass&quot;, ylab = &quot;survival rate&quot;, xlab = &quot;pclass&quot;, ylim = c(0,0.70),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 0.8 shall change&#125;pclass_analysis(df_train) sibsp12345678910111213141516171819202122232425sibsp_analysis = function(trainDf)&#123; trainDf = data.frame(trainDf, sibsp = ifelse(trainDf$SibSp&gt;0, 1, 0)) a &lt;- CrossTable(trainDf$SibSp, trainDf$Survived) b &lt;- CrossTable(trainDf$sibsp, trainDf$Survived) par(mfrow=c(2,2)) barplot(a$t[,2], beside = TRUE, sub = &quot;survival count of sibsp&quot;, ylab = &quot;survival count&quot;, xlab = &quot;sibsp&quot;, ylim = c(0,140),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 250 shall change barplot(a$prop.row[,2], beside = TRUE, sub = &quot;survival rate of sibsp&quot;, ylab = &quot;survival rate&quot;, xlab = &quot;sibsp&quot;, ylim = c(0,0.70),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 0.8 shall change barplot(b$t[,2], beside = TRUE, sub = &quot;survival count of sibsp&quot;, ylab = &quot;survival count&quot;, xlab = &quot;sibsp&quot;, ylim = c(0,250),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 250 shall change barplot(b$prop.row[,2], beside = TRUE, sub = &quot;survival rate of sibsp&quot;, ylab = &quot;survival rate&quot;, xlab = &quot;sibsp&quot;, ylim = c(0,0.50),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 0.8 shall change&#125;sibsp_analysis(df_train) Parch12345678910111213141516171819202122232425Parch_analysis = function(trainDf)&#123; trainDf = data.frame(trainDf, parch = ifelse(trainDf$Parch&gt;0, 1, 0)) a &lt;- CrossTable(trainDf$Parch, trainDf$Survived) b &lt;- CrossTable(trainDf$parch, trainDf$Survived) par(mfrow=c(2,2)) barplot(a$t[,2], beside = TRUE, sub = &quot;survival count of Parch&quot;, ylab = &quot;survival count&quot;, xlab = &quot;Parch&quot;, ylim = c(0,140),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 250 shall change barplot(a$prop.row[,2], beside = TRUE, sub = &quot;survival rate of Parch&quot;, ylab = &quot;survival rate&quot;, xlab = &quot;Parch&quot;, ylim = c(0,0.70),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 0.8 shall change barplot(b$t[,2], beside = TRUE, sub = &quot;survival count of Parch&quot;, ylab = &quot;survival count&quot;, xlab = &quot;Parch&quot;, ylim = c(0,250),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 250 shall change barplot(b$prop.row[,2], beside = TRUE, sub = &quot;survival rate of Parch&quot;, ylab = &quot;survival rate&quot;, xlab = &quot;Parch&quot;, ylim = c(0,0.50),col = c(&quot;mistyrose&quot;,&quot;lightblue&quot;,&quot;lightyellow&quot;)) # 0.8 shall change&#125;Parch_analysis(df_train) corr123456789101112131415161718col1 &lt;- colorRampPalette(c(&quot;#7F0000&quot;,&quot;red&quot;,&quot;#FF7F00&quot;,&quot;yellow&quot;,&quot;white&quot;, &quot;cyan&quot;, &quot;#007FFF&quot;, &quot;blue&quot;,&quot;#00007F&quot;))col2 &lt;- colorRampPalette(c(&quot;#67001F&quot;, &quot;#B2182B&quot;, &quot;#D6604D&quot;, &quot;#F4A582&quot;, &quot;#FDDBC7&quot;, &quot;#FFFFFF&quot;, &quot;#D1E5F0&quot;, &quot;#92C5DE&quot;, &quot;#4393C3&quot;, &quot;#2166AC&quot;, &quot;#053061&quot;))col3 &lt;- colorRampPalette(c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;))col4 &lt;- colorRampPalette(c(&quot;#7F0000&quot;,&quot;red&quot;,&quot;#FF7F00&quot;,&quot;yellow&quot;,&quot;#7FFF7F&quot;, &quot;cyan&quot;, &quot;#007FFF&quot;, &quot;blue&quot;,&quot;#00007F&quot;))wb &lt;- c(&quot;white&quot;,&quot;black&quot;)par(ask = TRUE)#-----------------------# correlation analysis# cov = covariance matrix# cor = correlation matrixM&lt;-cor(train)corrplot(M, method=&quot;color&quot;, col=col1(20), cl.length=21,order = &quot;AOE&quot;,tl.cex = 0.6,,addCoef.col=&quot;grey&quot;)]]></content>
      <tags>
        <tag>kaggle</tag>
        <tag>R</tag>
        <tag>visualization</tag>
        <tag>å¯è§†åŒ–åˆ†æž</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 List]]></title>
    <url>%2F2018%2F01%2F20%2F2018-List%2F</url>
    <content type="text"><![CDATA[å…¶å®žæ˜¯çœŸçš„éžå¸¸æ„Ÿæ…¨2017å¹´ï¼Œæ¯”èµ·ç§¯ç´¯ï¼Œå¯èƒ½æ›´å¤§çš„è¿›æ­¥æ˜¯çŸ¥é“äº†æ–¹å‘ã€‚é‚£ä¹ˆ2018å¹´çš„æƒ³æ³•å°±æ˜¯çŸ¥è¡Œåˆä¸€å§ã€‚ Rough List Jan 2018å¹´çš„èŒä¸šè®¡åˆ’ï¼Œä¹°æ–°ç”µè„‘ï¼Œæ­å»ºå„ç§çŽ¯å¢ƒ Feb kaggleæ¯”èµ›å¼€å¯ Mar æ‰¾å¼€æºçš„é¡¹ç›®åšï¼Œgithubä¸Š Apr çˆ¬è™«â€”python May æ€»ç»“è¿‡åŽ»ä¸€å¹´å®žçŽ°è¿‡çš„ç®—æ³•ï¼Œå†™æ•™ç¨‹ Jun Cè¯­è¨€ç®—æ³•â€”-æ•°æ®ç»“æž„ä¸Žç®—æ³• July ä¸°å¯Œgithub/repository Aug åšä¸€ä¸ªå°æ¸¸æˆâ€”GitHubä¸Šçš„å¼€æº Sep å‰ä»–/é’¢ç´/æž¶å­é¼“ Oct å¤§æ•°æ®â€”mapreduce/ spark/ Nov æ½œæ°´/é™è½ä¼ž/æ·±æµ· Dec å¦‚ä½•å¹³è¡¡ï¼šå­¦ä¹  è®¡ç®—æœºè¯­è¨€ ç®—æ³•/å·¥ä½œ å¼€æºé¡¹ç›®/å¥èº«/é˜…è¯» ä¹¦å•/å‡ºè¡Œâ€”â€”â€”æ‰€æœ‰çš„é¢‘çŽ‡æŽ§åˆ¶ è¿™é‡Œï¼Œåªè®°å½•ä¸€ä¸ªçƒ­çˆ±æ•²ä»£ç çš„ç¾Žå°‘å¥³çš„ä¸­äºŒç”Ÿæ´»ã€‚]]></content>
      <tags>
        <tag>plan</tag>
        <tag>2018</tag>
      </tags>
  </entry>
</search>
